






/*
trail notes
you've been doing a great job of making each of these a one screen show!

[x]check
[x]integer, text convert
[x]data, text convert
[x]string clip to parse
[x]string validation, like only alphanumeric, regex from chatgpt
[x]randbetween, inclusive, just in case you need it
[x]ticks to nice text for the user in the current time zone, see if you can avoid a library actually--yes, you can avoid a library and do all the internationalization you need, and very succinctly
[x]say four digit byte size, don't otherwise internationalize numbers

[x]browser crypto, symmetric and asymetric, []hashing
[~]uri encoding, get taht in here just for compleness, have tests that show one+space or one%20space
[]email validation, another one for chatgpt, 1raw, 2cleaned for use, 3normalized for duplicate detection

do all the text checking you need for the database
and for validating single-line user input, like a screen name
not, at this time, the text checking for a user writing a comment
*/








once you've figured out pinia, you might code the sorted record thing
allows immediate lookups by tag
and also browsing in tick count order
keeps sorted
fast for adding already sorted input data
doesn't readmit duplicates, which will be super common
or maybe pinia already has something better
or maybe this isn't the right way to do things



/*
TODO be able to manually run all the tests these places:
[x]icarus, vite
[]nuxt, local, client-side
[]nuxt, local, server-side, emulated worker
[]nuxt, deployed, client-side
[]nuxt, deployed, server-side, deployed worker
[]lambda, local, server-side, emulated lambda
[]lambda, deployed, server-side, deployed lambda
*/














/*
2024sep4 notes coding look
-design this well enough that you improve upon it, rather than replacing it, in the future
-be able to hand it anything, global, window, and it doesn't choke

moar feature ideas for look
-single depth and short composed {} and [] stay on one line
-very long arrays have ... in the middle
*/


















server logs
dog() and log() goes to datadog
or, figure out your pattern for an api function
with a total covereage try catch
and then maybe if there's an error, look it into text and send it back to the browser
and even use status codes, maybe--ask chatgpt
first just see if you can get a stack trace from a worker
but then you have to make sure this isn't an attack vector, you have to know if you're built for production or not

the api full catch also dogs if it takes longer than 2s

a little clock appears in the page at 2s indicating (clock) working 2s...

dog is great because it's secure and private even in deployment--if the attacker figures out how to break an api, they can't get information mistakingly included in a log

duh the workign clock can totally be completely client-side
ask chatgpt if there's a convention in nuxt to guard against repeat requests to the same api
the don't click or you'll mess up your order, thing

ask chatgpt about the confirm form resubmission browser warning
you don't think you're coding any forms that could cause this, but understand it for the first time ever to be sure









/*
Date of birth
You must be 18+
Enter like YYYYMMDD four digit year, two digit month, two digit day
Tell us your real birthday!
Not posted on profile, kept private and secure
We check IDs for creators, date must match
Got that as January 22, 1999

If you become a creator
Must match your birthdate on your ID
Private, secure, not posted on profile
We won't post on profile
Must match your ID
Kept private from fans and collaborators
It'll
Birthday with 
that's February 14, 1980
*/







/*
//here's a bike shed--write a tight little deindent

so you can
{
	{
		s = `
		first list
		second line
		third line
		`
	}
}

and what you get is
`first line
second line
third line
`

so it removes the leading newline
keeps the other newlines the same
and removes whatever space is at the start of the first line
from all the other lines

maybe remove that number of things
like that number of

confirm you don't chop off any nonwhitespace doing this
*/












consider this pattern

checkThing - throws if not
isThing - false if not
makeThing - shapes to bring into compliance and returns

then you put all the tests around isThing
and instead of *Soft here, you use isThing when you don't want an exception















== moar web security, CORS

net23 apis will check the origin header right in the lambda node code
you should also make cold3.cc/api apis only talk to cold3.cc and net23 on https
you could check origin in code, but a more standard way is to implement the CORS preflight thing
and a standard way to do that is:
https://nuxt-security.vercel.app/documentation/middleware/cors-handler
"Testing CORS configuration can be done by running one application with NuxtSecurity enabled and creating a second application (that is running on a different port) that will send a request to the first app. Then, a CORS error could be easily observed that proves that CORS is working as expected."

if you've got things all CORS-ed up, can you still develop on localhost?






== moar web security, CSP

chatgpt suggests:

<head>
  <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self'; style-src 'self'; object-src 'none'; frame-ancestors 'none'">
</head>

but not sure what this prevents. not injecting script directly into the page through inspector, but maybe a chrome extension running script?
also not sure what adding this could break








== iphone faceid for the web

for four years now you've been able to do faceid on the web!
but you've never seen it anywhere!
and, it's easy!

https://developer.apple.com/videos/play/wwdc2020/10670/
https://webkit.org/blog/11312/meet-face-id-and-touch-id-for-the-web/
https://support.apple.com/en-us/HT213449

const options = {
    publicKey: {
        rp: { name: "example.com" },
        user: {
            name: "john.appleseed@example.com",
            id: userIdBuffer,
            displayName: "John Appleseed"
        },
        pubKeyCredParams: [ { type: "public-key", alg: -7 } ],
        challenge: challengeBuffer,
        authenticatorSelection: { authenticatorAttachment: "platform" }
    }
};
const publicKeyCredential = await navigator.credentials.create(options);

Registration (Face ID Setup)
-The user clicks the "Register Face ID" button.
-A new credential is created using navigator.credentials.create().
-The options include information about the relying party (your website), the user, and the desired credential parameters.
-The public key credential is logged and should be sent to your server for storage.

Authentication (Using Face ID)
-The user clicks the "Authenticate with Face ID" button.
-The user's Face ID is authenticated using navigator.credentials.get().
-The options include a challenge and the allowed credentials (user's registered credential ID).
-The assertion is logged and should be sent to your server for verification.














       _                 _               
 _ __ (_)_ __   __ _    | | ___   __ _   
| '_ \| | '_ \ / _` |   | |/ _ \ / _` |  
| |_) | | | | | (_| |_  | | (_) | (_| |_ 
| .__/|_|_| |_|\__, ( ) |_|\___/ \__, ( )
|_|            |___/|/           |___/|/ 
                 _             _     _       
  __ _ _ __   __| |  _ __ ___ | |__ (_)_ __  
 / _` | '_ \ / _` | | '__/ _ \| '_ \| | '_ \ 
| (_| | | | | (_| | | | | (_) | |_) | | | | |
 \__,_|_| |_|\__,_| |_|  \___/|_.__/|_|_| |_|
                                             

== four similar but distinct tasks

1 logs for development (logflare)
only use while coding
console dot log works great locally, but not pushed
confirm that you're getting the same error both places
doesn't need to be permanent
needs to be absolutely reliable and instantaneous

2 logs for production (datadog)
use in production
keep a permanent record of all the details that happened with every third party api
do more analysis later, asking questions then you haven't though of now

3 uptime checking (pingdom, datadog synthetics, checkly)
know if cloudflare pages, workers, lambda, or supabase goes down or becomes slow
be on pagerduty to confirm and complain
if there's an outage in the middle of the night, know how long it lasted, and how frequently this happens

4 round robin control, api performance, and fault alerting (kinesis, datadog, kafka)
balance load between two equivalent third party apis, like for sending SMS, or charging credit cards
immediately stop using a service that starts rejecting use, or no users can complete tasks on
use for third party services that are easy to stack redundantly and likely to break or be unreliable
see a dashboard of median response time and processing time of api calls and database queries

and you started into kinesis by saying imagine you're sending it a stream of numbers, and want the hourly average

https://docs.aws.amazon.com/msk/latest/developerguide/serverless.html
is fast, serverless, and lets you query by timestamp range (kinesis just lets you look at the last 5mb or whatever)

>next morning thinking

[0] logs for development
use logflare, it's simple and faster than datadog

[1] downtime and pagerduty for pages, workers, and supabase
a provider like pingdom is setup for sanity checking, is my site online
for having their own dashboard put up a graph for, how fast is page load? hello world api? increment count api? testing cloudflare pages, workers, and supabase speeds
sends you a pagerduty alert (can they do telegram?) if cloudflare pages, workers, or supabase are down
but this doesn't test speeds as encountered by real users, and that's fine, you trust the providers to be good about that

[2] redundant fragile apis like sms, email, credit cards. characteristics:
these have the following properties:
likely to be slow and unreliable
likely to stop working for a day, or forever
easy to place redundant ones in a list and use round-robin
some also have user interaction steps, you only know a sms got through when the user types it in (and can only compare speeds when you've got really a lot of users doing this)

[2b] needs
here, the system needs a few things:
(i) real time knowledge when one breaks, not to tell a human, rather to automatically steer around
(ii) auditable record of everything we told the api, and everything it said back, to use later maybe

[2c] datadog for audit logs
datadog is for historic logs
every api interaction, you dog() everything you told the api, and everything it said back
you don't use this, but it's there for later, if you need it

[2d] sql pattern for round robin visibility
you make a pattern in supabase to track the minute to minute speed and reliability of fragile apis
a table is really simple, just a tick count, a millisecond duration, and success or failure
and then that table is named, like the name of one provider in one category, like aws email, or braintree cards
these tables only hold the last hour or two of data
if something is very rare, it also holds the last 1000 records or whatever
you call it with a function like robin(name, duration, outcome) and it adds a row, finding the right table
you query it with another function, and it tells you the average, median, fastest and slowest N events, and if there are a bunch of recent failures, like it's been broken for 20 minutes and counting, or something
when a call comes in and sees that it's the first call in a short time period (one minute? 6 minutes?) it automatically performs the totalling step
this step involves totalling up everything from the previous time period, storing the computed answers in another table, which has like one row per timeperiod, and then removing the rows about that time period from the granular table

[2e] this is an opportunity to try out another provider alongside supabase, like Neon, to see what the developer experience is like, and maybe it's faster
but, you code this for generic sql just like the regular database
these tables are also different because they drop rows

ok, based on all that, let's snapshot the design concept by looking at javascript function prototypes, and providers

log() - fancied up and sent to console dot log
flare() - log to logflare, to see an error once pushed, and then remove it ~ uses logflare
dog() - goes to datadog, to add a note to the audit trail for real data in production ~ uses datadog

robinStart('email', 'amazon', tag) - we've asked aws to send a sms, and local code tracks the request with tag in a local variable
robinBad('email', 'amazon', tag) - some time later, the api denied our request
robinGood('email', 'amazon', tag) - the api and the user succeeded. oh here the tag local variable doesn't work anymore
robinChoose('sms') - returns which one to use, like 'amazon' or 'sendgrid' based on flipping a coin and avoiding errors

oh, there's a page at /staff that shows the performance of all of these
and make it so that to add a new one, you just start talking about it, you don't have to do anything else
because there are like two dozen tables named 001-012, and then an organizer table which holds the names
this is a really cool idea, you can get what you need in just a few screenfuls of code
~ uses supabase or neon/planetscale, one of the others

real pages pingdom and users can go to
/ping1 static page from cloudflare pages
/ping2 page that depends on a hello world cloudflare worker
/ping3 essentially your count button, page->worker->supabase settings table
~ try pingdom, datadog synthetics, and checkly, and see what you like

you're really happy with all of this
notice how much you got done just in .txt, not .js
this also completely avoids kinesis and kafka, which you might have dove into
also, instead of making log() more complex, log(), flare() and dog() are separate and simple this way

actually the robin interface is like this:

robin1 - about to call the api
robin2 - api returned reporting success
robin3 - user completed task proving success
robin4 - api returned refusal or failure

14 - the api hates us now, duration not really important here, "red fourteens"
123 - everything's working for the user, 13 duration is important "green onetwentythrees"
10 - the api never got back to us, really broken "red one nothings"
120 - the api says it worked, but the user never was able to do it or gave up "purple twelve nothings"

current minute - add records
previous minute - wait for users
two minutes ago - total up, but keep around for the daily digest. these three do the real time intelligence

current hour
previous hour
two hours ago - total up and move to archival table, which will grow one record per service per user, which is fine

a single lastcycle tick in the control table notices new minutes and hours, and performs the necessary totaling

more later:

you can totally do this efficiently and easily without sets of tables for provider
there are three tables
robin_control - settings just for robin
robin_hours - finished records of each service each hour, totaled and averaged for 1-2pm at the start of 3pm
robin_minutes - individual recent records

hours counts the 14s, 123s, 1nothing, and 12nothings
the 123s duration is meaningfull, hours includes the mean, median, standard deviation, and specific outliers

columns like:
service
provider
tick1
tick2
tick3
tick4
and then you select to total and average
and select to remove old rows only if there are too many rows

the intelligence to pick a service for a task isn't baked into the robin api, rather it's all in the worker, using that api
functions like robinRecent(service, provider) returns a js object with mean, median, failure rates, and so on
and from those results the business logic in the worker

maybe do to the minute, and then every 6 hour period, quarter day
so the site can be up years and the records don't get very huge
and an api that breaks can be detected really quickly and steered around

all you need to do are: 10, 50, 90th percentile value, those three, 50 is median of course
you don't need average, and you don't need slowest and fastest list

more several days later:

ok, but you also want to know how fast the site seems for users
here's a separate, additional, and still pretty simple way to do that

every api request is lumped together, those with no, small, and complex database interactions
telemetry is entirely from the worker, so the durations are trusted

the public system status page shows the 10/50/90 percentiles of response times in milliseconds
this goes beyond just saying, "everything's fine" and a bunch of granular, but detail-free, green checkmarks

chatgpt says i could make this with aws kinesis, but aws cloudwatch is a better fit
the example code uses cloudwatch, dynamodb, and lambda to store and then summarize

more later:

cloudwatch and datadog are actually designed for this
you like datadog in that it's separate and may be simpler
datadog can compute percentiles on the server, you don't have to download all the datapoint and run them yourself

// Define the query to get 10th, 50th, and 90th percentiles, count, sum, and last value
const query = `percentile:my_service.my_duration{my_tag:my_value}.rollup(60, p10, p50, p90), 
	count:my_service.my_duration{my_tag:my_value}.rollup(60),
	sum:my_service.my_duration{my_tag:my_value}.rollup(60),
	last:my_service.my_duration{my_tag:my_value}`
const params = new URLSearchParams({
	from: Math.floor(new Date(from).getTime() / 1000),
	to: Math.floor(new Date(to).getTime() / 1000),
	query: query})

not sure on use of rollup(), as_count(), or nothing on the ends
https://docs.datadoghq.com/dashboards/functions/rollup/#rollup-interval-enforced-vs-custom

you'll want to scrutinize the slow ones
you could do this by counting them, but also if it's slow, your robin() code logs them to a different tag or whatever of datadog, and the dashboard picks them up
this is easier and better than trying to search for them later

moar moar than a month later:

i want to use datadog for a variety of purposes. for instance, here are three:

(1: round robin) high frequency performance analysis: logs of different named attempts, their duration, and success, failure, or timeout. there could be a lot of these (many per second). also, the app will need to query them, to find out what's working quickly and reliably, and get percentiles over recent time periods

(2: api record) verbose documentation of third party api performance: here, the logs will be longer, and contain json of objects that go perhaps several references deep. with this use case, there's no querying--this is for archival, only. later on, if an api is misbehaving, developers may go into datadog to look at this record to try to determine the cause

(3: exceptional alert) important and immediate information for developers: let's say a truly exceptional exception occurs, like code that we wrote that's part of our app throws, in a way that should be impossible. this third category of logs (top level uncaught exceptions) needs to be extremely verbose, separate from the other two types, and immediately for the attention of the development team

(4: daily development) current development in deployed environment: when coding, a developer might use console log to see some variables they're watching in code as it runs. then, when deployed, it can be useful to also see those kinds of logs. on the next push, these log statements might be removed. and, these logs are meant to be throwaway--they won't be saved, and they won't be consistent

before we dive into using datadog, are these use cases known and common practice? are there names for these? are there some others im not thinking of?

your goal is to do it all to datadog
so it's all fetch skipping the response
and it's super simple

(and now moar 2024sep9)

=======================================================

2024sep7 the whole horizon of logging, pre and post 1.0

[I.] Services

datadog is good at logs
you send them js objects of your own structure and taxonomy
and later can save and sort and alert based on them
you can send several logs at once, just by sending an array, to have a summary one and a detailed one

datadog has a separate product called RUM, real user monitoring
their js on your page measures how long pages fetch, and take to finish, for real users

pingdom and equivalent are good at uptime
they have graphs that show median and 95th percentile
and you can configure their pinger to look for a tag on a page to confirm it really got it

postgres is good at analysis
right in sql, you can filter rows and then compute averages and percentiles

local development is good at seeing console log output
and for durable output, you can write to log.txt

console log in a lambda automatically goes to amazon cloudwatch
and this is more reliable than datadog, as it's first party, you didn't turn it on, you can't forget to do it
cloudflare's logging isn't on by default, but also uses the standard streams out
so it makes sense to call these as a backup to datadog, also

[II.] Needs, you've found six

current needs, before v1:
PING: service availability and speed (pingdom) the site was down for 6 hours last night, but it's back up again. lots of the time, lambda has a cold start of over 2 seconds
DEBUG: debug logging during development (datadog, maybe logflare) i can log out this note while coding, and want to see the same note in production. once it's working, ill comment it out
ALERT: uncaught exception (datadog) a mistake in our own code caused an exception caught by the last resort try block--wake up the developers!
AUDIT: api auditing (datadog) that one time that that one service failed, what exactly did we send it? recently, the information twilio is sending us back has changed

eventual needs, after v1:
ROBIN: round robin analysis (postgres) twilio refused our request, immediately switch to amazon! users type in codes faster when we email them through sendgrid, use that more often! these two services perform equally well, but one is half the cost, use that one more frequently!
RUM: experience for real users (datadog rum) all the little parts are fast, but the average user signed in still has to wait 1200ms for their feed to load. ok, after that last update, we got it down to 400ms

[III.] Construction

PING (pingdom) (roadmap v0.5)
make pages on the site that, unlike the others, have code on the page rather than factoring into components
and are coded so that SSR always happens (ask chat) nuxt is doing this by default
cold3.cc/ping1 - static
cold3.cc/ping2 - script renders
cold3.cc/ping3 - fetch to api
cold3.cc/ping4 - call supabase
cold3.cc/ping5 - call lambda at net23
it'll be interesting to see the average times, if things are slower in europe or asia compared to the us, and also if there are slow cold starts for ping4, and not the others
it's great that the simple deployed cors restrictions are all in place, even while ping4 reaches over to net23

(datadog generally) send objects in this form, below. type is required. tag and tick are there, but likely redundant or unnecessary. message and watch are one or the other or both
{
  type: 'alert',//high level category to sort later on, like ALERT, AUDIT, DEBUG
  tag: 'kmo3cWhe27B5pgmM2RnGV'//tag for this datadog log
  tick: 1725723263168,//tick when we logged it
  message: 'text, one line, meant to fit on a screen and be read by a human',
  watch: {},//payload object, all the details, including more ticks and tags
}

DEBUG (datadog)
{
  type: 'debug'
  tag,
  tick,
  message,
  watch//here's where we, the developers, can see something that we just pushed to confirm deployed behavior matches local behavior
}

ALERT (datadog) (roadmap v0.1)
{
  type: 'alert'
  tag,
  tick,
  message,
  watch//here's where we, the developers, can find out about uncaught exceptions
}
here's where you probably want information about the environment, lambda, nuxt ssr, csr, otherwise you'll be looking at one of these scratching your head not knowing even what computer where ran into it

AUDIT (datadog) (roadmap v0.1)
{
  type: 'audit'
  tag,
  tick,
  (no message on this one?)
  watch: {}//here's the object with all the details about the fetch we did and what happened
}

ROBIN (postgres)
there are two postgres tables, one for quick current records, another for historical summary records
naming services and providers, like "amazon.email" and "sinch.sms" is free-form, you just send a new name, you don't have to change schema to do this
a record in the table records what happened and how long it took, like this sms service refused to send for us, immediately returning an error, or this user successfully entered a otp code 2 minutes after we emailed it to them through sendgrid
there aren't more details about the thing that happened, but tags referenced here means we can find it in datadog or elsewhere in the database
it's fast and easy to query the current table to see the last 10 results from a particular service, or the average success time over the last day or hour
some results are known in the same worker invocation, like if amazon sent an email in 150ms. other results are noticed between worker invocations, like if the user typed in the code successfully 5 minutes later. those are represented by two rows in the quick table that have a matching tag, or by finding the first row and adding to blank columns, yeah, that's probably better
when the system gets a request and checks the time and notices that there are some older records in the recent table, it performs an additional step to clear out older records, condensing them into summaries, and moving them into the historical table
this keeps both tables short, even with thousands of users and years of history
services that are used rarely don't get archived this way--to get archived, a row needs to be older than a week, let's say, and also not the most recent 100 records about that service
on the history table, there's a row summarizing each service and provider's performance for each 6 hour period
these rows include:
-number of records, mean and median
-how many succeeded, failed
-1st, 5th, 10th, 90th, 95th, 99th percentiles of success durations
-tags of a random capped handful of failed requests

RUM (datadog rum) (roadmap v1.5)
use datadog's rum feature as intended
and check out the nice charts and graphs and maps on the dashboard
will this mean there are cookies now, though?

==
























== nuxt public

[]in the nuxt project, get rid of public/favicon.ico






== all the ways code can run

>all the ways code can run:

icarus libarary tests (vite, local, client)
node library tests (node, local, server)

nuxt server side api code (nuxt, local|deployed, server)
nuxt client side code running on the server as part of the hydration step (nuxt, local|deployed, server)
nuxt client side code running on the client (nuxt, local|deployed, client)

serverless framework lambda code running on the server (serverless, local|deployed, server)

(all those x these additional possibilities)

nuxt local development
nuxt deployed to cloudflare

serverless framework local lambda emulation
serverless framework deployed to aws lambda






















~ security note ~

good security design is always a balance between security and usability

the goal is to keep the user signed in without expiration
and to keep that as secure as possible

this is not the current experience of the web, a short timeout, any IP address change, or any use from another device leads to automatic sign-out
and the poor user experience harms security, as users choose bad passwords, or discontinue using the site altogether

the only place i've noticed signin without expiration is facebook
i use facebook less than once a year, but whenever i go to facebook.com, im still signed in
meta likely has metrics that link signing out a user with losing that user

essentially, a browser is identified by a tag
and if a signed-in browser reports the same tag to the server, it's still signed in
but there are two security enhancements:

(1) scary naming
in local storage, the key and name look like this:
current_session_password: account_access_code_DO_NOT_SHARE_hi1y5ICjnEQLVDKtawm0C
imagine a n00b user is on a discord server or subreddit dedicated to power users of an instance of the platform, where a sophisticated attacker coaches users into compromising their accounts
warning language to the n00b may give them pause











totally make the page where the user sees their query string and ip address of their current and previous sessions
and you can include their webgl nvidia stuff there, also
























