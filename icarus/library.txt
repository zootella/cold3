













/*
trail notes
you've been doing a great job of making each of these a one screen show!

[x]check
[x]integer, text convert
[x]data, text convert
[x]string clip to parse
[x]string validation, like only alphanumeric, regex from chatgpt
[x]randbetween, inclusive, just in case you need it
[x]ticks to nice text for the user in the current time zone, see if you can avoid a library actually--yes, you can avoid a library and do all the internationalization you need, and very succinctly
[x]say four digit byte size, don't otherwise internationalize numbers

[x]browser crypto, symmetric and asymetric, []hashing
[~]uri encoding, get taht in here just for compleness, have tests that show one+space or one%20space
[]email validation, another one for chatgpt, 1raw, 2cleaned for use, 3normalized for duplicate detection

do all the text checking you need for the database
and for validating single-line user input, like a screen name
not, at this time, the text checking for a user writing a comment


*/



/*
you want to use tag and now as local variables in code
but they're clashing with tag() and now() as short commonly imported functions

you were thinking makeTag(), but tickNow() is not awesome
maybe Tag() and Now()



*/





once you've figured out pinia, you might code the sorted record thing
allows immediate lookups by tag
and also browsing in tick count order
keeps sorted
fast for adding already sorted input data
doesn't readmit duplicates, which will be super common
or maybe pinia already has something better
or maybe this isn't the right way to do things



/*
TODO be able to manually run all the tests these places:
[x]icarus, vite
[]nuxt, local, client-side
[]nuxt, local, server-side, emulated worker
[]nuxt, deployed, client-side
[]nuxt, deployed, server-side, deployed worker
[]lambda, local, server-side, emulated lambda
[]lambda, deployed, server-side, deployed lambda
*/














/*
2024sep4 notes coding look
-design this well enough that you improve upon it, rather than replacing it, in the future
-be able to hand it anything, global, window, and it doesn't choke

moar feature ideas for look
-single depth and short composed {} and [] stay on one line
-very long arrays have ... in the middle
*/


















//maybe wrap json parse and stringify to deal with errors you've encountered





//    _                 
//   (_)___  ___  _ __  
//   | / __|/ _ \| '_ \ 
//   | \__ \ (_) | | | |
//  _/ |___/\___/|_| |_|
// |__/                 

export function jsonStringify(o) {//watch out for a circular reference
	/*
//watch out for a circular reference
try {
return JSON.stringify(o, null)//single line
} catch (e) { return '(circular reference)' }//watch out for circular references
*/
}

export function jsonParse(s) {//watch out for a blank body
	/*
//watch out for a blank body
//get the text first, and keep that, too

r.responseText = await response.text()//might be nothing, even on success
if (r.responseText) r.responseData = JSON.parse(r.responseText)//throws if you give it nothing
*/

}



























/*
2024apr26
code encoding, some bikeshedding

[x]from separate project entry point, Ctrl+S leads to green or red check on page

level0 ~

[]base16 data
[]base64 data
[]base62 data, your code
[]base62 integer, chatGPT's code

[]round trip check
[]toss on failure
[]speed test your base62 with the browser's base64

level1 ~

[]random 1-9
[]random 0-9
[]random 0-9 A-Z a-z

not in scope for this first pass:
-page rendered width analysis
-unicode search for narrow accents that still render
-unified chinese base256





*/























/*
2024may16
dustiest corner of the bike shed
make sure your log and see plan is good before you go back in there
-get rid of log destinations
-do program an indented deep object sayer
-dont use it when there's a browser inspector which has arrows




*/




/*
more bikeshedding here, but what if log worked like this
log(a)
turns a into text, and prefixes it with the timestamp
log(a, b)
not sure anymore
the thing you forgot when designing this refactor above was that in the browser, you don't want everything text, because the browser inspector has arrows to go deep into nested objects, which you won't code yourself, and which is incredible




*/


/*
moar notes and ideas

instead of see doing lines like
(string) s "hello"
all you need is "hello" because quotes mean string
{object}
[array]
"string"
7 with no punctuation is a number
true with no punctuation is a boolean


you made the separte vite entry point
but now nuxt localhost:3000/test is working so well you don't need it(?)
like, it's just as fast
and you can Ctrl+S library0.js several times in a row and it refreshes each time
and, the text stays in place on the page, and also scrolls down in the console
so, what more is there to do here?
i guess refactor it so you don't have log destinations anymore

*/


















do encoding at the same time as you do cryptography
maybe make a little object so you dont have to type Uint8array over and over again
the object is indelible
holds some binary data
and has methods to convert between all these different formats:

text, like a string of cleartext
length, how long is it in bytes when you look at it as bytes
base16
base64, checking roundtrips and whatnot

you create these from:
text strings
base16
base64
random data, cryptographically secure

following hte model, this should be tiny, it shouldn't be your node project's Data object



remind yourself of the pattern, but you think you remember it
and now you can use const!

export function Data(c) {
	const o = {}
	o.type = 'Data'//no how do you make that const?
	const type 
	

	const function fromText(s) {

	}

	const function base16()
	const function text()

	return o
}
this constructor makes a new data object

have it lazily create and then save the different forms
understand how utf8 text encoding plays a role with all this, bake that into Data as the default









server logs
dog() and log() goes to datadog
or, figure out your pattern for an api function
with a total covereage try catch
and then maybe if there's an error, look it into text and send it back to the browser
and even use status codes, maybe--ask chatgpt
first just see if you can get a stack trace from a worker
but then you have to make sure this isn't an attack vector, you have to know if you're built for production or not

the api full catch also dogs if it takes longer than 2s

a little clock appears in the page at 2s indicating (clock) working 2s...

dog is great because it's secure and private even in deployment--if the attacker figures out how to break an api, they can't get information mistakingly included in a log

duh the workign clock can totally be completely client-side
ask chatgpt if there's a convention in nuxt to guard against repeat requests to the same api
the don't click or you'll mess up your order, thing

ask chatgpt about the confirm form resubmission browser warning
you don't think you're coding any forms that could cause this, but understand it for the first time ever to be sure









/*
this one is for birthdates, stored in the database as text like "1980-02-14"
take the user's input free-form
allow months as feb, 2, 02, whatever
allow years 02, 2002, 76, whatever
figure out what they may mean and put up one or more buttons that snap to

you also need to take a well formatted one, and return the year, month, day as numbers

Date of birth
You must be 18+
Enter like YYYYMMDD four digit year, two digit month, two digit day
Tell us your real birthday!
Not posted on profile, kept private and secure
We check IDs for creators, date must match
Got that as January 22, 1999

If you become a creator
Must match your birthdate on your ID
Private, secure, not posted on profile
We won't post on profile
Must match your ID
Kept private from fans and collaborators
It'll
Birthday with 
that's February 14, 1980


have icarus have a little expandable text box where you can live type into a function to test it



idea for how to get dates the fastest
worst, by the way, is putting up a calendar clicker for today, asking for a dob
phone puts up number pad
instruction text says birthday, YYYYMMDD
see if you can infer other orders, including shorter orders like 040176
if they type YYMMDD, YYYYDDMM
one or two buttons pop up that say "do you mean 2002 Jan 26"
if it's ambiguous, multiple buttons pop up
a single backspace clears the whole field, maybe
*/







/*
//here's a bike shed--write a tight little deindent


so you can
{
	{
		s = `
		first list
		second line
		third line
		`
	}
}

and what you get is
`first line
second line
third line
`

so it removes the leading newline
keeps the other newlines the same
and removes whatever space is at the start of the first line
from all the other lines

maybe remove that number of things
like that number of

confirm you don't chop off any nonwhitespace doing this


*/






/*
function f() {

	const alphabet = '0123456789ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz';
	let census = {}
	for (let c of alphabet) census[c] = 0

	let duration = 1*Time.second
	let t = now()
	let d, b
	while (now() < t + duration) {
		d = Data({random: randomBetween(1, 32)})
		b = d.base62()
		for (let c of b) census[c]++
	}

	let s = ''
	for (let c of alphabet) s += `\r\n${c}\t${census[c]}`
	log(s)


}
//f()

test(() => {


})
*/







/*
noop(() => {

	//get some random bytes
	let b16 = Data({random: 40}).base16()
	//encode into base62
	let b62 = arrayToBase62(Data({base16: b16}).array())
	//and back again
	let b16a = Data({array: base62ToArray(b62)}).base16()
	//log to confirm looks good
	log(b62, b16, b16a)


})


noop(() => {

	let d = Data({random: 40})
	let b62 = _arrayToBase62(d.array())
	log(
		b62,
		d.base16(),
		Data({array: _base62ToArray1(b62)}).base16(),
		Data({array: _base62ToArray2(b62)}).base16()
	)


})


/*
async function f() {
	let d, s
	let size = 50
	let duration = 4*Time.second


	let r1 = 0
	let t = now()
	while (now() < t + duration) {
		d = Data({random: size})
		s = arrayToBase64(d.array())
		base64ToArray(s)
		r1++
	}
	log(r1+' base64')

	let r2 = 0
	t = now()
	while (now() < t + duration) {
		d = Data({random: size})
		s = _arrayToBase62(d.array())
		_base62ToArray1(s)
		r2++
	}
	log(r2+' base62 to array method 1')

	let r3 = 0
	t = now()
	while (now() < t + duration) {
		d = Data({random: size})
		s = _arrayToBase62(d.array())
		_base62ToArray2(s)
		r3++
	}
	log(r3+' base62 to array method 2')
}
f()
*/








/*
async function f2() {
	let k = await symmetricCreateKey()
	let b = await symmetricExportKey(k)
	let k2 = await symmetricImportKey(b)
	let p = 'a short message, like card info'
	let c = await encrypt(p, k)
	let d = await decrypt(c, k)
	if (d != p) log('decryption mismatch')
}
async function f3() {
	let k = await createKey_new()
	let b = await exportKey_new(k)
	let k2 = await importKey_new(b)
	let p = 'a short message, like card info'
	let c = await encrypt_new(p, k)
	let d = await decrypt_new(c, k)
	if (d.text() != p) log('decryption mismatch')
}
async function f() {
	log('just in a function f')
	let r1 = 0
	let t = now()
	while (now() < t + 4*Time.second) {
		r1++
	}
	log(r1+' empty')
	let r2 = 0
	t = now()
	while (now() < t + 4*Time.second) {
		await f2()
		r2++
	}
	log(r2+' direct')
	let r3 = 0
	t = now()
	while (now() < t + 4*Time.second) {
		await f3()
		r3++
	}
	log(r3+' custom')
}
f()
*/



/*
improve tiny tests

TODO make this an object that fills up here
but which is exported, and you can bring into a vue component
and run tests there and see results

also, get tests to await async tests etc correctly
this should be pretty easy

also, did you have mustThrow or something before? you need that again; right now you're just skipping writing tests of check* functions

and have it return an object of stats and outcomes, one of which is the composed status line
*/









TODO do you need soft versions of the text functions?

//TODO these throw if anything is out of bounds, maybe add startSoft, endSoft, beyondSoft that instead return shorter or blank
//bookmark you added this really fast, go through the larger process of actually testing them, and them using them where you need them
export function startSoft(s, n)  { return clipSoft(s, 0, n) }
export function clipSoft(s, i, n) {
	s = s+''//fix everything
	if (!minInt(i)) i = 0
	if (!minInt(n)) n = 0

	if (i     > s.length) i = s.length
	if (i + n > s.length) n = s.length - i

	return clip(s, i, n)
}
test(function() {
	ok(clipSoft('abc', 1, 0) == '')
	ok(clipSoft('abc', 1, 1) == 'b')
	ok(clipSoft('abc', 1, 2) == 'bc')
	ok(clipSoft('abc', 1, 3) == 'bc')
});


/*
TODO
consider this pattern

checkThing - throws if not
isThing - false if not
makeThing - shapes to bring into compliance and returns

then you put all the tests around isThing
and instead of *Soft here, you use isThing when you don't want an exception
*/


//make Process and Fetch that work node or web worker, vue front or back end
const Access = (typeof process != 'undefined' && process.env) ? process.env : import.meta.env
const Fetch = (typeof fetch != 'undefined') ? fetch : (await import('node-fetch')).default









== browser fingerprint

the browser key in local stroage can keep the user signed in forever
but, an attacker could coach a n00b through discord to share it with him
so also use a browser fingerprint
you're looking for something script can't affect, and which doesn't change
even as the device moves geographically and goes through browser and os updates

WebGL Renderer: NVIDIA GeForce GTX 1050 Ti/PCIe/SSE2
WebGL Vendor: NVIDIA Corporation
User Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36
Screen Resolution: 1920x1080

don't do screen resolution because multiple monitors and turning your phone
add window.location to get https://cold3.cc, make sure there's not a page afterwards
and add the browser tag
so you're hashing text like this:

agent:Mozilla/. (Macintosh; Intel Mac OS X __) AppleWebKit/. (KHTML, like Gecko) Chrome/... Safari/.;renderer:NVIDIA GeForce GTX 1050 Ti/PCIe/SSE2;vendor:NVIDIA Corporation;tag:lYPvabMNWoWwlrgkguy0g;

you make and save tag if it's not already there before doing this
you never send tag over the wire, just the hash
you never save that hash to disk, just the wire
yeah, this is a great idea
keep the user signed in forever, (unless he upgrades his video card)
but also make it so a reddit coach can't get a n00b victim to look in local storage and share his browser tag

maybe even call it
account-access-tag-DO-NOT-SHARE

current_session_password: account_access_code_DO_NOT_SHARE_lYPvabMNWoWwlrgkguy0g

what if you did that, except for user agent, you remove all the numerals

also, save it in a global variable so you only have to hash it once on page load, not every time you make a request

async function getBrowserFingerprint() {
  // Get WebGL Renderer and Vendor
  const getWebGLFingerprint = () => {
    const canvas = document.createElement('canvas');
    const gl = canvas.getContext('webgl') || canvas.getContext('experimental-webgl');
    if (!gl) return null;

    const debugInfo = gl.getExtension('WEBGL_debug_renderer_info');
    if (!debugInfo) return null;

    const renderer = gl.getParameter(debugInfo.UNMASKED_RENDERER_WEBGL);
    const vendor = gl.getParameter(debugInfo.UNMASKED_VENDOR_WEBGL);

    return {
      renderer: renderer,
      vendor: vendor
    };
  };

  // Get User Agent
  const userAgent = navigator.userAgent;

  // Get Screen Resolution
  const screenResolution = `${screen.width}x${screen.height}`;

  // Combine all pieces of information
  const webGLFingerprint = getWebGLFingerprint();
  if (!webGLFingerprint) return "WebGL information not available";

  const combinedFingerprint = `
    WebGL Renderer: ${webGLFingerprint.renderer}
    WebGL Vendor: ${webGLFingerprint.vendor}
    User Agent: ${userAgent}
    Screen Resolution: ${screenResolution}
  `;

  return combinedFingerprint.trim();
}

// Example usage
getBrowserFingerprint().then(fingerprint => {
  log(fingerprint);
});











== moar web security, CORS

net23 apis will check the origin header right in the lambda node code
you should also make cold3.cc/api apis only talk to cold3.cc and net23 on https
you could check origin in code, but a more standard way is to implement the CORS preflight thing
and a standard way to do that is:
https://nuxt-security.vercel.app/documentation/middleware/cors-handler
"Testing CORS configuration can be done by running one application with NuxtSecurity enabled and creating a second application (that is running on a different port) that will send a request to the first app. Then, a CORS error could be easily observed that proves that CORS is working as expected."

if you've got things all CORS-ed up, can you still develop on localhost?






== moar web security, CSP

chatgpt suggests:

<head>
  <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self'; style-src 'self'; object-src 'none'; frame-ancestors 'none'">
</head>

but not sure what this prevents. not injecting script directly into the page through inspector, but maybe a chrome extension running script?
also not sure what adding this could break








== iphone faceid for the web

for four years now you've been able to do faceid on the web!
but you've never seen it anywhere!
and, it's easy!

https://developer.apple.com/videos/play/wwdc2020/10670/
https://webkit.org/blog/11312/meet-face-id-and-touch-id-for-the-web/
https://support.apple.com/en-us/HT213449

const options = {
    publicKey: {
        rp: { name: "example.com" },
        user: {
            name: "john.appleseed@example.com",
            id: userIdBuffer,
            displayName: "John Appleseed"
        },
        pubKeyCredParams: [ { type: "public-key", alg: -7 } ],
        challenge: challengeBuffer,
        authenticatorSelection: { authenticatorAttachment: "platform" }
    }
};
const publicKeyCredential = await navigator.credentials.create(options);

Registration (Face ID Setup)
-The user clicks the "Register Face ID" button.
-A new credential is created using navigator.credentials.create().
-The options include information about the relying party (your website), the user, and the desired credential parameters.
-The public key credential is logged and should be sent to your server for storage.

Authentication (Using Face ID)
-The user clicks the "Authenticate with Face ID" button.
-The user's Face ID is authenticated using navigator.credentials.get().
-The options include a challenge and the allowed credentials (user's registered credential ID).
-The assertion is logged and should be sent to your server for verification.














       _                 _               
 _ __ (_)_ __   __ _    | | ___   __ _   
| '_ \| | '_ \ / _` |   | |/ _ \ / _` |  
| |_) | | | | | (_| |_  | | (_) | (_| |_ 
| .__/|_|_| |_|\__, ( ) |_|\___/ \__, ( )
|_|            |___/|/           |___/|/ 
                 _             _     _       
  __ _ _ __   __| |  _ __ ___ | |__ (_)_ __  
 / _` | '_ \ / _` | | '__/ _ \| '_ \| | '_ \ 
| (_| | | | | (_| | | | | (_) | |_) | | | | |
 \__,_|_| |_|\__,_| |_|  \___/|_.__/|_|_| |_|
                                             

== four similar but distinct tasks

1 logs for development (logflare)
only use while coding
console dot log works great locally, but not pushed
confirm that you're getting the same error both places
doesn't need to be permanent
needs to be absolutely reliable and instantaneous

2 logs for production (datadog)
use in production
keep a permanent record of all the details that happened with every third party api
do more analysis later, asking questions then you haven't though of now

3 uptime checking (pingdom, datadog synthetics, checkly)
know if cloudflare pages, workers, lambda, or supabase goes down or becomes slow
be on pagerduty to confirm and complain
if there's an outage in the middle of the night, know how long it lasted, and how frequently this happens

4 round robin control, api performance, and fault alerting (kinesis, datadog, kafka)
balance load between two equivalent third party apis, like for sending SMS, or charging credit cards
immediately stop using a service that starts rejecting use, or no users can complete tasks on
use for third party services that are easy to stack redundantly and likely to break or be unreliable
see a dashboard of median response time and processing time of api calls and database queries

and you started into kinesis by saying imagine you're sending it a stream of numbers, and want the hourly average

https://docs.aws.amazon.com/msk/latest/developerguide/serverless.html
is fast, serverless, and lets you query by timestamp range (kinesis just lets you look at the last 5mb or whatever)

>next morning thinking

[0] logs for development
use logflare, it's simple and faster than datadog

[1] downtime and pagerduty for pages, workers, and supabase
a provider like pingdom is setup for sanity checking, is my site online
for having their own dashboard put up a graph for, how fast is page load? hello world api? increment count api? testing cloudflare pages, workers, and supabase speeds
sends you a pagerduty alert (can they do telegram?) if cloudflare pages, workers, or supabase are down
but this doesn't test speeds as encountered by real users, and that's fine, you trust the providers to be good about that

[2] redundant fragile apis like sms, email, credit cards. characteristics:
these have the following properties:
likely to be slow and unreliable
likely to stop working for a day, or forever
easy to place redundant ones in a list and use round-robin
some also have user interaction steps, you only know a sms got through when the user types it in (and can only compare speeds when you've got really a lot of users doing this)

[2b] needs
here, the system needs a few things:
(i) real time knowledge when one breaks, not to tell a human, rather to automatically steer around
(ii) auditable record of everything we told the api, and everything it said back, to use later maybe

[2c] datadog for audit logs
datadog is for historic logs
every api interaction, you dog() everything you told the api, and everything it said back
you don't use this, but it's there for later, if you need it

[2d] sql pattern for round robin visibility
you make a pattern in supabase to track the minute to minute speed and reliability of fragile apis
a table is really simple, just a tick count, a millisecond duration, and success or failure
and then that table is named, like the name of one provider in one category, like aws email, or braintree cards
these tables only hold the last hour or two of data
if something is very rare, it also holds the last 1000 records or whatever
you call it with a function like robin(name, duration, outcome) and it adds a row, finding the right table
you query it with another function, and it tells you the average, median, fastest and slowest N events, and if there are a bunch of recent failures, like it's been broken for 20 minutes and counting, or something
when a call comes in and sees that it's the first call in a short time period (one minute? 6 minutes?) it automatically performs the totalling step
this step involves totalling up everything from the previous time period, storing the computed answers in another table, which has like one row per timeperiod, and then removing the rows about that time period from the granular table

[2e] this is an opportunity to try out another provider alongside supabase, like Neon, to see what the developer experience is like, and maybe it's faster
but, you code this for generic sql just like the regular database
these tables are also different because they drop rows

ok, based on all that, let's snapshot the design concept by looking at javascript function prototypes, and providers

log() - fancied up and sent to console dot log
flare() - log to logflare, to see an error once pushed, and then remove it ~ uses logflare
dog() - goes to datadog, to add a note to the audit trail for real data in production ~ uses datadog

robinStart('email', 'amazon', tag) - we've asked aws to send a sms, and local code tracks the request with tag in a local variable
robinBad('email', 'amazon', tag) - some time later, the api denied our request
robinGood('email', 'amazon', tag) - the api and the user succeeded. oh here the tag local variable doesn't work anymore
robinChoose('sms') - returns which one to use, like 'amazon' or 'sendgrid' based on flipping a coin and avoiding errors

oh, there's a page at /staff that shows the performance of all of these
and make it so that to add a new one, you just start talking about it, you don't have to do anything else
because there are like two dozen tables named 001-012, and then an organizer table which holds the names
this is a really cool idea, you can get what you need in just a few screenfuls of code
~ uses supabase or neon/planetscale, one of the others

real pages pingdom and users can go to
/ping1 static page from cloudflare pages
/ping2 page that depends on a hello world cloudflare worker
/ping3 essentially your count button, page->worker->supabase settings table
~ try pingdom, datadog synthetics, and checkly, and see what you like

you're really happy with all of this
notice how much you got done just in .txt, not .js
this also completely avoids kinesis and kafka, which you might have dove into
also, instead of making log() more complex, log(), flare() and dog() are separate and simple this way

actually the robin interface is like this:

robin1 - about to call the api
robin2 - api returned reporting success
robin3 - user completed task proving success
robin4 - api returned refusal or failure

14 - the api hates us now, duration not really important here, "red fourteens"
123 - everything's working for the user, 13 duration is important "green onetwentythrees"
10 - the api never got back to us, really broken "red one nothings"
120 - the api says it worked, but the user never was able to do it or gave up "purple twelve nothings"

current minute - add records
previous minute - wait for users
two minutes ago - total up, but keep around for the daily digest. these three do the real time intelligence

current hour
previous hour
two hours ago - total up and move to archival table, which will grow one record per service per user, which is fine

a single lastcycle tick in the control table notices new minutes and hours, and performs the necessary totaling

more later:

you can totally do this efficiently and easily without sets of tables for provider
there are three tables
robin_control - settings just for robin
robin_hours - finished records of each service each hour, totaled and averaged for 1-2pm at the start of 3pm
robin_minutes - individual recent records

hours counts the 14s, 123s, 1nothing, and 12nothings
the 123s duration is meaningfull, hours includes the mean, median, standard deviation, and specific outliers

columns like:
service
provider
tick1
tick2
tick3
tick4
and then you select to total and average
and select to remove old rows only if there are too many rows

the intelligence to pick a service for a task isn't baked into the robin api, rather it's all in the worker, using that api
functions like robinRecent(service, provider) returns a js object with mean, median, failure rates, and so on
and from those results the business logic in the worker

maybe do to the minute, and then every 6 hour period, quarter day
so the site can be up years and the records don't get very huge
and an api that breaks can be detected really quickly and steered around

all you need to do are: 10, 50, 90th percentile value, those three, 50 is median of course
you don't need average, and you don't need slowest and fastest list

more several days later:

ok, but you also want to know how fast the site seems for users
here's a separate, additional, and still pretty simple way to do that

every api request is lumped together, those with no, small, and complex database interactions
telemetry is entirely from the worker, so the durations are trusted

the public system status page shows the 10/50/90 percentiles of response times in milliseconds
this goes beyond just saying, "everything's fine" and a bunch of granular, but detail-free, green checkmarks

chatgpt says i could make this with aws kinesis, but aws cloudwatch is a better fit
the example code uses cloudwatch, dynamodb, and lambda to store and then summarize

more later:

cloudwatch and datadog are actually designed for this
you like datadog in that it's separate and may be simpler
datadog can compute percentiles on the server, you don't have to download all the datapoint and run them yourself

// Define the query to get 10th, 50th, and 90th percentiles, count, sum, and last value
const query = `percentile:my_service.my_duration{my_tag:my_value}.rollup(60, p10, p50, p90), 
	count:my_service.my_duration{my_tag:my_value}.rollup(60),
	sum:my_service.my_duration{my_tag:my_value}.rollup(60),
	last:my_service.my_duration{my_tag:my_value}`
const params = new URLSearchParams({
	from: Math.floor(new Date(from).getTime() / 1000),
	to: Math.floor(new Date(to).getTime() / 1000),
	query: query})

not sure on use of rollup(), as_count(), or nothing on the ends
https://docs.datadoghq.com/dashboards/functions/rollup/#rollup-interval-enforced-vs-custom

you'll want to scrutinize the slow ones
you could do this by counting them, but also if it's slow, your robin() code logs them to a different tag or whatever of datadog, and the dashboard picks them up
this is easier and better than trying to search for them later

moar moar than a month later:

i want to use datadog for a variety of purposes. for instance, here are three:

(1: round robin) high frequency performance analysis: logs of different named attempts, their duration, and success, failure, or timeout. there could be a lot of these (many per second). also, the app will need to query them, to find out what's working quickly and reliably, and get percentiles over recent time periods

(2: api record) verbose documentation of third party api performance: here, the logs will be longer, and contain json of objects that go perhaps several references deep. with this use case, there's no querying--this is for archival, only. later on, if an api is misbehaving, developers may go into datadog to look at this record to try to determine the cause

(3: exceptional alert) important and immediate information for developers: let's say a truly exceptional exception occurs, like code that we wrote that's part of our app throws, in a way that should be impossible. this third category of logs (top level uncaught exceptions) needs to be extremely verbose, separate from the other two types, and immediately for the attention of the development team

(4: daily development) current development in deployed environment: when coding, a developer might use console log to see some variables they're watching in code as it runs. then, when deployed, it can be useful to also see those kinds of logs. on the next push, these log statements might be removed. and, these logs are meant to be throwaway--they won't be saved, and they won't be consistent

before we dive into using datadog, are these use cases known and common practice? are there names for these? are there some others im not thinking of?

your goal is to do it all to datadog
so it's all fetch skipping the response
and it's super simple

(and now moar 2024sep9)

=======================================================

2024sep7 the whole horizon of logging, pre and post 1.0

[I.] Services

datadog is good at logs
you send them js objects of your own structure and taxonomy
and later can save and sort and alert based on them
you can send several logs at once, just by sending an array, to have a summary one and a detailed one

datadog has a separate product called RUM, real user monitoring
their js on your page measures how long pages fetch, and take to finish, for real users

pingdom and equivalent are good at uptime
they have graphs that show median and 95th percentile
and you can configure their pinger to look for a tag on a page to confirm it really got it

postgres is good at analysis
right in sql, you can filter rows and then compute averages and percentiles

local development is good at seeing console log output
and for durable output, you can write to log.txt

console log in a lambda automatically goes to amazon cloudwatch
and this is more reliable than datadog, as it's first party, you didn't turn it on, you can't forget to do it
cloudflare's logging isn't on by default, but also uses the standard streams out
so it makes sense to call these as a backup to datadog, also

[II.] Needs, you've found six

current needs, before v1:
PING: service availability and speed (pingdom) the site was down for 6 hours last night, but it's back up again. lots of the time, lambda has a cold start of over 2 seconds
DEBUG: debug logging during development (datadog, maybe logflare) i can log out this note while coding, and want to see the same note in production. once it's working, ill comment it out
ALERT: uncaught exception (datadog) a mistake in our own code caused an exception caught by the last resort try block--wake up the developers!
AUDIT: api auditing (datadog) that one time that that one service failed, what exactly did we send it? recently, the information twilio is sending us back has changed

eventual needs, after v1:
ROBIN: round robin analysis (postgres) twilio refused our request, immediately switch to amazon! users type in codes faster when we email them through sendgrid, use that more often! these two services perform equally well, but one is half the cost, use that one more frequently!
RUM: experience for real users (datadog rum) all the little parts are fast, but the average user signed in still has to wait 1200ms for their feed to load. ok, after that last update, we got it down to 400ms

[III.] Construction

PING (pingdom) (roadmap v0.5)
make pages on the site that, unlike the others, have code on the page rather than factoring into components
and are coded so that SSR always happens (ask chat) nuxt is doing this by default
cold3.cc/ping1 - static
cold3.cc/ping2 - script renders
cold3.cc/ping3 - fetch to api
cold3.cc/ping4 - call supabase
cold3.cc/ping5 - call lambda at net23
it'll be interesting to see the average times, if things are slower in europe or asia compared to the us, and also if there are slow cold starts for ping4, and not the others
it's great that the simple deployed cors restrictions are all in place, even while ping4 reaches over to net23

(datadog generally) send objects in this form, below. type is required. tag and tick are there, but likely redundant or unnecessary. message and watch are one or the other or both
{
  type: 'alert',//high level category to sort later on, like ALERT, AUDIT, DEBUG
  tag: 'kmo3cWhe27B5pgmM2RnGV'//tag for this datadog log
  tick: 1725723263168,//tick when we logged it
  message: 'text, one line, meant to fit on a screen and be read by a human',
  watch: {},//payload object, all the details, including more ticks and tags
}

DEBUG (datadog)
{
  type: 'debug'
  tag,
  tick,
  message,
  watch//here's where we, the developers, can see something that we just pushed to confirm deployed behavior matches local behavior
}

ALERT (datadog) (roadmap v0.1)
{
  type: 'alert'
  tag,
  tick,
  message,
  watch//here's where we, the developers, can find out about uncaught exceptions
}
here's where you probably want information about the environment, lambda, nuxt ssr, csr, otherwise you'll be looking at one of these scratching your head not knowing even what computer where ran into it

AUDIT (datadog) (roadmap v0.1)
{
  type: 'audit'
  tag,
  tick,
  (no message on this one?)
  watch: {}//here's the object with all the details about the fetch we did and what happened
}

ROBIN (postgres)
there are two postgres tables, one for quick current records, another for historical summary records
naming services and providers, like "amazon.email" and "sinch.sms" is free-form, you just send a new name, you don't have to change schema to do this
a record in the table records what happened and how long it took, like this sms service refused to send for us, immediately returning an error, or this user successfully entered a otp code 2 minutes after we emailed it to them through sendgrid
there aren't more details about the thing that happened, but tags referenced here means we can find it in datadog or elsewhere in the database
it's fast and easy to query the current table to see the last 10 results from a particular service, or the average success time over the last day or hour
some results are known in the same worker invocation, like if amazon sent an email in 150ms. other results are noticed between worker invocations, like if the user typed in the code successfully 5 minutes later. those are represented by two rows in the quick table that have a matching tag, or by finding the first row and adding to blank columns, yeah, that's probably better
when the system gets a request and checks the time and notices that there are some older records in the recent table, it performs an additional step to clear out older records, condensing them into summaries, and moving them into the historical table
this keeps both tables short, even with thousands of users and years of history
services that are used rarely don't get archived this way--to get archived, a row needs to be older than a week, let's say, and also not the most recent 100 records about that service
on the history table, there's a row summarizing each service and provider's performance for each 6 hour period
these rows include:
-number of records, mean and median
-how many succeeded, failed
-1st, 5th, 10th, 90th, 95th, 99th percentiles of success durations
-tags of a random capped handful of failed requests

RUM (datadog rum) (roadmap v1.5)
use datadog's rum feature as intended
and check out the nice charts and graphs and maps on the dashboard
will this mean there are cookies now, though?

==

































== big divide, done

some bike shed
fraction([1, 2, 3], [4])
always takes numerator and denomonator arrays, , [1] if you just want to multiply
always takes js numbers, not strings, not bigints
checks that they're all numbers and integers
multiplies top and bottom and then does the division, that part uses bigint so it can go over
throws if the answer is too big to fit as a regular js integer
returns an object of answers which include answer and remainder





== nuxt public

[]in the nuxt project, get rid of public/favicon.ico






== all the ways code can run

>all the ways code can run:

icarus libarary tests (vite, local, client)
node library tests (node, local, server)

nuxt server side api code (nuxt, local|deployed, server)
nuxt client side code running on the server as part of the hydration step (nuxt, local|deployed, server)
nuxt client side code running on the client (nuxt, local|deployed, client)

serverless framework lambda code running on the server (serverless, local|deployed, server)

(all those x these additional possibilities)

nuxt local development
nuxt deployed to cloudflare

serverless framework local lambda emulation
serverless framework deployed to aws lambda

>design

we're in the bike shed with this one, but wouldn't it be cool to have

a function you query that tells you where you are

the floppy disk ascii where you set
name and version, which are available to code everywhere

and then every time you git commit or upload push, a prerun task computes and writes
####s showing how much code you've written, on the disk
sha256 hash of all the code you've written, filled in each time
from the date and hash a simple version slug like v2024aug14h00FF00FF

 ____________________
| |cold3           | |
|.|________________|H|
| |________________| |
| |________________| |
| |________________| |
| |________________| |
| |________________| |
|                    |
|    ____________    |
|   |   |  _     |   |
|   |   | | |    |   |
|   |   | |_|    | V |
|___|___|________|___|

stamp task looks at all the files except .gitignore and diskette* to generate stamp1.js:

export stamp1 = {
	name: 'cold3',//set by user
	tick: 456789456789456,//from developtment system
	files: 456,//from disk
	bytes: 456789456123,//totaled from disk
	hash1: 'DSJCBGLG2G5FLB6NFFUM4DQUPQBI7B4ZNWSYUOGC3YKE7UGMCNHQ'//computed from disk
}

and then looks at stamp1.js to generate disk2.js:

export stamp2 = {
	(add in the stuff from stamp1, also),
	hash2: 'QUPQBI7B4DSJCBGLG2G5FLB6NFFUM4DZNWS3YKE7UGMCNHQYUOGC',//computed from stamp1.js
	seal: '2024aug14_QUPQBI7B'//composed from stamp1.tick and stamp2.hash, like a wax seal over the whole thing
}

and then code everywhere in all three projects references and shows stamp2.stamp

this is called stamp, as in
$ npm run stamp
it doesn't happen automatically, you manually do stamp, deploy, push
so now you can match a git version on github with what the server shows in cold3.cc/about

this is a pretty cool idea

there are two files so you can put things back, but at a later time, and confirm you get the same stamp1.hash1

simple way to do this
$ npm run stamp
does node stamp
stamp.js loads the library itself
this is just a regular old fashioned node script
meant only to run on the development workstation

actually generate a stamp1.txt with hashes, sizes, and paths like:
DSJCBGLG2G5FLB6NFFUM4DQUPQBI7B4ZNWSYUOGC3YKE7UGMCNHQ 123456 folder/file.js
DSJCBGLG2G5FLB6NFFUM4DQUPQBI7B4ZNWSYUOGC3YKE7UGMCNHQ 123456 folder/file2.js

so the files in order are:
stamp1.txt, made from disk
stamp2.js, made from stamp1
stamp3.js, made from stamp2











refactor the subtle crypto in library0 so that
[]key import and export are handled for you (a user doesn't have to first import a key)
[]functions take and return Data (and are not specific to one form of encoding or another)
right now you think you'll end up using
-symmetric encryption for getAccess
-signed messages for net23 media access permission
-rsa for nothing
but you still want them all factored properly






you could combine sticker and library0
by pass-through importing all of sticker's exports at the top of library0
and then importing them from library0 elsewhere
this note after you defanged circular dependencies from grand






/*
~ security note ~

good security design is always a balance between security and usability

the goal is to keep the user signed in without expiration
and to keep that as secure as possible

this is not the current experience of the web, a short timeout, any IP address change, or any use from another device leads to automatic sign-out
and the poor user experience harms security, as users choose bad passwords, or discontinue using the site altogether

the only place i've noticed signin without expiration is facebook
i use facebook less than once a year, but whenever i go to facebook.com, im still signed in
meta likely has metrics that link signing out a user with losing that user

essentially, a browser is identified by a tag
and if a signed-in browser reports the same tag to the server, it's still signed in
but there are two security enhancements:

(1) scary naming
in local storage, the key and name look like this:
current_session_password: account_access_code_DO_NOT_SHARE_hi1y5ICjnEQLVDKtawm0C
imagine a n00b user is on a discord server or subreddit dedicated to power users of an instance of the platform, where a sophisticated attacker coaches users into compromising their accounts
warning language to the n00b may give them pause

(2) browser tag hashed, not sent
the browser tag is never sent to the server
the hash is never saved to the disk
if the user's disk or system is compromised, a rudimentary scanner can recover the browser tag, but must compute the hash

(3) hash is of tag and fingerprint
rather than hashing the tag alone, details specific to the browser are included in the data hashed
these details are designed to be specific to the user's device, but unlikely to change

and then later, (2) and (3) didn't work, so falling back to just (1)
*/









how you might still use getBrowserAgentRendererAndVendor()
//webgl render info didn't work for a hashed browser tag, but still include it as a query string
//show it to the user on their page of where you're signed in and have signed in
/*
so, browser tag below didn't work as a fingerprint

but you'll still need stuff like this to verbosely fingerprint:
the user's browser, as client code sees it, with stuff like the user agent string
an incoming client request, as a nuxt api handler sees it, with the supposed query string, ip address

totally make the page where the user sees their query string and ip address of their current and previous sessions
and you can include their webgl nvidia stuff there, also
*/






so your plan is to use ECDSA P-256 cryptographic signatures to sign and validate client requests into net23
but you could also just symmetric encrypt the messages, which would be simpler, you now realize
because, is it true that in the signature plan, both public and private keys are never revealed?
the private key, which signs, is kept secret on the worker
the public key, which verifies, is  kept secret on the lambda
















