
 _   _      _                      _      ____  _____ 
| \ | | ___| |___      _____  _ __| | __ |___ \|___ / 
|  \| |/ _ \ __\ \ /\ / / _ \| '__| |/ /   __) | |_ \ 
| |\  |  __/ |_ \ V  V / (_) | |  |   <   / __/ ___) |
|_| \_|\___|\__| \_/\_/ \___/|_|  |_|\_\ |_____|____/ 
																											

Network 23 ~ "Beta to the Max!" -ChatGPT

 _________________________________
/\                                \
\_| --y'know what? There's only   |
	| four things we do better than |
	| anyone else:                  |
	|                               |
	|   music                       |
	|   movies                      |
	|   microcode (software)        |
	|   high-speed pizza delivery   |
	|   ____________________________|_
	 \_/______________________________/

some notes building the megamultimedia empire:
       _      _                   
__   _(_)_ __| |_ _   _  ___  ___ 
\ \ / / | '__| __| | | |/ _ \/ __|
 \ V /| | |  | |_| |_| |  __/\__ \
  \_/ |_|_|   \__|\__,_|\___||___/
                                  
start out up here with design document
the requirements i set, the choices i made, and the reasoning behind them
TODO, but notes follow

values, requirements

in order, they are
1 reliable: never down, not for five minutes or five users
2 simple: written at boot camp level, all the code fits on a floppy disk
3 fast: most user tasks complete in the blink of an eye

serverless, because a server would mean you have to
restart it when it goes down
update it
secure it, know when it's been compromised, and recover from such an attack
pay too much when it's too big (cinderella's problem)
change configuration and software when it's too small (evil stepsisters' problem)

code, best to worst
pure javascript
npm modules with millions of weekly downloads
npm modules with hundreds of thousands of weekly downloads


factored downwards
biggest: .js files in the library
smallest: .vue files

100% isomorphic
the exact same function checkEmail() corrects user input on the page,
and keeps malformed data from being written to the database
execution from ten different environments could call into any file
local/deployed x
node/nuxt/serverless framework/vite tdd runner icarus
cloudflare workers/amazon lambda
page/server api
page code pre-rendering on server/page code rendering on client




we love vue. we hate react
jsx lets you mix everything, invalidating the wonderous code separation of the web stack
react wins when it's complex enough professionals base their careers around it
we win when it's simple enough we're focused on features and users, not platform patterns and what's new this year


configuration as code
do as much configuration in computer-readable as possible
where that's not possible, configuration is in english, step by step, in configuration.txt
and be wary of very difficult configuration code being replacable by a pretty simple and one time step by step



and the requirements list you presented to rf goes here, too
and before that, the slides for sp



Requirements
1. Simple, full-stack JavaScript
2. Really fast: <200ms
3. Really cheap: <$5/month
4. Minimal vendor-specific code
5. Ready for millions of users overnight
6. Secure even from sophisticated attacks





computer and software are made of instructions
since the dawn of software, there have been three kinds of instructions:

I. configuration (mental, or readable)
II. configuration as code (machine executable)
III. code (machine executable)

traditionally, instrutions in category I are kept and versioned in the human brains of the development team
the rule here is that can't be the only place: all instructions in category I must also be codified here, in human readable if not machine executable form
the test is: could you hand this file to a brand new team member, and could they use it to set everything up, without asking your human brain any questions

the second value related to this is to keep instructions as far down that list as possible
code is better than configuration as code
configuration as code is better than configuration (this idea has gained recent popularity in software engineering)

but exceptions to this guideline exist!: like maybe all the domain name stuff in serverless.yml
it took a week to craft screenfuls of potentially brittle configuration
and maybe all that could be done just once at the start of the project, and documented here, as a few human-readable instructions on what to click on amazon's website dashboard







       _ _          _                _         _               _   
  __ _(_) |_    ___| |__   ___  __ _| |_   ___| |__   ___  ___| |_ 
 / _` | | __|  / __| '_ \ / _ \/ _` | __| / __| '_ \ / _ \/ _ \ __|
| (_| | | |_  | (__| | | |  __/ (_| | |_  \__ \ | | |  __/  __/ |_ 
 \__, |_|\__|  \___|_| |_|\___|\__,_|\__| |___/_| |_|\___|\___|\__|
 |___/                                                             



$ git clone git@github.com:username/repository.git

$ git checkout -b name1      # Create a new branch and switch to it
$ git push -u origin name1   # Push the new branch to github and set the upstream

$ git fetch origin           # Later, on another computer, find out about new branches up at origin
$ git branch -vv             # List branches and their origins
$ git checkout name1         # And switch to one of the new ones there

$ git checkout name1         # Switch to the 'name1' branch
$ git checkout main          # Switch back to the main branch
$ git diff main              # Show differences between the current branch and 'main'
$ git diff name1             # Show differences between 'main' (or current branch) and 'name1'

$ git merge --dry-run name1  # Simulate a merge with 'name1' to check for conflicts
$ git merge name1            # Merge 'name1' into the current branch, handling conflicts if they arise
$ git merge --abort          # Cancel a merge if conflicts are difficult to resolve

$ git branch -d name1             # Delete 'name1' after merging it (safe deletion)
$ git push origin --delete name1  # And, you should delete it from GitHub, the origin





here are some steps you did to get main

after lots of branching away to try out new features,
land1 is ready to replace main
but, it's been so long that there are some changes on main
you did a diff to make sure that you don't need any of thm

to be able to do a fast forward merge,
backup the current state of main to a new branch named main1
reset main back to the branch point, ec2023c in this example
and then main's merge of land1 gets to be fast forward

$ git checkout main
$ git branch main1
$ git push origin main1
$ git reset --hard ec2023c
$ git push origin main --force-with-lease
$ git merge land1
$ git push origin main



$ git log --graph --oneline --decorate --all
$ git log --graph --oneline --decorate --all --pretty=format:"%h %ad %d %s" --date=format:"%Y%b%d"
$ git diff ec2023c..9edded6

see the subway graph, and do a diff between two commits picking them out by their hashes












										 _                         _           _   
	___ _ __ ___  __ _| |_ ___   _ __  _ __ ___ (_) ___  ___| |_ 
 / __| '__/ _ \/ _` | __/ _ \ | '_ \| '__/ _ \| |/ _ \/ __| __|
| (__| | |  __/ (_| | ||  __/ | |_) | | | (_) | |  __/ (__| |_ 
 \___|_|  \___|\__,_|\__\___| | .__/|_|  \___// |\___|\___|\__|
															|_|           |__/               

== install aws cli

npm's aws-cli is deprecated as of 6 years ago
it seems most people install aws-cli with python
https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html
amazon docs say for windows install that msi

$ aws --version
aws-cli/2.16.4 Python/3.11.8 Windows/10 exe/AMD64

(note on mac, you're trying $ brew install awscli)

== give aws cli access to your aws account

$ aws configure

AWS Access Key ID [None]: YOUR_ACCESS_KEY_ID
AWS Secret Access Key [None]: YOUR_SECRET_ACCESS_KEY
Default region name [None]: us-east-1
Default output format [None]: json

dashboards at aws.amazon.com
iam dashboard
click the number of users
create user
net23dev2, example name
attach policies directly
check AdministratorAccess, second one
create user

click new user
access key 1, create access key
use case, command line interface, check understand reccomendation, next
no description, create access key

secret access key only shows once, paste it right into bash

$ aws sts get-caller-identity
$ aws s3 ls

confirm you're in, and list your buckets

== install serverless framework

https://www.serverless.com/

$ npm install -g serverless
$ serverless --version

== setup repository on github

made repo on github with node defaults
$ git clone https://github.com/zootella/net23

$ cd net23
move aside .gitignore as serverless create will also make one
$ serverless create --template aws-nodejs --path .

$ npm init
$ npm install -S aws-sdk

amazon access is configured, but make .env to mirror non-amazon secrets the lambdas will use
$ touch .env
and make sure .gitignore lists it
		 _                       _                                    
	__| | ___  _ __ ___   __ _(_)_ __    _ __   __ _ _ __ ___   ___ 
 / _` |/ _ \| '_ ` _ \ / _` | | '_ \  | '_ \ / _` | '_ ` _ \ / _ \
| (_| | (_) | | | | | | (_| | | | | | | | | | (_| | | | | | |  __/
 \__,_|\___/|_| |_| |_|\__,_|_|_| |_| |_| |_|\__,_|_| |_| |_|\___|
																																	

== domain name

registered net23.cc at a third party registrar

aws web console, route53 dashboard, hosted zones, create hosted zone
net23.cc, domain name
public already selected, create hosted zone
leads to page for new hosted zone, titled net23.cc, with two records
the NS record shows the name servers amazon has picked for us:

ns-1619.awsdns-10.co.uk.
ns-1281.awsdns-32.org.
ns-682.awsdns-21.net.
ns-211.awsdns-26.com.

go back to the registrar to put those in
nameservers, basic dns, changing that to custom dns
entering the four name servers above, *without* the periods at the ends

dnspropegation happens pretty quick, then check with nslookup

$ nslookup net23.cc
$ nslookup net23.cc 8.8.8.8
$ nslookup -type=NS net23.cc

first two won't work yet because you haven't made any A records
the 8s uses google's dns to check, rather than whatever is local

== ssl certificate

acm, list certificates, request
request a public certificate
add another name to this certificate to get two boxes, then enter
net23.cc
*.net23.cc
defaults of dns validation, rsa2048, request

at this point you can get the ARN
and status is pending validation

there's a button create records in route53
follow that flow, it'll make a cname record
in route53 you can see it made the record
still pending, taking a few minutes, then issued, green check

then put the ARN in .env and run the script:

$ serverless deploy

											_ _ 
	___ _ __ ___   __ _(_) |
 / _ \ '_ ` _ \ / _` | | |
|  __/ | | | | | (_| | | |
 \___|_| |_| |_|\__,_|_|_|
													

== email

goals and steps:
[x]review how you setup email forwarding already in your personal cloudflare account
[x]setup forwarding so support@cold3.cc forwards to another domain
[x]configure noreply@cold3.cc to correctly bounce or kill email
[x]configure DKIM and SPF for maximum security and correctness
[]get a lambda at api.net23.cc to send email from noreply@cold3.cc on surface and deep in headers

cloudflare dashboard for previous example domain, where you setup pages and email forwarding
websites, example.com
back on the left, dns, records
seeing these records as a result of setting up pages and email forwarding:

AAAA   www          100::                                         Proxied
CNAME  example.com  example-com.pages.dev                         Proxied
MX     example.com  route3.mx.cloudflare.net                      DNS only
MX     example.com  route2.mx.cloudflare.net                      DNS only
MX     example.com  route1.mx.cloudflare.net                      DNS only
TXT    example.com  "v=spf1 include:_spf.mx.cloudflare.net ~all"  DNS only

back on the left, email
configuration summary, custom addresses, 2 dns records, routing status enabled, email dns records configured

cloudflare dashboard for cold3.cc, where so far you've setup pages and workers but no email yet
websites, cold3.cc
back on the left, dns, records
there are only two:

AAAA   www       100::            Proxied
CNAME  cold3.cc  cold3.pages.dev  Proxied

back on the left, email, just a link to this documentation:
https://developers.cloudflare.com/email-routing/
and a big blue get started button, clicking it

breadcrumbs are
1 Create a custom address
2 Verify destination address
3 Configure your DNS

you're goign to set these two up now:

k----@cold3.cc   -> ke---@efa--.ll-
support@cold3.cc -> support@efa--.ll-

on the first one, step 2 is they send you an email link
and step 3 is they show you the extra dns records they'll add
then to add the second forwarding address click
email, email routing, routing rules, custom addresses, create address

you see how you can create an alias that drops emails, and will set that up now, as well
cloudflare dashboard, cold3.cc, email, email routing, routing rules, custom addresses, create address
custom address, noreply@cold3.cc
action, Drop

now the routing rules tab has a table with the three custom addresses
you can turn them on and off individually there, too
also, going back to dns, records, you can see the original AAAA and CNAME records
and the new MX and TXT ones that configuring email routing created

(...next, you leave the cloudflare dashboard and head over to aws)

remember, you have setup your domain ef---.ll- at namecheap and squarespace
aws has never heard of it outside of the pinpoint verification you just completed
you have working email forwarding like support@ef---.ll-
the domain you've setup at aws, net23.cc, the user should never see, not even in headers
and considering all this, your goal:
"a lambda at api.net23.cc can send an email from noreply@ef---.ll-"
should be possible

											_ _     ____  
	___ _ __ ___   __ _(_) |   |___ \ 
 / _ \ '_ ` _ \ / _` | | |     __) |
|  __/ | | | | | (_| | | |_   / __/ 
 \___|_| |_| |_|\__,_|_|_( ) |_____|
												 |/         

you started down the breadcrumbs with cold3.cc, but then read this on step 2:
"By adding your domain to Amazon SES it also allows your recipients to know that the emails coming from you. For your initial domain, we recommend verifying a domain that loads a webpage with identifiable information about your organization."
so, you need to use ef---.ll- not cold3.cc

aws dashboard labyrinth, ses homepage, get started button

here are the breadcrumbs:
Step 1 Add your email address
Step 2 Add your sending domain
Step 3 - optional Add MAIL FROM domain
Step 4 Review and get started with SES

Step 1

Add your email address, To get started with Amazon SES you must provide an email address so that we can send you a verification link. This verification process shows us you're the owner of the email address., A verification email will be sent to you at this address.

support@ef---.ll-

Step 2

Add your sending domain; A domain identity usually matches your website or business name. Amazon SES needs to be linked to your domain and verified in order to send emails to your recipients through SES. By adding your domain to Amazon SES it also allows your recipients to know that the emails coming from you. For your initial domain, we recommend verifying a domain that loads a webpage with identifiable information about your organization; To verify ownership of a domain, you must have access to its DNS settings to add the necessary records.

ef---.ll-

Step 3

Add MAIL FROM domain - optional; Configuring a custom MAIL FROM domain for messages sent from this identity enables the MAIL FROM address to align with the From address. Domain alignment must be achieved in order to be DMARC compliant; The MAIL FROM domain refers to the domain that appears in the 'From' field of an email message and is recommended for better deliverability, reputation management, and branding purposes. The MAIL FROM domain must be a subdomain of the verified identity from which you’re sending.

mail.ef---.ll-

Behavior on MX failure; Choose which action Amazon SES should take if your MAIL FROM domain's MX (Mail Exchange) record is not set up correctly

(x) Use default MAIL FROM domain; Will use a subdomain of amazonses.com instead of your custom MAIL FROM domain.
( ) Reject message; Will automatically reject the message without sending it.

Done

You are in Sandbox; In a sandbox environment, you can use all of the features offered by Amazon SES; however, certain sending limits and restrictions apply. When you're ready to move out of the sandbox, submit a request for production access. Before you submit a request for production access you must complete the tasks below.
[]Verify email address; To verify ownership of this email, check your inbox for a verification request email and click the link provided.
[]Send test email (Optional but recommended); Amazon SES mailbox simulator lets you test how your application handles different email sending scenarios.
[]Verify sending domain; Click on the button below and add the generated CNAME records to your domain's DNS provider.

Get DNS Records

there's a csv file to download with 3 CNAME, 2 TXT, and a MX record
going to namecheap to add them
already in namecheap are dns records for squarespace, and google workspace

(notes)

chatgpt suggested "mail" as the subdomain name
and explained there are four things i should configure here:
chat suggested dns records, as well, all of which are covered by amazon's instructions
SPF (Sender Policy Framework) Ensures AWS SES is authorized to send emails on behalf of your domain.
DKIM (DomainKeys Identified Mail) Adds email authentication to prevent spoofing.
DMARC (Domain-based Message Authentication, Reporting & Conformance) Helps manage and report on email authentication.
MAIL FROM Domain. Aligns the MAIL FROM domain with your From address for DMARC compliance and improved deliverability.

											_ _     _____ 
	___ _ __ ___   __ _(_) |   |___ / 
 / _ \ '_ ` _ \ / _` | | |     |_ \ 
|  __/ | | | | | (_| | | |_   ___) |
 \___|_| |_| |_|\__,_|_|_( ) |____/ 
												 |/         

2024jul3 going hard on dns for ef---.ll- to setup a net23 lambda that sends a message from ef---.ll-
first, some dns settings that are true:

amazon says, add these records to the dns for ef---.ll-:

CNAME  lzbihthz6dhrqvobiijpla77a4zfd5mo._domainkey.ef---.ll-  lzbihthz6dhrqvobiijpla77a4zfd5mo.dkim.amazonses.com
CNAME  ruhjtsmevec56krss4pvkw6b5kwalsmy._domainkey.ef---.ll-  ruhjtsmevec56krss4pvkw6b5kwalsmy.dkim.amazonses.com
CNAME  xhea6vwl7aw62ocquwlrerepecsrl6da._domainkey.ef---.ll-  xhea6vwl7aw62ocquwlrerepecsrl6da.dkim.amazonses.com
MX     mail.ef---.ll-                                         10 feedback-smtp.us-east-1.amazonses.com
TXT    mail.ef---.ll-                                         "v=spf1 include:amazonses.com ~all"
TXT    _dmarc.ef---.ll-                                       "v=DMARC1; p=none;"

remember that at this point, ef---.ll- is a squarespace with email through google workspace
namecheap says, these records are already current on ef---.ll-:

A      @                     198.185.159.144
A      @                     198.185.159.145
A      @                     198.49.23.144
A      @                     198.49.23.145
CNAME  www                   ext-cust.squarespace.com.
CNAME  3xwzwwzn2sx39xjmdp8w  verify.squarespace.com.

and doesn't show you the gmail ones, those they provide automatically
nslookup says, these are the current mx records for ef---.ll-:

MX  ef---.ll-  preference = 10, mail exchanger = aspmx3.googlemail.com
MX  ef---.ll-  preference = 5, mail exchanger = alt1.aspmx.l.google.com
MX  ef---.ll-  preference = 5, mail exchanger = alt2.aspmx.l.google.com
MX  ef---.ll-  preference = 1, mail exchanger = aspmx.l.google.com
MX  ef---.ll-  preference = 10, mail exchanger = aspmx2.googlemail.com

by the way, here are cloudflare's current dns records for cold3.cc:

AAAA   www       100::
CNAME  cold3.cc  cold3.pages.dev
MX     cold3.cc  route3.mx.cloudflare.net
MX     cold3.cc  route2.mx.cloudflare.net
MX     cold3.cc  route1.mx.cloudflare.net
TXT    cold3.cc  "v=spf1 include:_spf.mx.cloudflare.net ~all"

options for how to get this all combined:
(option 1) keep your dns at namecheap. less of a change, but more cowboy. website should stay up, but gmail could go down
(option 2) move dns to cloudflare. closer to eventual finish, squarespace could go down, saves $7/month on gmail
also, chatgpt says the mx records won't conflict because the amazon one is mail.ef---.ll- and the google ones are ef---.ll-
you're also noticing a txt record about spf1, but hopefully the mail.ef---.ll- and ef---.ll- will keep those from conflicting, too

you're picking option 2, if the squarespace goes down nobody's looking at it right now, less cowboy, closer to finish

steps for option 2:
[x]manually forward email you want from k----@ef---.ll- gmail to ef---ll-@gmail.com before you break squarespace google workspace
[x]namecheap stays the registrar, but move DNS from namecheap to cloudflare, like you did for cold3.cc
[x]get squarespace working again, first just put in the same records as before
[x]setup email routing in cloudflare, follow steps you just followed for cold3.cc for k----@, support@, noreply@
[x]add amazon records and complete domain verification on the ses dashboard
[x]apply to be let out of the sandbox

official steps above, some more notes within steps:
web records that point to squarespace will hopefully stay the same
intentonally break squarespace google workspace, unsignup to save $7/month, delete chrome profile
setup email routing in cloudflare, everything goes to ef---ll-@gmail.com

== 2024jul4 move ef---.ll- dns from namecheap to cloudflare

manually forwarded email from k----@ef---.ll- to ef---ll-@gmail.com
in squarespace, cancelled google workspace

cloudflare ef---ll- dashboard
websites, lists coldstart.cc, cold2.cc, cold3.cc
big blue button, add a site

link to documentation: https://developers.cloudflare.com/learning-paths/get-started
"This process sets up your web traffic to proxy through Cloudflare. Proxying speeds up and protects websites and services served by this domain."
https://developers.cloudflare.com/fundamentals/concepts/how-cloudflare-works
"If the domain’s status is active and the queried DNS record is set to proxied, then Cloudflare responds with an anycast IP address, instead of the value defined in your DNS table. This effectively re-routes the HTTP/HTTPS requests to the Cloudflare network, instead of directly reaching the targeted the origin server.
"In contrast, if the queried DNS record is set to DNS only, meaning the proxy is off, then Cloudflare responds with the value defined in your DNS table (that is, an IP address or CNAME record). This means HTTP/HTTPS requests route directly to the origin server and are not processed or protected by Cloudflare."
cold3.cc dashboards show DNS setup: full, and Proxied on

ef---.ll-, free plan
cloudflare says it's bringing in existing dns records, and it found
4 A, 1 CNAME, 5 MX
shoulda found 2 cname, but whatever

cloudflare says "Avoid DNS resolution issues caused by DNSSEC; Find the DNSSEC setting at your registrar (per-provider instructions). If it is on, you have two options: Turn DNSSEC off at least 24 hours before updating your nameservers. Most common; Migrate your existing DNS zone without turning off DNSSEC. More advanced; After your domain activates, we recommend turning DNSSEC on through Cloudflare."
namecheap dashboard, dnssec is off
is dnssec on for cold3.cc? no! you found the start of the flow in cloudflare, websites, cold3.cc, dns, settings, first box is dnssec
https://developers.cloudflare.com/dns/dnssec/
TODO add dnssec to cold3.cc as practice and then ever.fans as production, following steps in cloudflare and namecheap

namecheap, ef---.ll-, before changes:
nameservers is set to Namecheap BasicDNS
advanced dns tab shows the records above, the 4 A records and CNAME
dnssec is off
mail settings is Gmail, text says Gmail automatically configured for ef---.ll-

switching from Namecheap Basic DNS to Custom DNS
entering the nameservers from cloudflare:

carl.ns.cloudflare.com
maya.ns.cloudflare.com

2024jul4 1:13p finished flow on cloudflare, which says "Cloudflare is now checking the nameservers for ef---.ll-. Please wait a few hours for an update."
1:23p "Great news! Cloudflare is now protecting your site; Data about your site's usage will be here once available."
dnschecker.org also shows maya and carl for ef---.ll-, just like cold3.cc
your website site is down, ssl mismatch

cloudflare dashboard, quick start guide
automatic https rewrites, yes
always use https, yes

cloudflare dns records
4 A are correct
only one CNAME, [x]add the second
5 MX records all about gmail, [x]remove them, so now they again look like this:

A      @                     198.185.159.144
A      @                     198.185.159.145
A      @                     198.49.23.144
A      @                     198.49.23.145
CNAME  www                   ext-cust.squarespace.com
CNAME  3xwzwwzn2sx39xjmdp8w  verify.squarespace.com

you pasted a trailing period when adding the verify cname, but cloudflare isn't showing it

sqarespace dashboards, red text and boxes that says "DNS Error, We couldn't verify DNS settings with Namecheap."
and then they have a table of right and wrong settings, but it changes every time you refresh
cloudflare dashboard, ef---.ll-, dns records, turned proxy off on all records
at squarespace, refresh still is crazy
changed nothing, refreshed site, ef---.ll- is back online, ssl looks ok
squarespace dashboard, settings, domains and email, domains managed by third-party, ef---.ll-, red message changed to green Connected

interestingly(?) in firefox it's really easy to see who issued the certificate, and right now it shows
net23.cc: Verified by: Amazon (makes sense, as this ssl came from AWS ACM)
cold3.cc: Verified by: Google Trust Services (this is cloudflare pages and workers)
ef---.ll-: Verified by: Let's Encrypt (this is cloudflare dns, squarespace hosting, no idea where the ssl came from)

ok, on to setting up email forwarding
cloudflare dashboard, ef---.ll-, email, email routing, get started
destination already verified, so that step is faster

k----@ef---.ll-   -> ef---ll-@gmail.com
support@ef---.ll- -> ef---ll-@gmail.com
noreply@ef---.ll- -> Drop

all the tests worked, except nothere@ef---.ll- didn't bounce, but whatever

now at last you go back to the amazon steps
you configured aws at mail.ef---.ll-
you think later you can configure twilio at mail2.ef---.ll-, TODO

aws dashboard, ses
yellow note saying im in the sandbox
verify sending domain, get dns records, download record set, got a csv file:

CNAME  lzbihthz6dhrqvobiijpla77a4zfd5mo._domainkey.ef---.ll-  lzbihthz6dhrqvobiijpla77a4zfd5mo.dkim.amazonses.com
CNAME  ruhjtsmevec56krss4pvkw6b5kwalsmy._domainkey.ef---.ll-  ruhjtsmevec56krss4pvkw6b5kwalsmy.dkim.amazonses.com
CNAME  xhea6vwl7aw62ocquwlrerepecsrl6da._domainkey.ef---.ll-  xhea6vwl7aw62ocquwlrerepecsrl6da.dkim.amazonses.com
MX     mail.ef---.ll-                                         10 feedback-smtp.us-east-1.amazonses.com
TXT    mail.ef---.ll-                                         "v=spf1 include:amazonses.com ~all"
TXT    _dmarc.ef---.ll-                                       "v=DMARC1; p=none;"

left proxying on entering the first cname, and got an error that says it can't be proxied

entering the mx record, there's a box that says, Priority (required)
you're going to enter just feedback-smtp.us-east-1.amazonses.com as the value, and separately dial in priority 10
also, you entered the name "mail.ef---.ll-" but cloudflare is showing it as just "mail"

for the txt record, cloudflare gives you a larger box, you're pasting in as above including the quotes
it saves with the quotes

ok, they're all in there, a mix of records about squarespace, about cloudflare email forwards, and about ses
only some things are proxyable, but you've turned off proxying on all of them

amazon doesn't have a check now button, rather it says
"Identity status: Verification pending; Last checked: July 4, 2024 at 14:21 (UTC-04:00)" and it's 14:56
you might have gotten the records with that button, and then not set them quick enough for the first check
15:02 without refresh, looking back at the page, green checkmark Verified

last of three boxes on ses page is to send test email, so let's see what's in there
"Send test email Info; The Amazon SES mailbox simulator lets you test how your application handles different email sending scenarios."
from is already set to support@ef---.ll-
there's no to box, it looks like messages go to your choice of options like success@simulator.amazonses.com
there is a cc box, so let's try that? ef---ll-@outlook.com
doesn't like it because not a verified identity

aws dashboards, ses, configuration, identities
already there are two listed, both green verified:

ef---.ll-, Domain
support@ef---.ll-, Email address

so you need to [x]turn noreply into a forward, [x]verify it, then [x]turn it back into a drop target
this is easy to do you just edit the existing row
verified that one, and also ef---ll-@outlook.com
and sent a test email. now the box in the ses dashboard says green check test email sent, but you haven't gotten it in outlook yet, but whatever

aws dashboard, ses, get set up
box on the top, you're still in the sandbox (but now the boxes below are all green), button request production access

== filled out the aws ses form to request production access

To help us evaluate your request for production access, fill out the following form outlining how you plan to use Amazon SES to send email once your account has moved out of the sandbox; Production access means you can send email to any recipient, regardless of whether the recipient's address or domain is verified. However, you must still verify all identities that you use as "From", "Source", "Sender", or "Return-Path" addresses.
https://docs.aws.amazon.com/ses/latest/dg/request-production-access.html
and they say approval takes just 24 hours, which is great

Mail type; Choose the option that best represents the types of messages you plan on sending. A marketing email promotes your products and services, while a transactional email is an immediate, trigger-based communication.

Transactional

Website URL; Provide the URL for your website to help us better understand the kind of content you plan on sending.

https://www.ef---.ll-/

Use case description: Explain how you plan to use Amazon SES to send email. Specifically, tell us:
-How do you plan to build or acquire your mailing list?
-How do you plan to handle bounces and complaints?
-How can recipients opt out of receiving email from you?

next day, approved: "Thank you for submitting your request to increase your sending limits. Your new sending quota is 50,000 messages per day. Your maximum send rate is now 14 messages per second. We have also moved your account out of the Amazon SES sandbox." huzzah. with great power comes great responsibility. and Network 23 will use it wisely

											_ _     _  _   
	___ _ __ ___   __ _(_) |   | || |  
 / _ \ '_ ` _ \ / _` | | |   | || |_ 
|  __/ | | | | | (_| | | |_  |__   _|
 \___|_| |_| |_|\__,_|_|_( )    |_|  
												 |/          

[]make a hello world lambda
[]make lambdas that send sms and email
[]secure them so only cold3.cc can call them















	 _                          _                 
	(_)___  __   _____ _ __ ___(_) ___  _ __  ___ 
	| / __| \ \ / / _ \ '__/ __| |/ _ \| '_ \/ __|
	| \__ \  \ V /  __/ |  \__ \ | (_) | | | \__ \
 _/ |___/   \_/ \___|_|  |___/_|\___/|_| |_|___/
|__/                                            

coding with big int literals, nuxt freaked out
it seems like it was building everything to es2019, and those are es2020
also, soon you're going to start using library functions from lambdas, which are node
so it's time to figure out what javascript version you want, and set it explicitly

here's what amazon supports for lambda right now, 2024jul:
https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html
Node.js 20, current
Node.js 18, current
Node.js 16, deprecation date 2024jun12
so from that, picking Node 20

you just updated node and npm on your development workstation:
$ node --version, 20.15.0
$ npm --version, 10.8.1
so that matches well

it seems there isn't a good way to match a node version to an esXXXX version
because platform vendors, like google making v8, implement different features from the spec at different times
this page is detailed, but useless:
https://v8.dev/features

so what version is cloudflare workers running?
https://developers.cloudflare.com/workers/runtime-apis/web-standards/
"The Workers runtime is updated at least once a week, to at least the version of V8 that is currently used by Google Chrome’s stable release. This means you can safely use the latest JavaScript features, with no need for transpilers."
ok, so that means way more recent than 2019 or 2020

$ nuxt build was the simplest command to hit the big int literal error:

	Kevin@ncom3 MINGW64 /.../repot1 (main)
	$ npm run build
	> nuxt build
	Nuxt 3.12.3 with Nitro 2.9.7
	i Building client...

	[nitro]  ERROR  Error: Transform failed with 2 errors:
	C:\Documents\code\code21site\repot1\server\api\myapi.ts:12:8: ERROR: Big integer literals are not available in the configured target environment ("es2019")

(as an aside: this doesn't really make sense because even if it is targeting es2019, shouldn't transplation correctly turn my big int literals into something that es2019 can understand? isn't that the whole point of transplilation?)
this configuration addition fixes it, in /nuxt.config.txt

	export default defineNuxtConfig({
		compatibilityDate: '2024-04-03',
		devtools: { enabled: true },
		nitro: {
			preset: "cloudflare-pages",
			esbuild: { options: { target: 'esnext' } }// <-- new line that fixes big int literal error
		},
		modules: ["nitro-cloudflare-dev"],
	})

so that's cool. "es2020" also fixes it, but you're using "esnext", because:
2020 was still four years ago,
es2023 and es2024 might not be real yet, and
esnext is also a good match for cloudflare's "what's our version? current!" thing



(bookmark)
then add more here about versioning net23 lambdas with babel transpilation to node20








										 _       _           
 _ __ ___   ___   __| |_   _| | ___  ___ 
| '_ ` _ \ / _ \ / _` | | | | |/ _ \/ __|
| | | | | | (_) | (_| | |_| | |  __/\__ \
|_| |_| |_|\___/ \__,_|\__,_|_|\___||___/
																				 

first, you want wrangler and serverless installed locally, not globally
commands to install, version, and uninstall globally:
$ npm install -g wrangler
$ npm list -g wrangler
$ npm uninstall -g wrangler
serverless 4.0.35 was installed globally, uninstalled that to install serverless 3 in project

commands to build up net23 package.json
note that you have to install serverless version 3, as current is 4 but even serverless-webpack, which has 235k weekly downloads, isn't ready for serverless 4
all of these modules are plenty popular, except perhaps for serverless-s3-sync
installing in this order, two of them were already there, but still installing to list in package.json

$ npm install -S aws-sdk                      8 million weekly downloads, added 37 packages

$ npm install -D serverless@3                 1 million, 550 packages

$ npm install -D babel-loader                15 million, 122 packages
$ npm install -D @babel/core                 45 million, this one is already there!
$ npm install -D @babel/preset-env           22 million, 112 packages

$ npm install -D webpack                     24 million, this one is already there!
$ npm install -D webpack-cli                  6 million, 32 packages

$ npm install -D serverless-webpack         235 thousand, 52 packages
$ npm install -D serverless-offline         519 thousand, 85 packages
$ npm install -D serverless-domain-manager  220 thousand, 62 packages
$ npm install -D serverless-s3-sync          32 thousand,  9 packages

as an aside, "$ serverless deploy" works, but doesn't notice changed files in www
you added a script to package.json so "$ npm run www" force syncs everything




(bookmark)
next three scopes:
1 hello world lambda, no custom domain, no webpack
2 lambdas as api.net23.cc
3 webpack and babel
try running local and deployed
can you see the bundle size before and after?
make an api endpoint to run tests, should be fine because local node 20 works great






 _          _ _         _                 _         _       
| |__   ___| | | ___   | | __ _ _ __ ___ | |__   __| | __ _ 
| '_ \ / _ \ | |/ _ \  | |/ _` | '_ ` _ \| '_ \ / _` |/ _` |
| | | |  __/ | | (_) | | | (_| | | | | | | |_) | (_| | (_| |
|_| |_|\___|_|_|\___/  |_|\__,_|_| |_| |_|_.__/ \__,_|\__,_|
																														

# lambda step 1: hello world and local development

notes getting started with lambda
$ npm run local
runs serverless offline, which emulates AWS Lambda and API Gateway
then you can hit your APIs at urls like:
http://localhost:3000/prod/hello1
http://localhost:3000/prod/hello2
live updates don't seem to work, but maybe that's not an intended feature

$ npm run deploy
first time, right now with two lambdas that aren't using a custom domain

# lambda step 2: manual steps to get custom domain

serverless.yml should be able to create api.net23.cc, but you're getting this error:
$ npm run deploy
Deploying net23 to stage prod (us-east-1)
Warning: V1 - 'api.net23.cc' does not exist.
Error: V1 - Make sure the 'api.net23.cc' exists.

aws dashboard, route 53, net23.cc, you see stuff for the root and www, but nothing yet for api.net23.cc
aws dashboard, api gateway, apis, there is a listing named prod-net23, and clicking into it you do see hello1 and hello2

ok, instead of trying to get this one-time configuration to be created by serverless.yml,
you're going to document steps here to set it up manually:

aws dashboard, api gateway, custom domain names
api.net23.cc
tls 1.2
regional (not edge optimized)
net23.cc, choose ACM certificate from drop down, only one there
create domain name button, green success banner

aws dashboard, api gateway, custom domain names, api.net23.cc
configurations, endpoint configuration, api gateway domain name
copy this as you'll paste it into route53 in a moment

aws dashboard, api gateway, custom domain names, api.net23.cc
api mappings, second tab
configure api mappings, add new mapping
api dropdown, one thing listed, what was created by serverless, prod-net23 (REST)
prod, stage, only option
path (optional), leave blank because you want root
save

aws dashboard, route53, hosted zones, net23.cc, create record
api, record name
A, record type, already set to this
switch on Alias
Route traffic to, dropdown with lots of options, Alias to API Gateway API
us-east-1, N. Virginia, region
choose endpoint, dropdown with one item, suggests the domain like blah.execute-api.us-east-1.amazonaws.com we just created
simple routing policy, yes evaluate target heath, leaving these defaults
create records

back in route23 for net23.cc, all your dns records from before are there, along with the new one for api.net23.cc
and it looks like it's directing to, and saying, the under-the-hood domain like:
blahblah.execute-api.us-east-1.amazonaws.com

# lambda step 3: turn off side door access, hard or impossible and gave up

related to the checks below, there's the www | root X http | https checks
the api subdomain means we don't have to worry about www | root here, which is by design
checking http, chrome takes a long time and then reports connection refused
firefox takes a long time, and then redirects to https
so all that looks ok for now

https://api.net23.cc/hello1 ~ works, correctly
https://jkXXXXXXo3.execute-api.us-east-1.amazonaws.com/prod/hello1 ~ works, wrongly, id from api gateway
https://d-1hXXXXXXyh.execute-api.us-east-1.amazonaws.com/prod/hello1 ~ blocked, correctly, value from route53

as always with aws, there are manual steps or you can try automatic
manual steps involve pasting in code
automatic steps involve a secret that includes your aws account number

as both include blocks of json, trying infrastructure as code
added ACCESS_API_GATEWAY_RESOURCE_ARN to .env and section that uses it to serverless.yml

before running, checked the dashboard:
aws dashboard, api gateway, prod-net23
halfway down on the left, resource policy
starting out seeing policy details, no resource policy

ok, spent a few hours going in circles with chatgpt trying to turn off the side door
chat repeatedly went to controlling who can call the function, not where the function is callable
using the referer header, which of course can be spoofed anyway
then got it to explain that aws doesn't have a way to turn off access at the amazonaws.com domain
"Unfortunately, AWS does not provide a direct option to completely disable the default amazonaws.com endpoint for API Gateway. ... Although you cannot completely disable the default domain, you can create a Lambda@Edge function or use API Gateway’s HTTP integration to redirect any traffic from the default domain to your custom domain."
so that's way too much work and complexity for this that likely doesn't even matter, giving up

# lambda step 4: let in webpack and babel to reduce bundle size and use modules

$ npm run pack
$ npm run deploy

pack just runs webpack, building to dist/hello1.js
deploy does that as part of the serverless framework upload to aws lambda
before adding webpack, each hello was 7.1 MB; now serverless framework says it's 157 bytes
on disk from pack its 494 bytes, though, but whatever

also, deploy seems to delete the dist folder!
it's there after pack, gone after deploy

fought an error about ES6 module syntax or not in the webpack configuration
generally, trying to pick ES6 module syntax everywhere because works in the browser and server, more recent and current, and helps tree shaking perform best
right now it's working with these choices:
-package.json includes "type": "module", keeping ES6 setting
-webpack.config.cjs ends .cjs not .js or .mjs, reverting to CommonJS setting
-serverless.yml custom.webpack.webpackConfig references the webpack config with that extension
-insde webpack.config.cjs, require, module.exports, and __dirname are all legacy Node that matches CommonJS
but also, there are two mentions of commonjs2, setting the output format, necessary for webpack to bundle for lambda's node environment

# lambda step 5: three horrible hacky workarounds

here are scripts in package.json:

	"scripts": {
(1) "www": "serverless s3sync"
(2) "deploy": "webpack && serverless deploy"
(3) "local": "serverless offline --config local.yml",

(1) sync the www bucket
serverless deploy should sync the www bucket
except it doesn't
so instead www calls s3sync directly

(2) pack and upload lambda functions
serverless deploy should run webpack and then upload the packed functions
except it like deletes them and then can't find them or something
so intead running webpack manually beforehand works
and npm run deploy does this in a single command that bundles two steps

(3) run lambda functions locally
serverless-offline can emulate api gateway and lambda locally
except somehow it can't find the named handler in the output bundle
so this hack uses a separate yml config that points at the source files
and that way, runs them fine

here are some paths related to all this:

on disk:
./src/hello1.js ~ source code file
./dist/hello1.js ~ bundled webpack output file

in settings in local.yml:
	handler: src/hello1.handler # Paths to src to run before webpack
in settings in serverless.yml:
	handler: dist/hello1.handler # Paths to dist to upload after webpack

in code in ./src/hello1.js:
	"... export const handler = async (event) => {..."
in code in ./dist/hello1.js:
	"...e.d(o,{handler:()=>t});const t=async..."

on the web:
	http://localhost:3000/prod/hello1 ~ path to function emulated locally
	https://api.net23.cc/hello1 ~ path to function deployed  

you've seen subfolders like src/handlers and dist/service appear, but for the moment at least, have managed to keep them away

so, this is working, but hacky as f**k with three workarounds, and you haven't even tested using a module or library code yet, ugh

# lambda step 6: switched to rollup

switched from webpack and babel to rollup, and everything works great
$ npm run local   is "local": "rollup -c && serverless offline",
$ npm run deploy  is "deploy": "rollup -c && serverless deploy",
and the serverless-webpack module is replaced with nothing

here is net23's package.json with rollup, looking at popularity and recency:

serverless                 1 million      1month
serverless-offline           536k         2months
serverless-domain-manager    198k         5months
serverless-s3-sync           31k          6months ~ a little too rare

rollup                        22 million  1month
@rollup/plugin-commonjs        3 million  1month
@rollup/plugin-node-resolve    6 million  9months
@rollup/plugin-terser          1 million  9months
@rollup/plugin-json            2 million  7months
rollup-plugin-node-polyfills     776k     5years ~ a little too old

# lambda step 7: great success

here's what we finished:
[x]hello lambda
[x]develop locally, using serverless framework's emulation of api gateway and lambda
[x]do some math big int style
[x]use a node module directly, nanoid probably
[x]import library code
[x]run all the tests

																						 _      
	__ _  ___ ___ ___  ___ ___    ___ ___   __| | ___ 
 / _` |/ __/ __/ _ \/ __/ __|  / __/ _ \ / _` |/ _ \
| (_| | (_| (_|  __/\__ \__ \ | (_| (_) | (_| |  __/
 \__,_|\___\___\___||___/___/  \___\___/ \__,_|\___|
																										

only cold3.cc should be able to call network 23 apis
in addition to cors and checking the origin header and so on,
the simplest way this is secured is with an api access key
this is just like how we access supabase and other third party apis
except here, we're coding both sides
here's an example:


when cold3 makes a request to net23, it includes this access key
cold3 gets the key from cloudflare secrets
net23 checks the key from lambda environment variables or aws secret manager or whatever

you need to store this secret a lot of places:
-env files for local development, both cold3 and net23
-cloudflare secrets
-lambda's equivalent of cloudflare secrets

.env:
ACCESS_NETWORK_23=VeryLongSecretCodeLikeDsqmB9YxbtseDuUHwyJnjDsqmB9YxbtseDuUHwyJnjAndSoOn

cloudflare:
cloudflare dashboard, workers and pages, cold3, settings tab, environment variables

serverless.yml:
provider:
	environment: # Name variables in .env file here so they are on process.env for the Lambda code
		ACCESS_NETWORK_23: ${env:ACCESS_NETWORK_23}

and then in lambda code like hello2.js:
	let access = 'not found'
	if (typeof process.env.ACCESS_NETWORK_23 == 'string') access = process.env.ACCESS_NETWORK_23.length

you don't have to mess with the aws dashboard, which is great

			 _             
 _ __ (_)_ __   __ _ 
| '_ \| | '_ \ / _` |
| |_) | | | | | (_| |
| .__/|_|_| |_|\__, |
|_|            |___/ 

2024sep setup ping pages and got service providers to hit them and generate nice graphs

cold3.cc/ping/ping1                                            -a nuxt page with no script tag
cold3.cc/ping/ping2                                            -a nuxt page with a trivial script tag
cold3.cc/ping/ping3 > cold3.cc/api/ping3                       -a nuxt page that fetches to a trivial nuxt api
cold3.cc/ping/ping4 > cold3.cc/api/ping4 > Supabase            -all that, but the server api code queries the global count from supabase
cold3.cc/ping/ping5 > cold3.cc/api/ping5 > api.net23.cc/ping5  -instead of calling supabase, fetches to our own trivial lambda

example output, deployed to cloud, curl and browser:

ping1: template says: ping1done (curl)
ping1: template says: ping1done (browser)

ping2: script setup says: CloudPageServer:Envi.Proc.Scri.Self.Serv.Zulu,           1725904851660, v2024sep8b, ping2done (curl)
ping2: script setup says: CloudPageClient:Achr.Asaf.Awin.Docu.Doma.Self.Stor.Wind, 1725904716570, v2024sep8b, ping2done (browser)

ping3: fetch to worker took 0ms to say: worker says: CloudNuxtServer:Aclo.Envi.Proc.Scri.Self.Zulu, 1725904908685, v2024sep8b, ping3done (curl)
ping3: fetch to worker took 2ms to say: worker says: CloudNuxtServer:Aclo.Envi.Proc.Scri.Self.Zulu, 1725904728918, v2024sep8b, ping3done (browser)

ping4: fetch to worker to database took 494ms to say: worker says: database took 494ms to get count 569, CloudNuxtServer:Aclo.Envi.Proc.Scri.Self.Zulu, 1725904949932, v2024sep8b, ping4done (curl)
ping4: fetch to worker to database took   2ms to say: worker says: database took 217ms to get count 569, CloudNuxtServer:Aclo.Envi.Proc.Scri.Self.Zulu, 1725904740498, v2024sep8b, ping4done (browser)

ping5: fetch to worker to lambda took 354ms to say: worker says: lambda took  354ms to say: lambda says: CloudLambda:Eigh.Envi.Glob.Lamb.Node.Proc.Regi.Zulu, 1725904979928, v2024sep8b, ping5done (curl)
ping5: fetch to worker to lambda took   2ms to say: worker says: lambda took 1277ms to say: lambda says: CloudLambda:Eigh.Envi.Glob.Lamb.Node.Proc.Regi.Zulu, 1725904754597, v2024sep8b, ping5done (browser)

notes making sense of these results
ping2, curl is from server rendering, so zulu time, and browser replaces that with client rendering, which has a window object
ping3, server render adds 0ms because it's all happening at once; senseEnvironment the same because it's always from a fetch to the nuxt api
ping4 and ping5, real times are on curl page, when re-rendered on the client it must cache or skip or something so you get 2ms instead of nearly half a second
and while supabase takes a half second, lambda frequently takes over a second

looked for service providers with these capabilities:
-simple request, don't emulate a rendering, code-running browser
-try from all over the world, randomly
-look for text in the page that you get
-say and show how long it took, average and higher percentile

trying out:
-pingdom.com - västerås now owned by solarwinds in austin, $33/month, strange duplication of uptime and speed, graphs good
-checklyhq.com - berlin, $0 hobby plan, graphs great, nicely formatted html result view
-uptime.com - nyc, $20/month, graphs ok
so you'll probably just go with checkly
they talk about complex stuff with playwright, but you found their simple thing

for each of their dashboards, you found and set:
-name
-url
-look for text like "ping5done" in page
-frequency, 1hr
-round robin or lots of random global locations

it'll be interesting to see if this affects your cloudflare and amazon bills :0
and this was a good trail, other than senseEnvironment() being a messy day in the bike shed

      _                 _    __                  _   _             
  ___| | ___  _   _  __| |  / _|_   _ _ __   ___| |_(_) ___  _ __  
 / __| |/ _ \| | | |/ _` | | |_| | | | '_ \ / __| __| |/ _ \| '_ \ 
| (__| | (_) | |_| | (_| | |  _| |_| | | | | (__| |_| | (_) | | | |
 \___|_|\___/ \__,_|\__,_| |_|  \__,_|_| |_|\___|\__|_|\___/|_| |_|
                                                                   
                       _ _      _ _          _   _                                       
 _ __   __ _ _ __ __ _| | | ___| (_)______ _| |_(_) ___  _ __   __      _____   ___  ___ 
| '_ \ / _` | '__/ _` | | |/ _ \ | |_  / _` | __| |/ _ \| '_ \  \ \ /\ / / _ \ / _ \/ __|
| |_) | (_| | | | (_| | | |  __/ | |/ / (_| | |_| | (_) | | | |  \ V  V / (_) |  __/\__ \
| .__/ \__,_|_|  \__,_|_|_|\___|_|_/___\__,_|\__|_|\___/|_| |_|   \_/\_/ \___/ \___||___/
|_|                                                                                      

in the eternal triangular tug of war between simplicity, reliability, and speed
you've decided for initial launch to keep cloud functions synchronous
what follows is a discussion of fancier things you tried, and why they were difficult, worrysome, or didn't work at all

[I.] you can fetch in parallel in your own code while handling an api request

these will run one after the other:

await fetch('https://datadog.com/api/log', ...)
await fetch('https://datacat.com/api/log', ...)

and these will run both at the same time:

let promiseDog = fetch('https://datadog.com/api/log', ...)
let promiseCat = fetch('https://datacat.com/api/log', ...)
await Promise.all([promiseDog, promiseCat])

and that code is fine in workers and lambda
the difficulty comes in when you have everything you need to get back to the client,
and want to do that right away, with cleanup tasks like logging happening in parallel or afterwards
it doesn't work because cloudflare and lambda see the request sent, think we're done here, and tear down the environment

[II.] performing tasks in a worker after responding to the client is possible but more complicated

cloudflare pins a method to the event object:

event.waitUntil(p)

you can pass it a promise, and then the worker will delay teardown,
even after you've responded to the client and returned from everything
so that's great. there is no lambda equivalent you can find

[III.] performing tasks in a lambda after responding to the client is either not possible or so rare you can't find it

the return from a lambda handler is the response back up to the client
you can't find a way to talk to the client before returning

the common pattern for lambda functions starts like this:
	exports.handler = async (event, context) => {

and an alternative documented form that looked promising (get it?) is this:
	exports.handler = (event, context, callback) => {

but to do what you want, you'd have to combine them,
which either won't work or is so weird no one else has thought of it:
exports.handler = async (event, context, callback) => {

	//first, we process the request, using steps that we do have to await
	let messageForClient = await processRequest({event, context})

	//second, we get back to the client quickly
	const response = {
		statusCode: 200,
		body: JSON.stringify({ message: 'OK', messageForClient }),
	}
	callback(null, response)

	//third, perform tasks like logging. the response has been sent, but the lambda must keep running!
	let p = performLoggingTasks()
	return p//by returning this promise, we're telling lambda to not tear down until it resolves!
}

looking more shed even more doubt on this--the returned promise p needs to resolve into the response body
at which point we're all the way back where we started

chat says lambda doesn't even support streaming a response back to a client
the whole thing, headers and complete body, needs to go out once at the end

and there's also something called context.succeed() as an alternative to callback
but it seems like that's an older api that has been replaced by the callback parameter
not a different thing that could help you here

and hours later you found callbackWaitsForEmptyEventLoop, which is already set to true
which could keep the lambda alive long enough to finish logging
but might also delay sending a returned response back to the client

so, you found a lot of complexity and very little certainty in this exploration, and are backing out

[IV.] so here are some features you're removing

const defaultFetchTimeLimit = 5*Time.second
async function ashFetchum(c, q) {//early nickname to wrapped fetch; gotta fetch 'em all
	if (!q.timeLimit) q.timeLimit = defaultFetchTimeLimit
	let response, bodyText, body, error, success = true

	let a = new AbortController()//js way of telling fetch to give up after a time limit
	let m = setTimeout(() => a.abort(), q.timeLimit)
	let o = {method: q.method, headers: q.headers, body: q.body, signal: a.signal}
	q.tick = Now()

	try {
		if (q.skipResponse) {
			let p = fetch(q.resource, o)             //get the promise instead of awaiting here for it to resolve
			if (event?.waitUntil) event.waitUntil(p) //tell cloudflare to not tear down the worker until p resolves
			p.then(() => { clearTimeout(m) })
		} else {
			response = await fetch(q.resource, o); clearTimeout(m)

in your wrapped server side fetch, there used to be
-a fire and forget option to skip waiting for the response entirely; the call isn't even async anymore
-an option to skip reading the body
-an upper time limit imposed by an abort controller

to provide the fire and forget feature, you'd have to save the promise and
on cloudflare, feed it to event.waitUntil()
on lambda, manually await it before returning
this is possible, but far less simple and likely less reliable,
either because you make a coding error due to the more complex flow
or because it causes cloudflare or amazon to act in an unusual way
also there's no benefit on lambda, only cloudflare

you could do the option to not read the body, but
the body is probably short,
has probably already arrived,
and you probably always want to log it to have a full audit of api behavior

abort controller is is a feature of fetch, and interesting, but
workers have a not configurable 30 second maximum lifetime, and
you've set your lambdas to match, from a default of 3 seconds and upper limit of 15 minutes

[V.] the big surrounding picture here

you're picking servlerless offerings like cloudflare workers, lambda, and supabase for simplicity, reliability, and scalability
you don't have to maintain a server, and if things get popular overnight, nothing should break

but this also means that every read, write, and log is another ~150ms
many user interactions will involve a handful of these, back to back
and that means the experience isn't fast anymore

a traditional architecture--with web server, database, and log file all on the same virtual instance--avoids this problem
also on a regular server, you can absolutely stream a response piece by piece back up to a client
and you can finish a response, and stay alive to perform some clean up tasks afterwards

your hope is that the site still seems remarkable to users even with some steps causing second-long pauses
the first navigation will still happen in like 35ms
clicking around will be instantaneous because vue is just showing and hiding components that already have cached data
and when the user is changing their email or uploading a file, it makes sense that those tasks would take seconds

[VI.] the next day, a map if you get back to this trail

maybe this would work for lambda, or maybe its an ai hallucination, you're not sure:
exports.handler = async (event, context) => {
	context.callbackWaitsForEmptyEventLoop = false//change away from default true
	setTimeout(() => {//do 0ms later, but in the next turn of the event loop
		console.log("Performing background task like logging...")
	}, 0)
	return {//return response to client right away
		statusCode: 200,
		headers: { 'Content-Type': 'application/json' },
		body: JSON.stringify({ message: 'Response to client' })
	}
}

and then on the cloudflare side, it's context.waitUntil(), and you just found tail workers:
https://developers.cloudflare.com/workers/observability/logging/tail-workers/
specifically designed for logging and https://developers.cloudflare.com/queues/ is in beta

also, you can call console.log and console.error in both
in lambda, they automatically go into a cloudwatch log you can see in the dashboard

[VII.] the next month, reports from the field


with lambda, it never works
with cloudflare, it works most of the time, but there are missing logs
so, for reliability you also get simplicity, and never touch this


to eliminate setWorkerEvent entirely, also load the secrets explicitly
just load their names, from process.env
this also works on worker and lambda same code
because both can dereference them
even if cloudflare can't list them


simplicity and reliabi

no, here's the plan
simple, isomorphic, reliable

instead of listing secrets from process or worker event
parse what you need to redact into words, look for words that end _SECRET, then dereference them from process

and here's how you make dog and all logs reliable but not await async
implement your own promise wait until
and then wait on it before returning the result from the handler

reliability should be unaffected
things are slightly faster because fire and forget events can proceed in parallel
although the response is still delayed by the longest one, it's not delayed by the sum duration of all of them

thre is that theoretical problem that this keeps lambdas, called quickly, from ever returning
so maybe build in a tick of first addition, and if that's too old, just return--like four seconds too old
and also put in the alert for detected overlapping lambda or worker, put that in, too

cloudPromise(p)

dog
awaitDog
logAlert
awaitLogAlert
















                      _     _   _                  _             _   
 _ __ ___ _ __   ___ | |_  | |_| |__   ___   _ __ | | __ _ _ __ | |_ 
| '__/ _ \ '_ \ / _ \| __| | __| '_ \ / _ \ | '_ \| |/ _` | '_ \| __|
| | |  __/ |_) | (_) | |_  | |_| | | |  __/ | |_) | | (_| | | | | |_ 
|_|  \___| .__/ \___/ \__|  \__|_| |_|\___| | .__/|_|\__,_|_| |_|\__|
				 |_|                                |_|                      





quick repot1

$ npm create cloudflare@latest
repot1
website or web app
nuxt
yes git for version control
yes deploy to cloudflare
(quickly get the right chrome profile forward, and already signed in to cloudflare there!)
yeah it's up at
https://repot1.pages.dev/

$ npx nuxi add mypage
$ npx nuxi add mycomponent
$ npx nuxi add myapi










										_            _       
 _ __ _   _ _ __   | |_ ___  ___| |_ ___ 
| '__| | | | '_ \  | __/ _ \/ __| __/ __|
| |  | |_| | | | | | ||  __/\__ \ |_\__ \
|_|   \__,_|_| |_|  \__\___||___/\__|___/
																				 

tiny tests, in library0.js, exports test(), ok(), and runTests()
all calls to test() are in the library
and, as much as possible, factor all *code* into the library as well!
pages, components, and api handlers should be really short

run tests in...

(1) Node 20
$ cd ./library
$ node test.js
test results are logged out

(2) Vite
$ cd ./icarus
$ npm run icarus
and then see test results on the home route

(3) Nuxt
$ cd ./
$ npm run local
and then see test results on the route /tests, TODO!

(and add more)
nuxt really is local/deployed x client/server--can you run tests in all four environments?
lambda serverless framework is local/deployed--run in those two environments













																				_            
 ___  ___ _ __ __ _ _ __    _ __   ___ | |_ ___  ___ 
/ __|/ __| '__/ _` | '_ \  | '_ \ / _ \| __/ _ \/ __|
\__ \ (__| | | (_| | |_) | | | | | (_) | ||  __/\__ \
|___/\___|_|  \__,_| .__/  |_| |_|\___/ \__\___||___/
									 |_|                               


(This page intentionally left blank.)





2024oct8
after updating wrangler, you got supabase working again
in the cloudflare dashboard
cloudflare dashboard, workers and pages, cold3
there's an Integrations tab, on it are a bunch of third party services in boxes, including Supabase
you went through this flow
you let cloudflare oauth into your supabase account
at the end, it had made two new cloudflare secrets
and all of that was unnecessary--you can just copy and paste the endpoint url and api key from supabase yourself
you deleted the integration, and supabase still works fine
so this note is for, in the future, if you're fixing supabase again, the integrations tab probably won't help







beyond aws services, another nice thing about net23 is that
if you do run into some node module or third party service
that you cannot get working in a cloudflare worker
then you just call out to a lambda at net23 which does it there
and you've got that capability at easy reach
without making every page load slow and expensive




2024aug30 turn off default email tracking pixel in sendgrid
https://app.sendgrid.com/settings/tracking
dashboard, settings, tracking
-Enabled; Open Tracking; An invisible image is being appended to HTML emails to track if they have been opened.
-Enabled; Click Tracking; Every link is being overwritten to track every click in emails.
-Disabled; Subscription Tracking; Allows every link to be overwritten to track every Subscription in emails.
-Disabled; Google Analytics Tracking; Allows tracking of your conversion rates and ROI with Google Analytics.
turned them all off








your stupid goal of total isomorphism
like, every environment can include and call into every library file
you accomplished this by doing two things:

(1) lazy and on demand loading, following this pattern

let _twilio,
async function loadTwilio() {
	if (!_twilio) _twilio = await import('twilio')
	return _twilio
}

and then the idea is that only code that should be able to use twilio calls down in here
and if code erroneously does, then you just get a run time exception
this alone should be enough, but it's not
nitro sees the mention, and freaks out with a build time error

chat suggested nuxt.config.ts vite.build.rollupOptions.external
but this is for hiding nuxt server only modules from nuxt client
not hiding not for nuxt at all modules

you previously somehow accidentally had aws-sdk as a cloud3 development module
and this is a working solution:

(2) install as a development dependency
here are your root nuxt package.json, and serverless framework lambda net23/package.json

./package.json
	"dependencies": {
		"@supabase/supabase-js": "^2.39.8",
	},
	"devDependencies": {
		"@sendgrid/mail": "^8.1.4",
		"aws-sdk": "^2.1691.0",
		"twilio": "^5.3.4",
	}

./net23/package.json
	"dependencies": {
		"@supabase/supabase-js": "^2.39.8",
		"@sendgrid/mail": "^8.1.4",
		"aws-sdk": "^2.1691.0",
		"twilio": "^5.3.4"
	},

both actually use supabase
amazon, twilio, and sendgrid have to be listed in both
but you hide these three in devDependencies just to calm nuxt down

and your hope is that it won't get bundled, but you're not sure about this
chat suggets 
$ npm install -D rollup-plugin-visualizer
$ npm install -D source-map-explorer
as tools to do this

(3) if that hadn't worked
you considered, and probably still should, make a separate library file:
./library/lambda.js
not imported by grand; not imported by nuxt
and then the nuxt package.json doesn't install amazon, twilio, sendgrid modules at all
imported directly by the lambda endpoints, alongside their imports of library stuff through grand
it's possible this will make the nuxt build faster
if you run into problems in this area again, quickly switch to this more straightforward design

note also this doesn't mean you make a library file called worker.js alongside lambda.js
lambda.js is by itself because it's where you can install and use any node module

oh, the note entrypoint might mean that you separate the 
when looking at real monorepo options, you tried moving the nuxt project into ./site/
but cloudflare didn't know what to do with it
so instead you could keep nuxt as the root, and move the pure node tasks into ./library
so then there's a new ./library/package.json













2024oct24

you've given up trying to get node modules like twilio working in a worker
none of these node modules were written imagining a web worker future
and their service providers don't mention web workers at all
even if you got one to work, it might be less reliable in a worker

also, even though javascript is full stack, there are big differences between client and server modules, tools and patterns
client js is all about es6, include, tree shaking, and small modules with few dependencies
server js is all about commonjs, require, node, and big modules with hundreds of dependencies
building server modules for the client is a neat demo for a meetup, but not a fruitful practice for a workday

so, you'll just try the lambda first now, instead of last
thank the bezos lambda exists--otherwise you'd be securing a virtual instance with chatgpt right now!

and even with this hybrid model, cloudflare is still great at:
-loading really fast, including as a cold start to make a good first impression to a brand new prospective user
-running trusted code that can't be tampered with
-using supabase, with their module, and datadog, with fetch
don't try to get it to do more than that

this hybrid model still keeps all the application logic in the worker
the lambda code is just "send this email" or "upload this file"--individual atomic commands,
and notes in database tables as to what happened afterwards










2024nov4

you've switched to yarn, using yarn 1.22.22 on both mac and windows
deleting and then git ignoring yarn.lock, which isn't standard
but as you checkout and build on mac, windows, and linux (through docker)
you want yarn to get stuff specific to that new platform, not a yarn.lock generated on a different os









   ____  _                    __              _ _                  
  / __ \(_)_ __ ___   __ _   / _| ___  _ __  | (_)_ __  _   ___  __
 / / _` | | '_ ` _ \ / _` | | |_ / _ \| '__| | | | '_ \| | | \ \/ /
| | (_| | | | | | | | (_| | |  _| (_) | |    | | | | | | |_| |>  < 
 \ \__,_|_|_| |_| |_|\__, | |_|  \___/|_|    |_|_|_| |_|\__,_/_/\_\
  \____/             |___/                                         


network 23 needs to do some image manipulation tasks, like:
-convert an open graph protocol card from svg generated by nuxt to png so whatsapp can display it
-resize and image and add watermark text
we've found the node module sharp to do this
while sharp is a node module, it's fast because native libraries do the image processing

getting yarn to install the right libraries for amazon linux was challenging
after long journeys on many dead-end trails sherpa'd by chatgpt, we found two working solutions:
(1) a "stowaway" method, detailed below
(2) a clean build in Docker, detailed in the next section

the stowaway method:

manually chuck the linux binaries into net23's node modules @img folder
yarn add, remove, install will find them and chuck them overboard,
so after running yarn, so you have to stow them away again
serverless.yml's exclude keeps the mac or windows binaries out of the zip,
but lets the stowaways through

more info and example commands in net23/stowaway.mjs


     _            _             
  __| | ___   ___| | _____ _ __ 
 / _` |/ _ \ / __| |/ / _ \ '__|
| (_| | (_) | (__|   <  __/ |   
 \__,_|\___/ \___|_|\_\___|_|   
                                
to be able to get the files to stow away, you learned docker
and, docker will always be the most correct, clean, and secure way to build and deploy
perhaps the stowaway method will break,
or some new node module will need to actually compile binaries for amazon linux
if so, you've got docker ready to (again) save the day

here are the commands you learned to use docker generally,
and build in amazon linux to make a serverless package with a sharp that works on lambda
dollar sign on the left is your host computer; hashtag indented are commands inside the container

==== docker setup

$ brew update
$ brew install --cask docker

$ docker --version
$ docker-compose --version
$ docker run hello-world

==== docker cleanup

$ docker images
$ docker rmi image9
$ docker ps -a
$ docker rm container9

==== docker setup

$ docker run -it --platform linux/amd64 --name container23 amazonlinux:2023 /bin/bash
$ exit
$ docker start -ai container23

macmini% uname -m, confirm arm64
bash-5.2# uname -m, confirm x86_64 (by default, docker will match host architecture, and yarn will get the same wrong native sharp libraries! but also: you should probably have started with the new one, amazon graviton processors, which are arm!!)

		# yum update -y
		# yum install -y which git tar zip
		# curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.5/install.sh | bash
		# source ~/.nvm/nvm.sh
		# nvm install 20
		# npm install -g yarn@1

		# mkdir code
		# cd code
		# git clone https://github.com/zootella/cold3
		# cd cold3
		# git checkout auto1
		# rm yarn.lock
		# yarn install

		# node --version
		# yarn --version
		# exit

$ docker cp ../.env           container23:/code/cold3/.env
$ docker cp ../.env.local     container23:/code/cold3/.env.local
$ docker cp ../card.js        container23:/code/cold3/card.js

		# git pull
		# yarn install
		# yarn seal
		# yarn test
		# cd net23
		# yarn build

$ rm -rf .serverless
$ docker cp container23:/code/cold3/net23/.serverless .serverless
$ docker cp container23:/code/cold3/wrapper.txt ../wrapper.txt
$ docker cp container23:/code/cold3/icarus/wrapper.js ../icarus/wrapper.js

$ yarn justdeploy (have serverless deploy the .serverless folder without building it first)
$ cd ../site (then also deploy the nuxt site to cloudflare)
$ yarn deploy
$ cd ..
$ git status (and commit and push to github the sealed version you deployed)

==== docker loop

$ docker start -ai container23

		# source ~/.nvm/nvm.sh
		# nvm install 20
		^when you go back in, you have to repeat just these two; afterwards yarn is there

		# cd /code/cold3
		# git stash
		# git stash clear
		# git pull

		# yarn install
		# yarn seal
		# yarn test
		# cd net23
		# yarn build

$ rm -rf .serverless
$ docker cp container23:/code/cold3/net23/.serverless .serverless
$ docker cp container23:/code/cold3/wrapper.txt ../wrapper.txt
$ docker cp container23:/code/cold3/icarus/wrapper.js ../icarus/wrapper.js
		# exit

$ yarn justdeploy
$ cd ../site
$ yarn deploy
$ cd ..
$ git status (and so on to commit and push)

==== docker end

from this experimentation and use, here's what you've got on your mac:

% docker images
REPOSITORY    TAG       IMAGE ID       CREATED       SIZE
amazonlinux   2023      5bf4cf420ef7   2 weeks ago   214MB
% docker ps -a
CONTAINER ID   IMAGE              COMMAND       CREATED        STATUS                    PORTS     NAMES
c65859f86d9a   amazonlinux:2023   "/bin/bash"   22 hours ago   Exited (0) 17 hours ago             container23
0c25625e7aaf   amazonlinux:2023   "/bin/bash"   30 hours ago   Exited (0) 29 hours ago             layer1

enter container23 to do the loop tasks above
the other container, layer1, is where you made lambda layers for sharp and twilio that you could not get working

using these notes, you were building the zip inside docker,
then copying it back out, and deploying it from the host computer
this is because you didn't authenticate the container to be able to deploy directly
but if you return to docker later to do clean secure deployments, you should
and it shouldn't be hard to get github, cloudflare, and amazon authenticated
so then you'd seal and deploy and commit all from inside the container

chat kept on encouraging you to make a Dockerfile,
but you just used it manually like a virtual computer
you also worked on scripting the loop tasks, using a bash .sh script
the hard part there was being able to tell on the outside when the build on the inside finished
you planned a little done.txt file inside that outside would watch
you never ran the script draft now that stowaway is the fast way, and all inside docker is the good way

                _     _ _            _                  
  __ _ _ __ ___| |__ (_) |_ ___  ___| |_ _   _ _ __ ___ 
 / _` | '__/ __| '_ \| | __/ _ \/ __| __| | | | '__/ _ \
| (_| | | | (__| | | | | ||  __/ (__| |_| |_| | | |  __/
 \__,_|_|  \___|_| |_|_|\__\___|\___|\__|\__,_|_|  \___|
                                                        
starting that over, you probably should have picked arm64 for lambda instead of x86_64
arm may have slightly longer cold start times

x86_64 (Intel/AMD) the default you're using (but not specifying--you could specify it explicitly)
ARM64 (AWS Graviton) 20-30% quicker cold start time, 20-40% better performance for multi-threaded tasks, 20-30% cheaper monetary cost

you can specify architecture in serverless.yml:
provider:
  name: aws
  runtime: nodejs20.x
  architectures:
    - arm64

and detect it in the javascript of the handler:
process.arch
process.platform
process.version//node version

make a docker container to match it and install modules to deploy:
$ docker run -it --platform linux/arm64 --name container23arm amazonlinux:2023 /bin/bash

                  _    
 _ __   __ _  ___| | __
| '_ \ / _` |/ __| |/ /
| |_) | (_| | (__|   < 
| .__/ \__,_|\___|_|\_\
|_|                    

you had configured a custom rollup bundle step for the lambda functions,
but just removed it
it wasn't working--it was packing the library code, which is small to begin with
and then ignoring the node modules folder, which serverless would zip up entirely

also, with it gone, you can start the local server nearly instantly,
and error stacks have findable line numbers

you got net23.zip from 50mb down to 20mb just by chasing away aws-sdk as a dev dependency,
and clumsily, line item excluding the largest node modules that came in as icarus dev dependencies

but, you are still drawn to the JavaScript dream--that since all code can run everywhere,
and transpilation can turn anything into anything,
and tree shaking can remove entire modules and individual functions in a file,
everything is awesome

<rant>
none of that worked, however--
there have been days of fighting to get CommonJS and ESM modules to work with each other,
weaving the boundary means that you have to await import, spreading async upwards and everywhere,
modules like twilio written for require() don't work well when you load them not with require(),
jimp, pure javascript, is prohibitively slower than sharp, which is all native,
and even though cloudflare has a node compatibility flag,
that absolutely does not mean that a worker can load or run a node module
</rant>

but, you could still mess around with the code that gets zipped for lambda
if you do, these are the tools that look promising:

https://parceljs.org/
2017, zero configuration, can struggle with complex mixtures of commonjs and esm

https://www.npmjs.com/package/serverless-bundle
2018, for serverless framework, uses webpack, may handle mixture well

https://esbuild.github.io/
2020, extremely fast, should be able to handle the mixture










                                                _     
  ___  _ __   ___ _ __     __ _ _ __ __ _ _ __ | |__  
 / _ \| '_ \ / _ \ '_ \   / _` | '__/ _` | '_ \| '_ \ 
| (_) | |_) |  __/ | | | | (_| | | | (_| | |_) | | | |
 \___/| .__/ \___|_| |_|  \__, |_|  \__,_| .__/|_| |_|
      |_|                 |___/          |_|          

you got open graph images working by adding nuxt-og-image:
https://www.npmjs.com/package/nuxt-og-image
https://nuxt.com/modules/og-image
https://nuxtseo.com/docs/og-image/getting-started/introduction

dashboard clicking steps were:
cloudflare dashboard, workers and pages, KV, create a namespace
OG_IMAGE_CACHE, name
then the dashboard generates an id, pasted that into wrangler.toml

this was enough to get image caching working; you can see them in the dashboard
keys have nuxt og image module version:route:not sure, like:
3.0.8:index:SQxRoEWOgn
3.0.8:card:whatever-you-put-here:a04axs96VI
values like:
{
	"value":"iVBORw0KGgoAAAANSUhEUgAABLA... (whole thing is 159 KB when downloaded) ...NW84AAAAASUVORK5CYII=",
	"headers":{
		"Vary":"accept-encoding, host",
		"etag":"W/\"fvJ5qGVPK5\"",
		"last-modified":"Sat, 09 Nov 2024 02:39:05 GMT",
		"cache-control":"public, s-maxage=604800, stale-while-revalidate"
	},
	"expiresAt":1731724745242
}

urls like:

http://localhost:3000/__og-image__/image/og.png
http://localhost:3000/__og-image__/image/og.svg

https://cold3.cc/__og-image__/image/og.png
https://cold3.cc/__og-image__/image/og.svg (deployed, svg doesn't work; not sure why but doesn't matter)

still to investigate further:

how is this actually working? it's amazing that it doesn't need a lambda after all!
https://github.com/vercel/satori renders the SVG, and then
https://github.com/thx/resvg-js converts SVG -> PNG, all on the worker

how does the cache expiration work?
if a post is very popular, you want to deliver the card instantly to thousands of followers
but also, if a user edits their post, they want the card to immediately show the edited information

and you've tried the ?purge flag, like
https://cold3.cc/__og-image__/image/og.png?purge
but it doesn't always regenerate the image

you've set cards with defineOgImageComponent() in index.vue and card/[more].vue
but what if someone sends a link to cold3.cc/settings
you want the same card as the home page
can you define a global card, which then gets overridden with a user page or post card?












										 _                       
 _ __ ___   __ _  __| |_ __ ___   __ _ _ __  
| '__/ _ \ / _` |/ _` | '_ ` _ \ / _` | '_ \ 
| | | (_) | (_| | (_| | | | | | | (_| | |_) |
|_|  \___/ \__,_|\__,_|_| |_| |_|\__,_| .__/ 
																			|_|    

== high level

>milestone 1, done

domain name
ssl certificate

www bucket
www distribution
www upload

>milestone 2

api functions
only callable from https://cold3.cc
that use imports
and library code
and survived webpack tree shaking

>milestone 3

vhs bucket
vhs distribution
vhs lambda@edge un-get-round-able gatekeeper
so also only callable from cold3

>milestone 4

send an email
send a sms
