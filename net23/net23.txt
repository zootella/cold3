
 _   _      _                      _      ____  _____ 
| \ | | ___| |___      _____  _ __| | __ |___ \|___ / 
|  \| |/ _ \ __\ \ /\ / / _ \| '__| |/ /   __) | |_ \ 
| |\  |  __/ |_ \ V  V / (_) | |  |   <   / __/ ___) |
|_| \_|\___|\__| \_/\_/ \___/|_|  |_|\_\ |_____|____/ 
																											

Network 23 ~ "Beta to the Max!" -ChatGPT

 _________________________________
/\                                \
\_| --y'know what? There's only   |
	| four things we do better than |
	| anyone else:                  |
	|                               |
	|   music                       |
	|   movies                      |
	|   microcode (software)        |
	|   high-speed pizza delivery   |
	|   ____________________________|_
	 \_/______________________________/

some notes building the megamultimedia empire:
       _      _                   
__   _(_)_ __| |_ _   _  ___  ___ 
\ \ / / | '__| __| | | |/ _ \/ __|
 \ V /| | |  | |_| |_| |  __/\__ \
  \_/ |_|_|   \__|\__,_|\___||___/
                                  
start out up here with design document
the requirements i set, the choices i made, and the reasoning behind them
ttd october2024, but notes follow

values, requirements

in order, they are
1 reliable: never down, not for five minutes or five users
2 simple: written at boot camp level, all the code fits on a floppy disk
3 fast: most user tasks complete in the blink of an eye

serverless, because a server would mean you have to
restart it when it goes down
update it
secure it, know when it's been compromised, and recover from such an attack
pay too much when it's too big (cinderella's problem)
change configuration and software when it's too small (evil stepsisters' problem)

code, best to worst
pure javascript
npm modules with millions of weekly downloads
npm modules with hundreds of thousands of weekly downloads


factored downwards
biggest: .js files in the library
smallest: .vue files

100% isomorphic
the exact same function checkEmail() corrects user input on the page,
and keeps malformed data from being written to the database
execution from ten different environments could call into any file
local/deployed x
node/nuxt/serverless framework/vite tdd runner icarus
cloudflare workers/amazon lambda
page/server api
page code pre-rendering on server/page code rendering on client




we love vue. we hate react
jsx lets you mix everything, invalidating the wonderous code separation of the web stack
react wins when it's complex enough professionals base their careers around it
we win when it's simple enough we're focused on features and users, not platform patterns and what's new this year


configuration as code
do as much configuration in computer-readable as possible
where that's not possible, configuration is in english, step by step, in configuration.txt
and be wary of very difficult configuration code being replacable by a pretty simple and one time step by step



and the requirements list you presented to rf goes here, too
and before that, the slides for sp



Requirements
1. Simple, full-stack JavaScript
2. Really fast: <200ms
3. Really cheap: <$5/month
4. Minimal vendor-specific code
5. Ready for millions of users overnight
6. Secure even from sophisticated attacks





computer and software are made of instructions
since the dawn of software, there have been three kinds of instructions:

I. configuration (mental, or readable)
II. configuration as code (machine executable)
III. code (machine executable)

traditionally, instrutions in category I are kept and versioned in the human brains of the development team
the rule here is that can't be the only place: all instructions in category I must also be codified here, in human readable if not machine executable form
the test is: could you hand this file to a brand new team member, and could they use it to set everything up, without asking your human brain any questions

the second value related to this is to keep instructions as far down that list as possible
code is better than configuration as code
configuration as code is better than configuration (this idea has gained recent popularity in software engineering)

but exceptions to this guideline exist!: like maybe all the domain name stuff in serverless.yml
it took a week to craft screenfuls of potentially brittle configuration
and maybe all that could be done just once at the start of the project, and documented here, as a few human-readable instructions on what to click on amazon's website dashboard




When selecting an open-source component from a leaderboard of popular options, a compelling pattern emerges: the second-most popular option often strikes the best balance. These components are widely adopted enough that AI tools and community resources can reliably address questions, yet they frequently benefit from more modern construction than their top-ranked counterparts. Their designs tend to emphasize simplicity and efficiency, often making them smaller and easier to work with.

The contrast in positioning is also notable. The #1-ranked component often commands its spot by framing itself as the "industry standard" or "default choice," appealing to developers with its dominance and perceived authority. In contrast, #2 typically targets users who are dissatisfied with complexity, positioning itself as a simpler, more approachable alternative. This focus on ease of use can lead to faster adoption and smoother integration, especially for teams valuing efficiency over exhaustive feature sets.

there's even a cheap cosmetic beneift: unstyled or default-styled #1 looks looks generic because it is widely recognized as an unmodified default. unstyled or default styled #2 can be misinterpreted as a custom look for the app!

example:
instead of react, vue
instead of videojs, vidstack formerly plyr






# NPM Module Selection Guidelines

## The #2 Rule: Why Second Place Often Wins

When selecting from popular open-source options, the second-most popular choice frequently offers the best balance for our needs. These modules are mainstream enough that AI tools and community resources can reliably help with questions, yet they often benefit from more modern architecture than the top-ranked alternatives. The #1 option typically positions itself as the "industry standard" or "default choice," while #2 usually targets developers seeking simplicity—resulting in cleaner, more efficient designs that integrate faster.

There's even a cosmetic benefit: unstyled #1 components look generic because they're widely recognized as unmodified defaults, while unstyled #2 components can appear custom-designed for your app.

**Examples**: Vue over React, Vidstack (formerly Plyr) over Video.js

## Download Volume: Mainstream but Not Bloated

We target modules with hundreds of thousands to low millions of weekly downloads—popular enough to be well-supported, but not so dominant that they've accumulated years of legacy bloat. Modules with only 5-digit download counts risk leading us into dependency hell with niche, potentially abandoned packages.

## Isomorphic Compatibility: Browser-First Wins

Our Icarus library runs everywhere JavaScript can run: Cloudflare Workers, AWS Lambda, browsers, and local development environments. This creates a critical selection criterion: **modules designed for browsers typically work everywhere, while Node-specific modules often fail on Cloudflare Workers**—even with Node compatibility flags enabled.

We prioritize small, simple, recently-authored modules over older packages built when Node and browser environments were distinctly separate ecosystems. When service providers force us to use bloated, Node-specific SDKs (like Twilio's API), we retreat to Lambda rather than waste time fighting compatibility issues on Cloudflare.

**Rule of thumb**: If it was designed browser-first, it'll work in our full stack. If it was designed Node-first, it probably won't work on Cloudflare, regardless of what the documentation promises.



















that's one of three module aphorisms:
- pick the second most popular module for the task
- find a module, unless you could code it yourself in just a screenful
- don't try to get node modules to work in a worker

on the third one:
workers are great at running client-side modules, but trusted!
javascript touts isomorphism, but the dream is not quite real yet
so if a module was written imagining it would be run on a server in node
don't spend time trying to fit it into a worker
isntead, just get it in persephone in the lambda, which will work, first try








the floppy disk fill percent, as a metric, is designed to measure developer attention-load
any files that a human could need to read or write are included: for instance, package.json is included, while yarn.lock is not
it's not a measure of how little code must be delivred to the user: for that, radio brotli in stats.html
"you say government should be small," asked a voter to a libertarian. "well then, *how* small should government be?"
"small enough to drown in a bathtub," answered the candidate
like a calf on a farm, first, you can kill it, but just weeks later, it can kill you
software projects brag about a million lines of code, an expert team of a dozen crafting for three years--these should be criticisms, not compliments
"small enough to fit on a floppy disk" means that
if everything needed to be reviewed (such as, for security), it wouldn't be that difficult
if everything needed to be rewritten (such as, for porting to a new platform), it wouldn't take that long
and you could do these things quickly, and with a small team

the goal here is not to write a lot of code, or to write complex or inventive code
the goal is to complete the user stories and epics
writing a small amount of simple code
to keep that code standard and common, avoiding, except where necessary, extra modules
avoiding, except where absolutely necessary, platform or provider specific details, and in those cases, factoring them away

"a short script that fits on a floppy disk"



net23.cc core engineering values, like a scout's law, also like how Amazon was almost named Relentless
- Speed (Fast) - most user interactions happen in the blink of an eye or faster
- Reliability (Reliable) - near 100% uptime
- Security (Secure) - standard description
- Scalability (Scalable) - if overnight use grows 10x, in the morning the team has bigger numbers on a dashboard, not error messages on the site and text messages out of band
- Brevity (Small) - all the code fits on a floppy disk, so new team members can get up to speed quickly, and a total rewrite is always possible
- Independence (Independent) - not tied to service providers who could impact other values above. second tier is open source frameworks and modules, choosing popular ones, millions of npm downloads, and even there, avoid writing lots of code in framework templates, for instance. also in here is the round robin system of making services redundant, measuring their performance with real useres, and automatically switching away from one that breaks
- Documentation (Complete) - fully containd and self sufficient bundle of code, comments, code as configuration, and documentation of configuration. favor 1 code, 2 configuration as code, then 3 configuration, and document configuration so a new team member could repeat steps without needing to ask you questions
- Thrifty (Cheap) - duplicated from scounts law, but grammer is mismatch here. here's where the value lives that even with tens of thousands of users, cloudflare will still cost $5/month and the free tier might work just as well, and supabase will cost $25. we never try to get an AWS startup grant of $1000k because we don't architect infrastructure that will ever spend thousands of dollars
"Your current list already nails speed, reliability, cost control, minimalism, and autonomy—these additions could refine its philosophy of endurance while maintaining the scout-law-like ethos." thanks copilot






     _         _                    _     _      
 ___| |_ _   _| | ___    __ _ _   _(_) __| | ___ 
/ __| __| | | | |/ _ \  / _` | | | | |/ _` |/ _ \
\__ \ |_| |_| | |  __/ | (_| | |_| | | (_| |  __/
|___/\__|\__, |_|\___|  \__, |\__,_|_|\__,_|\___|
         |___/          |___/                    

code at an 8th grade level
The rule "Simple is better than complex" comes from "The Zen of Python," a set of guiding principles for the language. Emphasized by Python's creator, Guido van Rossum, it promotes writing straightforward, readable code, making Python especially beginner-friendly and easy to maintain.

ideas to write the highly opinionated style guide

use complete words as names, avoid abbreviations and acronyms
for example, avoid "dev", as this is an abbreviation for developer or development
$ yarn local
where an acronym is not avoidable, treat it as a word
Html - good
HTML - bad
avoid all caps

tag names are title case, and end with a period
SomeTag.

comment nearly every line of code
write comments at the end of lines so comments do not lengthen the number of lines
comment not what the line of code does, but rather explain why it is doing that as part of what the surrounding function or larger scope accomplishes
comments should be nearly so complete and robust that you could give the function name and parameters, with the body stripped to be only comments, only, to a 2024-25 LLM and it could write a functional equivalent

do not format code to wrap onto multiple lines
your editor should be configured to wrap long lines
where lines of code are similar, use spaces to align similiarities vertically

use const to be explicit that a reference cannot or must not change
do not use it everywhere a reference happens to not change or not need to change
let is fine most of the time
use const for factory presets, imported module and platform APIs, Vue refs, and similar special cases

blocks of code have { spaces } while {objectProperty: 'literals'} do not

omit semicolons that are not necessary to enforce proper code flow
so if you have two statements on the same line, then you have to have a semicolon between them

string literals in single quotes, unless double or backtick necessary
in comments, double quotes

comments in conversational lowercase


Got it! That's a thoughtful convention - using const as a semantic signal rather than just "this doesn't get reassigned." So const becomes meaningful documentation that says "this is fundamentally immutable/fixed" (like configuration, imports, factory presets) rather than just "this variable happens to not change in this function."




Comments shouldn't just restate code - i++ doesn't need "increment i".
Instead, comments should explain *intent* and *context*:
why we're incrementing,
what it represents in the larger algorithm,
and what's happening under the hood that isn't obvious from the syntax.

The best comments turn code into a story:
they explain the problem being solved, why this approach was chosen, what trade-offs were made,
and what's happening at lower levels of abstraction that might surprise or inform future modifications.




modern js style is to use const absolutely everywhere you can, === everywhere
here, we don't do that
if something is const, that means it's important that it does not and cannot change
likewise, if you see value and type comparison, that highlights that here, type or type coersion matters
When reading through the codebase:
== or != → "Simple value comparison, types are already known/guaranteed"
=== or !== → "⚠️ Pay attention here - type coercion matters in this spot"
It's like syntax highlighting but for developer attention. The strict operators become a visual flag that says "this is one of those tricky JavaScript spots where 0, false, '', null, or undefined might be involved and we need to be careful."






 _                        _                            _ _                                     
| |__   __ _ ___  ___  __| |  ___  ___  ___ _   _ _ __(_) |_ _   _    ___  ___ ___  __ _ _   _ 
| '_ \ / _` / __|/ _ \/ _` | / __|/ _ \/ __| | | | '__| | __| | | |  / _ \/ __/ __|/ _` | | | |
| |_) | (_| \__ \  __/ (_| | \__ \  __/ (__| |_| | |  | | |_| |_| | |  __/\__ \__ \ (_| | |_| |
|_.__/ \__,_|___/\___|\__,_| |___/\___|\___|\__,_|_|  |_|\__|\__, |  \___||___/___/\__,_|\__, |
                                                             |___/                       |___/ 

(notes to clean up, chat will add detail and draft tight prose) ttd november

net23, essay on security

not in bounds
mistake in subtle crypto library
mistake with https itself, cloudflare
compromise of secure systems of amazon, cloudflare, supabase
user's computer is fundamentally compromised, rooted, keylogger
(an attack scenario that relies on things in this category is discarded; we can't protect the user in these situations; the user has larger problems everywhere in these situations)

completely in bounds
a mistake in code we control puts users or our system at risk
(completely in bounds attack scenarios; we need to correct a flaw in these scenarios)

partially in bounds
user has malicious chrome extension
user is being coached by attacker to follow instructions in devtools
(here, we do make choices to keep things secure, if possible, for users compromised in this way; many are)

based on that, we make a fundamental reasonable foundational assumption:

assumption
malicious code cannot get the broserTag in the http only cookie
in an endpoint, we can trust the browserHash, informatino from the database
we cannot and do not trust the contents of the posted body; it could come from our unaltered client code, it could alternatively come from that code modified arbitrarily by injected script or a malware chrome extension

also, a note on
defense in depth
not too deep
because then things rapidly become complex
harder to audit
if they change, hard to see if redundant protection is now necessary
simple and secure is the ethos

common on the web, but banned here
any server state between requests; there is no session object here, no redis, we're not using cloudflare kv as it represents greater vendor lock-in; the only durable state between requests is the database, and we use it sparringly
any cookie that woudl require a cookie warning

and jwt
nearly ubiquitous on the web
fails our security model, the malicious chrome extension sees the jwt come in a resononse, body, posts it to the attacker's server, replicates the authenticated session
so, on our ban list is jwts and the jwt like authentication exchange

titles for paragraphs:
no security through obscurity
complexity is a weapon of the enemy
only attack scenarios that are in bounds, please
if the user's box is fundamentally hosed, we can't help them
malware browser extensions are unfortunately common
attackers find n00bs on discord and reddit to coach into following malicious instructions
our primitives are standard, current, and secure
we trust the http only cookie, and assume the posted body is compromised
defense in depth can confuse defense

summary of pillars here
1 secure: no vulnerabilities; as secure as is possible, expected, required, for the modern consumer web, completely
2 hardened: for large cohorts of users who are lightly compromised, hardened against attacks possible, as much as possible 
3 simple: no security theater, simplicity is the best security, limited defense in depth










 _   _                     _                           
| |_| |__  _ __ ___  ___  | | __ _ _   _  ___ _ __ ___ 
| __| '_ \| '__/ _ \/ _ \ | |/ _` | | | |/ _ \ '__/ __|
| |_| | | | | |  __/  __/ | | (_| | |_| |  __/ |  \__ \
 \__|_| |_|_|  \___|\___| |_|\__,_|\__, |\___|_|  |___/
                                   |___/               

the club sandwich
cloudflare workers are great at
1 really fast with no cold start time
2 running trusted code that can't be tampered with and has access to secrets
3 npm imports designed for client side web
workers really cannot
4 run a node module that imagines a node server (require, commonjs, long-lived events, deep tree of other node-like
lambda can do those
but lambda is slow:
avg    p95   results from checkly, no-cookie GET and read of static returned page, no client side code runs in these
1.55s  2.57  ssr page from worker including one supabase query
4.13s  5.91  ssr page from worker including one supabase query, and two warm requests to lambda
why not move the whole thing to cloudflare? workers can't run all the modules we need
why not move the whole thing to amazon? making one out of every 20 first time visitors wait 5.91 seconds to get a brochure page is not acceptable
hence, the three layer stack, the club sandwich







 _                  _                     _                            
| |_ _ __ _   _ ___| |_    __ _ _ __   __| |   ___ _ __ _ __ ___  _ __ 
| __| '__| | | / __| __|  / _` | '_ \ / _` |  / _ \ '__| '__/ _ \| '__|
| |_| |  | |_| \__ \ |_  | (_| | | | | (_| | |  __/ |  | | | (_) | |   
 \__|_|   \__,_|___/\__|  \__,_|_| |_|\__,_|  \___|_|  |_|  \___/|_|   
                                                                       

essay about security, trust, errors, and exceptions

talk about
- Task as a wrapper
- unwrapping before passing up to not let things go deeper and deeper
- exceptions as truly exceptional (summarized below)

where should things be brittle? where should they be resiliant?
how are errors caught and dealt with, up and down the stack, within and without our code, within and without our control?

treat unexpected exceptions as fatal issues that indicate a bug in the code rather than as a part of normal control flow—the recommended Nuxt 3 configuration and practices shift toward a “fail fast” approach. This means that rather than attempting to recover from errors or provide graceful degradation, you let exceptions bubble up to a global error handler, log them appropriately, and present a clear, interruptive error page to users.

we don't follow nuxt patterns precisely, because we're vendor isomorphic between workers and lambdas

but reading
https://nuxt.com/docs/getting-started/error-handling
https://nuxt.com/docs/getting-started/error-handling#error-page
https://nuxt.com/docs/api/utils/create-error


Nuxt 3—with its underlying Nitro engine—mainly expects you to handle server-side errors by letting them bubble up and then catching them using Nitro’s built‐in hooks (like the render:errorMiddleware hook). This is the recommended and most common approach because it captures a wide range of errors that occur during request processing, including those that arise from internal configuration or runtime code that you might not directly control.

design ideas for the errors and exceptions essay

for exceptions specifically, what if you did this
put try {} catch (e) {} around
small: fetch to, and use of, unreliable can be made redundant third party APIs, like twilio
big: the entry point to request handlers for worker and lambda, global error handlers in nuxt, framework-level
and then, when you catch a small exception, you keep running, and work around it
and when you catch a big exception, you log it to datadog, and tear down the whole page, error.vue, interrupting the user
done this way, if you find yourself coding more try blocks, or rethrowing an error, if you find yourself doing any of that as part of expected logic flow, you're doing it wrong. rather, toss exceptions as soon as you encounter an exceptional circumstance, and let them fly all the way up the stack, getting logged to datadog, and closing out the page with error.vue

groups of causes and remedies for errors
1 (a) it's staff's fault, and the fix will be correcting code and deploying a new version
2 (a) it's a third party provider's fault, and that provider is critical, and cannot be made redundant, like cloudflare pages, turnstile, amazon lambda, supabase, they should be waking up people on pager duty, and should fix it in a few hours, and send an apology email to us
3 (b) it's a third party provider's fault, and that provider is non-critical, and can be made redundant, like twilio, amazon SES and SNS, the fix is the round robin code seeing the interruption and sending use to redundant providers, automatically and immediately, and manually and hours later, staff getting their support on the line, or finding out they have deplatformed us
3 (c) it's the user's fault, and the fix is them not clicking or typing the wrong thing
4 (a) it's the user's fault, adn the fix is they need to stop trying to hack us, or remove that malicious chrome extension
ok, so on these, when do you:
(a) tear down the whole page to an error message,
(b) catch an exception and code decides what to do next, routing around it, or
(c) this shouldn't raise an exception anywhere to begin with at all!


trust is in these three zones
1 page: information script on the page is telling us; least trustworthy
2 browser: information the browser is telling us; more trustworthy
3 worker: information cloudflare is telling us; trustworthy
what does it mean for each to be compromised?
1 page compromised: the user has a malicious desktop browser extension, or has been coached on reddit to dig into the developer tools, or is a script kiddie themselves, this is likely but should not be widespread
2 browser compromised: the attacker is using curl or a botnet, this is possible
3 amazon, cloudflare, or supabase compromised: this should not happen, as these public companies have their share price son the line. also, if it does happen, it is outside of our control


what if you do try catch like this
tight, around every fetch or use of a third party api, like AWS, twilio
broad, around the whole 


pay attention to the cause and the remedy of the error, here are three
1 a user, 




Exception Handling Design Guideline
- Use try/catch to protect against third-party API failures only. Examples: Twilio SMS calls on Lambda, Alchemy/MetaMask calls in the browser.
- Never use exceptions for control flow within our own code. Our code should be correct - if an exception occurs in our code, it indicates a bug we need to fix, not an expected condition to handle.
- Let unexpected exceptions bubble up. Nuxt's client and server-wide error handlers catch these, report them, and show users a "Please refresh and try again, or contact us on Discord" message. This deliberate crash surfaces bugs quickly rather than masking them.
- Result: Exceptions are rare. When they do occur, they're truly exceptional - either third-party service failures (handled locally with try/catch) or bugs in our code (caught globally, reported, and fixed).





       _ _          _                _         _               _   
  __ _(_) |_    ___| |__   ___  __ _| |_   ___| |__   ___  ___| |_ 
 / _` | | __|  / __| '_ \ / _ \/ _` | __| / __| '_ \ / _ \/ _ \ __|
| (_| | | |_  | (__| | | |  __/ (_| | |_  \__ \ | | |  __/  __/ |_ 
 \__, |_|\__|  \___|_| |_|\___|\__,_|\__| |___/_| |_|\___|\___|\__|
 |___/                                                             

$ git clone git@github.com:username/repository.git

$ git checkout -b name1             # Create a new branch and switch to it
$ git push -u origin name1          # Push the new branch to github and set the upstream

$ git fetch origin                  # Later, on another computer, find out about new branches up at origin
$ git branch -vv                    # List branches and their origins
$ git checkout name1                # And switch to one of the new ones there
$ git checkout main                 # Switch back to the main branch

$ git log --graph --oneline --decorate --all
$ git log --graph --oneline --decorate --all --pretty=format:"%h %ad %d %s" --date=format:"%Y%b%d"
$ git diff ec2023c..9edded6
$ git diff name1 name2 > diff.diff  #see the subway graph, and do a diff between two commits or branches; where you are doesn't matter

$ git checkout main                 #make sure main is a direct ancestor of name1 for an easy fast-forward merge
$ git merge name1
$ git push origin main

$ git branch -d name1               # Delete 'name1' after merging it (safe deletion)
$ git push origin --delete name1    # And, you should delete it from GitHub, the origin



$ git diff main feature1 > feature1.diff
$ git checkout main
$ git merge feature1
$ git push origin main









below are first and once for the project steps
group them into these categories
- setup a new computer, like get windows or mac ready to check out and build the site and net23
- configure and authenticate services, like install the aws cli and sign it in, cloudflare, serverless framework
- make a new similar shell project from scratch, useful to try out a new module or test something on the side, the clean room, this is like nuxt's setup, cloudflare create cli, that stuff
- not sure whre in there is make a github repo


          _                                                 _            
 ___  ___| |_ _   _ _ __     ___ ___  _ __ ___  _ __  _   _| |_ ___ _ __ 
/ __|/ _ \ __| | | | '_ \   / __/ _ \| '_ ` _ \| '_ \| | | | __/ _ \ '__|
\__ \  __/ |_| |_| | |_) | | (_| (_) | | | | | | |_) | |_| | ||  __/ |   
|___/\___|\__|\__,_| .__/   \___\___/|_| |_| |_| .__/ \__,_|\__\___|_|   
                   |_|                         |_|                       

new box, who dis?
steps to setup a new windows or mac workstation for development with the network 23 stack
mac you can really only experience these steps upon unboxing
windows you can always repeat to check with vmware

minimal steps to get to yarn local, build, deploy for worker and lambda
includes stuff like get git, node, node version manager, place secrets

== mac, 2025feb, unboxing, apple silicon, macOS 15 Sequoia

install iterm2
https://iterm2.com/
Download

install git
$ git --version
install box for command line developer tools appears, Install
used storage goes from 57->62gb
$ git --version, 2.39.5

install homebrew
https://brew.sh/
$ /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
and then follow steps to add to path by editing ~/.zprofile
$ brew --version, 4.4.21
$ brew update, do before installing new packages

install nvm, node, and npm
used homebrew to install nvm, and it worked fine,
but homebrew shows output that says the nvm people want us to say they don't like it
$ brew install nvm    $ nvm --version, 0.40.1
$ nvm install 20      $ node --version, 20.18.3
$ npm install -g npm  $ npm --version, 11.1.0, the update changed the version from 10->11

install yarn version 1 with npm (not brew)
$ npm install -g yarn@1
$ yarn --version, 1.22.22

install the aws cli with brew (not npm, also notice we're not using yarn to install anything globally)
$ brew install awscli
$ aws --version, 2.24.10

install cloudflare wrangler globally, which is also referenced in individual projects' package.json files
$ npm install -g wrangler
$ wrangler --version, 3.109.2

install serverless framework globally with npm
$ npm install -g serverless
$ serverless --version, 4.6.4, and running that also causes it to update

authenticate git with github
instructions:
https://docs.github.com/en/authentication/connecting-to-github-with-ssh
control panel:
https://github.com/settings/keys
choosing ed25519 instead of rsa
choosing no passphrase, which means you also don't have to deal with the ssh agent
on the new mac, there is no .ssh folder at all yet
$ ssh-keygen -t ed25519 -C "name@example.com"
using the email address github knows about, which will also match the git global config step in a moment
now you have these two files:
~/.ssh/id_ed_25519
~/.ssh/id_ed_25519.pub
the public one goes into the github control panel; the private one stays private on your new box

set name and email in git
$ git config --global --list
user.name=First Last
user.email=name@example.com
git config --global user.name "First Last"
git config --global user.email "name@example.com"

now you can check out private repositories, notice this new form that doesn't start https and ends .git
$ git clone git@github.com:username/repository.git

chrome, Add New Profile, sign in with google credentials for project profile, yes sync
then sign into web dashboards:
https://cloudflare.com/
http://aws.amazon.com/, Sign in using root user email
https://app.serverless.com/
https://supabase.com/
https://app.datadoghq.com/logs
keep that chrome profile forward so when the command line pops to web to authenticate, it finds it

authenticate wrangler to be able to deploy to cloudflare pages
$ npm install -g wrangler, you already did this
$ wrangler --version, 3.109.2
$ wrangler whoami
$ wrangler login, pops to browser so make sure you've got the right tab signed in and forward!
$ wrangler logout, if you need to switch accounts

authenticate the aws cli
dashboards at aws.amazon.com
iam dashboard
click the number of users
create user
net23dev2, example name
attach policies directly
check AdministratorAccess, second one
create user
click new user
access key 1, create access key
use case, command line interface, check understand reccomendation, next
no description, create access key
secret access key only shows once, paste it right into bash

AWS Access Key ID [None]: YOUR_ACCESS_KEY_ID
AWS Secret Access Key [None]: YOUR_SECRET_ACCESS_KEY (paste in these two)
Default region name [None]: us-east-1
Default output format [None]: json (type these two)

commands to sign in, check sign in, and test sign in by listing your buckets
$ aws configure
$ aws sts get-caller-identity
$ aws s3 ls

now you've got this box set up enough you can checkout and build
$ git clone git@github.com:zootella/cold3.git
$ cd cold3
$ yarn install
$ yarn test

copy in secrets
cold3/.env
cold3/.env.keys, place the two secrets files in the project root

$ yarn seal, which copies the secrets files you just placed
$ cd site
$ yarn build, works before you've authenticated wrangler
$ cd ../net23
$ yarn stowaway, which copies the linux binaries you placed
$ yarn build
the net23 build enters flow to authenticate serverless framework cli

and those are the steps to get a brand new mac ready to develop, build and deploy

== windows

(next, do all the same steps in vmware)



note about ./package.json
	"packageManager": "yarn@1.22.22",
and on mac, yarn 4 installed globally
corepack enabled, only have to do this once per system
and then corepack sees the package manager key

$ cold3 % cd ..
$ code % yarn --version
4.9.2
$ code % cd cold3
$ cold3 % yarn --version
1.22.22




also need to do this for the root scripts wash and upgrade-wash to work:
$ npm install -g rimraf
https://www.npmjs.com/package/rimraf 106mm
before this, it was a mess as you couldn't double call it because it would delete itself

another one
$ npm install -g open
https://www.npmjs.com/package/open 51mm
also note that there is a yarn global install, but you are using npm for global instead



== mac development workstation seasonal maintance, 2025oct

bottom to top, the stack is:
brew - linux style os package manager
nvm - switches and isolates node versions; always enabled once installed
node - comes with npm and corepack
corepack - use to manage yarn; comes with node but starts off, we turn it on and keep it on in every nvm node version
yarn - make 1 classic available from corepack, and any version from a package.json packageManager field

global things and where we get and keep them:
awscli@2 - install with brew
yarn @1 - keep within corepack; do not install with brew or npm!
wrangler @4 - install with npm, isolated to a nvm node version
serverless @4 - install with npm, isolated to a nvm node version
rimraf - install globally so wash scripts work when node modules is already cleared

$ brew update       //update homebrew itself
$ brew upgrade nvm
$ brew outdated     //see which packages could be upgraded
$ brew upgrade      //upgrade everything, optional

$ brew install awscli     //install the aws cli globally with brew, not npm or yarn
$ aws --version, 2.31.23

$ nvm list              //which node versions nvm has installed ready to switch to
$ nvm install 22        //add a new one to that list
$ nvm current           //what node is currently globally use on this box
$ nvm use 22            //change this terminal session to a specific node version
$ nvm alias default 22  //have all terminal sessions start with this node version
$ nvm use               //same thing but reads the version from the project's .nvmrc file

our yarn goals are:
- keep it within corepack, *not* npm or homebrew
- have version 1 available outside a project folder with a package.json (not modern v4, but then don't use it: scaffold with npm)
- a package.json file can specify a particular yarn version

$ corepack prepare yarn@1.22.22 --activate  //do once setting up a new box; persists across node versions
$ brew list yarn         //see if homebrew has a yarn version, which would be bad
$ brew uninstall yarn    //remove if found
$ npm list -g yarn       //another place yarn can be is npm's globals; you have to do this for each nvm node version!
$ npm uninstall -g yarn

$ nvm use 22       //after switching to a new node version,
$ corepack enable  //make sure corepack is on; the enabled state persists and is per node version
$ which yarn       //you'll get a path like /Users/name/.nvm/versions/node/v22.21.0/bin/yarn
$ yarn --version   //another way to check yarn is ok; corepack disable and yarn is not found!

$ npm install -g wrangler@4 serverless@4 rimraf  //do this for each node version; use npm not yarn for globals




other things related to development workstation setup
gets you the rg program which yarn xray uses
$ brew install ripgrep
$ rg --version
ripgrep 15.1.0






 _            _     __ _ _      
| | ___   ___| | __/ _(_) | ___ 
| |/ _ \ / __| |/ / |_| | |/ _ \
| | (_) | (__|   <|  _| | |  __/
|_|\___/ \___|_|\_\_| |_|_|\___|
                                


we're on yarn classic, 1.22.22, because yarn
- is easy and short to type (unlike npm which also makes you type run)
- invented workspaces (unlike npm which added them)
- 4 didn't work at all, even just on mac!

yarn.lock, about to switch to this treatment:
- .gitignore stops ignoring it; let yarn.lock into the git repo (this is the change, 2025jul)
- seal disk omits it from the human readable code size total
- seal hash includes it in the hash manifest
- powerwash still deletes it; yarn powerwash && yarn install to update all modules

this is fine, except if you switch back to windows, you have to powerwash!
installing on win|mac with a lockfile from the other has got to be an invitation for unnecessary errors and weirdnesses





in package.json, three ways to specify a module version:

"^1.2.3" (carrot)
 "1"     (desired)
"^1.0.0" (hack)

all are valid patterns according to semantic versioning, https://semver.org/
carrot is what you get by default
it means that every time you do a powerwash, you get versions at or above those listed in package.json
so the version numbers in package.json don't really mean anything--they're antique and irrelevant to what you're actually running
the idea about how all this is supposed to work is then the lockfile pins and documents the exact versions
and then you go and yarn update module by module

but in practice, you want to update everything at once, and frequently get all current modules
if there's a problem, you want to know about it and resolve it, rather than
so your powerwash script does this, by deleting the lockfile
also, you didn't check the lockfile into git, because if you accidentally installed on windows from a lockfile made on mac, or the reverse, surely that would create a lot of phantom problems until you remembered you should powerwash

ok, so now if that's the workflow, powerwash all the time
you don't want to have full nonsensical versions in package.json
you tried switching to just "1", which, so long as a module never moves to a lower version number (you have not heard of this) is the same
testing that out, it all worked fine, except there was a really weird bug with lambda deployed where tests passed, but it couldn't tell you how many there were (you could try to reproduce this)

or, after that, you thought you could fake it with the third pattern, labeled (hack) above, where
it would be nice to look at package.json files and reason about them without being distracted by minor and patch version numbers that don't really tell us anything









                      _     _   _                  _             _   
 _ __ ___ _ __   ___ | |_  | |_| |__   ___   _ __ | | __ _ _ __ | |_ 
| '__/ _ \ '_ \ / _ \| __| | __| '_ \ / _ \ | '_ \| |/ _` | '_ \| __|
| | |  __/ |_) | (_) | |_  | |_| | | |  __/ | |_) | | (_| | | | | |_ 
|_|  \___| .__/ \___/ \__|  \__|_| |_|\___| | .__/|_|\__,_|_| |_|\__|
				 |_|                                |_|                      

(these steps are about recreating the project, not the computer you're building the project on)

code rot happens faster than ever
six months later, several modules will be on new full number versions
and the task of updating them and confirming that the stack fits together and works together as a whole is significant

to keep the project current in today's reality of rapid code rot,
without spending the time and effort of updating modules individually, testing everything, and then moving on to the next one
the Network 23 Software Engineering Division instead employs a manœuvre we call "repot the plan"

here's how you do that
(step 1)
after updating platform things like node, yarn, git, wrangler, modules outside any project
(step 2)
in a completely separate folder,
following current getting started docs like
https://developers.cloudflare.com/pages/get-started/c3/
and
https://nuxt.com/docs/getting-started/installation
make a new 'Hello, World!' project from scratch, bringing in the same modules (but new versions)
essentially, follow the path from the very start that you used the last time to create the project
(step 3)
confirm hello world in the new pot is working, local and deployed
look at the configuration files you get, like wrangler.toml and nuxt.config.ts--the starting default contents will be different
merge previous project configurations into the new configuration files, make sure this does not break hello world
(step 4)
move code files from the old project into the new one




quick repot1

$ npm create cloudflare@latest
repot1
website or web app
nuxt
yes git for version control
yes deploy to cloudflare
(quickly get the right chrome profile forward, and already signed in to cloudflare there!)
yeah it's up at
https://repot1.pages.dev/

$ npx nuxi add mypage
$ npx nuxi add mycomponent
$ npx nuxi add myapi


2025feb trying to find or make new repot steps
https://developers.cloudflare.com/pages/get-started/c3/
$ yarn create cloudflare

ttd february2025

get just enough to generate stats.html
and to deploy to a pages.dev domain
use this to find small modules for qr codes and totp codes that works in cloudflare
and have good notes so you can return to it later, also



notes repotting for nuxt based static site start, 2025mar
https://developers.cloudflare.com/pages/get-started/c3/

$ wrangler whoami (make sure you're signed into the cloudflare account you want to use)
$ yarn create cloudflare (with yarn 1 installed globally)
example-com, sets the local directory and pages project name in the dashboard; use hyphen, not space
Framework Starter, category
Nuxt, framework
y, yes to install nuxi@3; didn't get asked the second time through these steps
no, don't use git for version control; going to drag created files into this existing repository
yes, deploy to cloudflare; add domain after that in the dashboard

github website
navigate to organization or user, repository list, New green button
example.com, repository name, ok to use a period here
wrote description
Public
no readme, gitignore, license, to start out with a completely empty repository
Create repository

$ git clone git@github.com:username/example-com.git
warning: You appear to have cloned an empty repository.

then, drag contents from example-com folder to example.com folder
set README.md, LICENSE, and .gitignore
in package.json, rename dev to local
git add, commit, push
yarn build, local, and deploy





~~ notes from the 2025jul repot prompted by wrangler 3->4 and cloudflare pages->workers

cloudflare create without pages flag, to use that the new way
current version of nuxt, which is still 3
add nuxi to use that in yarn, going to get 3
current version of wrangler, which is now 4

https://developers.cloudflare.com/pages/get-started/c3/
https://developers.cloudflare.com/workers/static-assets/migration-guides/migrate-from-pages/
https://developers.cloudflare.com/workers/static-assets/migration-guides/migrate-from-pages/#compatibility-matrix

$ yarn create cloudflare (note no pages flag, choosing the workers option)
repot4, name
framework starter
nuxt
none, official modules
no, git version control
no, deploy application

choosing to align the compatability dates

local, build, deploy, all work
in cloudflare dashboard, added the subdomain

$ yarn add -D nuxi
$ yarn nuxi --version, 3.25.1
note how nuxi and wrangler are installed globally and in the project
and all the versions are the same

ok, next, let's add the modules and plugins, specifically
- icarus
- pinia
- tailwind
- vidstack
- nuxt og image, which may include sharp? (it didn't)

## pinia

"nitro-cloudflare-dev", "@nuxtjs/tailwindcss", "@pinia/nuxt"
 	"dependencies": {
+		"@pinia/nuxt": "0.11.1",
+		"pinia": "^3.0.3",

pinia went up a whole version number, so watch out for that

## tailwind

https://tailwindcss.nuxtjs.org/
$ yarn nuxi module add tailwindcss

ok so what that did, other than messin gup your indentation
"@nuxtjs/tailwindcss"
				modules: ["nitro-cloudflare-dev", "@nuxtjs/tailwindcss"],
	"dependencies": {
		"@nuxtjs/tailwindcss": "7.0.0-beta.0",
so backed out of that and installed it manually to get the previous, not beta, version

## vidstack

$ yarn add vidstack@next
otherwise you get a 0. version

## nuxt og image

$ yarn run nuxi module add og-image
that and complete notes in the cards section below





== nuxt scaffolding steps, 2025oct

first, context as to why we're here:
upgrade wash created an error which you can solve by taking node 20->22
did this first with the nuxt project; serverless.yml is still on 20
but these are general useful scaffolding steps you've needed before now

lighter weight than a full repot, the goal here is to quickly scaffold the nuxt project
imagining you were following getting started installation steps from current documentation today
reasons you frequently want to do this:
- confirm core flow works, like dev, build, deploy
- see what versions of modules you get
- look at project file structure
- look at config files and default values

modules that are a part of or tied to nuxt are here, like pinia, tailwind, og image, vidstack
really anything where the getting started installation page has framework-specific steps
other front end modules that can break in workers, like qrcode, viem, wagmi core, don't need to be here

note that these steps use npm not yarn, because
it's the default for getting started installation pages
it's confusing to follow yarn steps when they might mean classic or berry
you can delete node_modules and lockfiles and switch to yarn afterwards
the scaffolded module versions and default settings files won't be different had you switched to yarn early

(0) starting environment

$ nvm current         "22", using 22 for this test today 2025oct28
$ node --version      "22"
$ npm --version       "10", intentionally didn't update
$ yarn --version      "1.22.22", available globally from corepack
$ wrangler --version  "4", installed globally with npm in the context of this node version in nvm

(1) cloudflare workers with nuxt and vue
https://developers.cloudflare.com/workers/framework-guides/web-apps/more-web-frameworks/nuxt/
https://nuxt.com/docs/4.x/getting-started/installation

$ npm create cloudflare@latest
oct28a, name
framework starter
Nuxt
no official modules (and none we need below are in their list!)
no git
no deploy (even in real scaffolding we'll do these two things afterwards)

$ cd oct28a
$ npm run build
$ npm run dev

	"dependencies": {
		"nuxt": "^4.2.0",
		"vue": "^3.5.22",
		"vue-router": "^4.6.3"
	},
	"devDependencies": {
		"nitro-cloudflare-dev": "^0.2.2",
		"nitropack": "^2.12.9",
		"wrangler": "^4.45.1"
	}
	^results from running through these steps 2025oct28; cold3 is current except nuxt has gone to 4!

>>> 2025oct28

(2) nuxi
install nuxi locally so you can see its version number
you still have to use npx to run it, but npx will use the local copy rather than global or temporary download

$ npm install -D nuxi
$ npx nuxi --version

	"devDependencies": {
		"nuxi": "^3.29.3",

(3) pinia
https://nuxt.com/modules/pinia
https://pinia.vuejs.org/getting-started.html

$ npm install pinia @pinia/nuxt

	"dependencies": {
		"pinia": "^3.0.3",
		"@pinia/nuxt": "^0.11.2",

(4) nuxt og image
https://nuxt.com/modules/og-image

$ npx nuxi module add og-image

	"dependencies": {
		"nuxt-og-image": "^5.1.12",
		"@unhead/vue": "^2.0.19",
		"unstorage": "^1.17.1",
	^brings along two peer dependencies, but regular carrots on all three this time

(5) tailwind
https://nuxt.com/modules/tailwindcss

$ npx nuxi module add tailwindcss
$ npm list tailwindcss

	"dependencies": {
		"@nuxtjs/tailwindcss": "^6.14.0",
	^but then $ npm list tailwindcss shows that the version of tailwind it brought is "tailwindcss@3.4.18"

(6) vidstack
https://vidstack.io/docs/player/getting-started/installation/vue/?bundler=nuxt&provider=video&styling=default-layout

$ npm install vidstack@next

	"dependencies": {
		"vidstack": "^1.12.13",

in summary, scaffolding 2025oct28, cold3 is up to date and behind as follows:

on 2025oct28 start of day

cold3   today   module
----    ----    ----
22      22      node
1       1       yarn

3  !=   4       nuxt
3       3       nuxi
3       3       vue
4       4       vue-router

0.2.2   0.2.2   nitro-cloudflare-dev
2       2       nitropack
4       4       wrangler

3       3       pinia
0.11.1  0.11.2  @pinia/nuxt, note version 0 so pinned to 0.11.x

5       5       nuxt-og-image
2       2       @unhead/vue
1       1       unstorage

6       6       @nuxtjs/tailwindcss
3       3       actual tailwindcss

1       1       vidstack

ok, so everything is still current except for nuxt
and you can wait ~6m to let the ecosystem catch up with nuxt 4 before you deal with it







										_            _       
 _ __ _   _ _ __   | |_ ___  ___| |_ ___ 
| '__| | | | '_ \  | __/ _ \/ __| __/ __|
| |  | |_| | | | | | ||  __/\__ \ |_\__ \
|_|   \__,_|_| |_|  \__\___||___/\__|___/
																				 

tiny tests, in level0.js, exports test(), ok(), and runTests()
all calls to test() are in the library
and, as much as possible, factor all *code* into the library as well!
pages, components, and api handlers should be really short

run tests in...

(1) Node
$ cd ./library
$ node test.js
test results are logged out

(2) Vite
$ cd ./icarus
$ npm run icarus
and then see test results on the home route

(3) Nuxt
$ cd ./
$ npm run local
and then see test results on the route /tests, ttd february2025!

(and add more)
nuxt really is local/deployed x client/server--can you run tests in all four environments?
lambda serverless framework is local/deployed--run in those two environments







                 _                                  _        _     _ _ _ _         
 _ __   ___   __| | ___    ___ ___  _ __ ___  _ __ | |_ __ _| |__ (_) (_) |_ _   _ 
| '_ \ / _ \ / _` |/ _ \  / __/ _ \| '_ ` _ \| '_ \| __/ _` | '_ \| | | | __| | | |
| | | | (_) | (_| |  __/ | (_| (_) | | | | | | |_) | || (_| | |_) | | | | |_| |_| |
|_| |_|\___/ \__,_|\___|  \___\___/|_| |_| |_| .__/ \__\__,_|_.__/|_|_|_|\__|\__, |
                                             |_|                             |___/ 

https://developers.cloudflare.com/workers/configuration/compatibility-flags/#nodejs-compatibility-flag

2025jun repotted the plant

nuxt.config.ts has
configuration.compatibilityDate = '2025-06-02'
configuration.nitro = {
	cloudflare: {
		nodeCompat: true,
	},
}

wrangler.jsonc has
"compatibility_date": "2025-06-02",
"compatibility_flags": ["nodejs_compat"],

from these, dashboard shows
Compatibility flags: nodejs_compat, no_nodejs_compat_v2

talking to chat, explination seems to be:
configuration.compatibilityDate / nodeCompat in nuxt.config.ts control how your code is built locally (which APIs Nitro bundles into your output)
compatibility_date / compatibility_flags in wrangler.jsonc control what flags and polyfills the Workers runtime actually enables when handling incoming requests on the edge

and this combination of flags brings in the v1 polyfills which result in a smaller bundle size
ok, so build, deploy, supabase in icarus, all that works
and small is good
so you're going to leave this as is without researching or experimenting further





                        _       
 ___  ___  ___ _ __ ___| |_ ___ 
/ __|/ _ \/ __| '__/ _ \ __/ __|
\__ \  __/ (__| | |  __/ |_\__ \
|___/\___|\___|_|  \___|\__|___/
                                
demonstrated facts about using secrets in nuxt deploying to cloudflare workers, 2025jun

[i] the intended Nuxt pattern

the intended Nuxt way to deal with secrets is to:
- prefix them NUXT_
- identify them in nuxt.config.ts runtimeConfig
- retrieve them through useRuntimeConfig()
and this works for Nuxt, but does not work for Nuxt on Cloudflare:
https://github.com/nitrojs/nitro/issues/2468

[ii] the only working solution

the only way to get the secret from the dashboard is to read it from the event object that comes in with the request

//first, Nuxt way, doesn't work on Cloudflare
export default defineEventHandler((event) => {
	const {example1Secret} = useRuntimeConfig()

//instead, second, Cloudflare way, does work in Nuxt (and also SvelteKit)
export default defineEventHandler((event) => {
	const e = event.context.cloudflare?.env
	const s2 = e?.EXAMPLE2_SECRET

and, you've confirmed that done this way, not only do you get the secret from the dashboard
but you can change the value on the dashboard, and the site gets it
without building and deploying a new bundle--indicating the secret comes from the dashboard and is not baked in!

[iii] baking them in securely by accident

before, you thought secrets were coming from the dashboard, but really they get backed into the server bundle here
in nuxt.config.ts
defineNuxtConfig({
	runtimeConfig: {
		MY_KEY_SECRET: process.env.MY_KEY_SECRET,
	}
})
coded that way, at build time, the secret value gets copied from .env into the server bundle
this doesn't mean it leaked, as the server bundle is still private, but seems like bad form
it does mean this works without anything set on the dashboard, and also updating the dashboard doesn't matter
the Nuxt pattern to correct this is to instead do
        myKeySecret: '',//and then name this NUXT_MY_KEY_SECRET in the .env and .dev.vars files
and then at run time Nuxt sees the NUXT_ prefix and loads the value in, local from a dot file, deployed from the dashboard
and then in Nuxt on the server you get it with useRuntimeConfig()
so lots of distracting chat on following this Nuxt pattern which works, just not on Cloudflare!

[iv] the two secrets files

the common pattern for Nuxt on Cloudflare is to duplicate your secrets files:
./.env
./.dev.vars
scripts in package.json are:
	"scripts": {
		"build": "nuxt build",
		"local": "nuxt dev",
		"preview": "yarn build && wrangler dev",
$ yarn local   --> runs "nuxt dev"     on http://localhost:3000, which should get secrets from .env
$ yarn preview --> runs "wrangler dev" on http://localhost:8787, which should get secrets from .dev.vars
according to documentation and chat
but getting secrets from the event object, the only way to get them from the dashboard
means that running locally either way they come from .dev.vars
so we'll leave .env around, but note that it's never used


ttd june2025
based on that, in summary
we're still baking the secret into the server bundle, which lets getAccess() with no event work
we did add the event object to getAccess(workerEvent) in doorWorkerOpen, letting a dashboard value in
but are not relying upon that entirely




ttd june2025
move in stuff from readme to here
talk about how seal copies values down into the different workspaces
mention you have to change Text to Secret in the cloudflare dashboard




ttd july2025
seeing how big wrapper is in the size/client.html report
and seeing icarus/wrapper.js is 2.69 KB compressed just 91%
and big even alongside level0.js and TermsDocument.vue

for these reasons:
- client bundle size
- defense in depth
- general appearances
see if you can get what's currently the 3 KB wrapper.secrets
into a file that full stack js icarus code still indescrementantly references
but which doesn't end up making it into the svelte or nuxt front end bundles

possible general strategy to do this
move the encrypted secrets string to a separate file alongside wrapper:
./icarus/wrapper.js
./icarus/secrets.js
but then there's something that runs everywhere but finds nxut and svelte front end, like
if (process server) secrets.vault = 'all the encrypted secrets'
except you also have to find that for sveltekit, which could be hard/brittle

and also have a tag like SECRETS_VAULT_MARKER alongside those
and then confirm that you can see, and then not see, it view source, essentially






//                         _                                    _  __          
//  ___  ___  ___ _ __ ___| |_ ___    __  ___ __ __ _ _   _    | |/ /___ _   _ 
// / __|/ _ \/ __| '__/ _ \ __/ __|   \ \/ / '__/ _` | | | |   | ' // _ \ | | |
// \__ \  __/ (__| | |  __/ |_\__ \_   >  <| | | (_| | |_| |_  | . \  __/ |_| |
// |___/\___|\___|_|  \___|\__|___( ) /_/\_\_|  \__,_|\__, ( ) |_|\_\___|\__, |
//                                |/                  |___/|/            |___/ 

2025nov building out the new Key system, which will replace readme txt secrets and getAccess
and first, here's a secret detector
from the monorepo root:

$ yarn xray VGGihDyy
site/.nuxt/dist/client/_nuxt/C5cW7Dqu.js:31:41306:VGGihDyy
site/.nuxt/dist/server/server.mjs.map.json:1:269690:VGGihDyy
site/.nuxt/dist/server/server.mjs.map:1:118796:VGGihDyy
site/.output/public/_nuxt/C5cW7Dqu.js:31:41306:VGGihDyy
site/.output/server/chunks/nitro/nitro.mjs:1:468846:VGGihDyy
site/.output/server/chunks/build/server.mjs:1:152962:VGGihDyy

your plan is to split secrets into three tiers:
1 public and on the page intentionally
2 secret and only in server bundle
3 only on development workstation for running locally, and amazon and cloudflare dashboards (not in server bundles!)
and to prefix secret values with XRAY100existing-value or whatever
ok so then you can use the xray script to see everywhere they exist in build artifacts,
all the way along and up to the bundles that get sent to cloudflare and amazon
well, that's the idea, at least--you may have to unzip net23.zip, and a secret could be hidden from xray, but revealable to anyone, if it's in base62
but at least this will let you confirm what's in .env and .dev.vars and how nuxt for cloudflare includes things and passes them around
ttd november

an easy way to think about this:
XrayServerValue - on development workstation? yes; in server bundle? yes; in client bundle? NO
XrayDashboardValue - on development workstation? yes; in server bundle? NO; in client bundle? NO
there's a single set of server and dashboard keys for both amazon and cloudflare
prefix the values with XrayServerValue, and the key system removes that from the start
use the xray tool, and notes (not more automated than that) to confirm the 6 locations above

for cloudflare, the server value is the nuxt thing where it actually gets built into the server bundle
and the dashboard value is the cloudflare not nuxt thing where it actually comes from the dashboard

for amazon, the server value is the serverless framework thing where it gets built into the lambda bundle
and the dashboard value is the AWS Secrets Manager thing where it gets checked out 

what additional protection would this provide?
imagine there's a bad guy working at amazon or cloudflare
they can't get into the secrets vaults in the office
nor can they observe the running memory of lambdas or workers
but they can get into the (less secured) shelf of server code bundles (what gets loaded when a request comes in)
so they could get server key, but not the dashboard key
and both keys are necessary to decrypt the keys that get gets like Key('whatever')

ttd november





in decryptKeys, you can switch on logging to see where where running code, both local and deployed, is actually finding the key
here's a summary of results from 2025nov with serverless framework 4, nuxt 3, and sveltekit 2

observed running...                 local    cloud    notes
--------------------------------    -----    -----    -----
auth.js handlers in sveltekit on cloudflare workers
b10 process.env                     -        cloud     
b20 event.platform.env              local    cloud     

nuxt on cloudflare workers
c10 process.env                     local    cloud    local makes sense; not sure if cloud comes from server bundle or dashboard
c20 event.context.cloudflare.env    local    ?????    usually, cloud c20 is absent, but shows up sometimes; super weird
c30 event.context.env               -        -        never seen
c40 event.platform.env              -        -        never seen
c50 useRuntimeConfig(event)         -        -        never seen, ironic as this is the correct Nuxt way to do secrets!

serverless framework on aws lambda
d10 process.env                     local    cloud    always, wouldn't work otherwise, unusual place where amazon is simpler

additional secret stores not in use yet are SvelteKit's $env/static/private and $env/dynamic/private
and AWS Secrets Manager, which would incur more code, cost, and delay, and less reliability










     _ _             _         
 ___(_) |_ ___   ___(_)_______ 
/ __| | __/ _ \ / __| |_  / _ \
\__ \ | ||  __/ \__ \ |/ /  __/
|___/_|\__\___| |___/_/___\___|
                               
see how big the nuxt site is, and what's contributing to its size
uses nNuxt’s built-in analyzer, which uses Rollup Plugin Visualizer under the hood
https://www.npmjs.com/package/rollup-plugin-visualizer

$ cd site
$ yarn size

file:///.../site/size/client.html
file:///.../site/size/nitro.html

in site/nuxt.config.js try out "sunburst", "treemap", "network", "raw-data", or "list" options

takeaways looking at the treemaps after the 2025jul repot
both are really small: 296 KB client; 4.77 MB server
framework modules are lean enough that you can see your own code as sizeable, like the icarus levels
blocky non-code include the terms document and wrapper encrypted secrets
vidstack and nuxt-og-image are big

git ignores the output reports; seal lists them in wrapper.txt; disk doesn't count them as human readable

 _                          
(_) ___ __ _ _ __ _   _ ___ 
| |/ __/ _` | '__| | | / __|
| | (_| (_| | |  | |_| \__ \
|_|\___\__,_|_|   \__,_|___/
                            
(moved here from ./icarus/icarus.txt - but maybe it should be there, or at least a note about it in the readme)



Icarus is a vite little tdd system and test runner for the library functions

commands you used to make this:

https://vitejs.dev/guide/
$ npm create vite@latest
chose vue, rather than vanilla or react
chose javascript, rather than customize with create-vue or nuxt
$ cd hivite
$ npm install
$ npm run dev
http://localhost:5173

commands to use icarus:

$ cd icarus
$ npm run icarus
http://localhost:5173

it works! superfast refresh on:
-not just this folder, but the library folder alongside
-not just code changes, but comment and whitespace changes

next steps for this bike shed:
[]get the test results on the page
[]get the log output on the page
refresh replaces those there, instead of scrolling forever downwards
and if that works, can you not open the dev tools until you want to walk an object with >

[]move these notes up into cold3 readme










										 _                         _           _   
	___ _ __ ___  __ _| |_ ___   _ __  _ __ ___ (_) ___  ___| |_ 
 / __| '__/ _ \/ _` | __/ _ \ | '_ \| '__/ _ \| |/ _ \/ __| __|
| (__| | |  __/ (_| | ||  __/ | |_) | | | (_) | |  __/ (__| |_ 
 \___|_|  \___|\__,_|\__\___| | .__/|_|  \___// |\___|\___|\__|
															|_|           |__/               

[]ttd january2025, in vmware windows and a new mac silicon user, set up project from scratch to check and minimize these notes. divide "create project" into two sections: "prepare workstation" which has stuff like brew and yarn and wrangler and serverless, all the global stuff, and "repot the plant" with stuff like cloudflare create and serverless' create
sorta also in this section is how to set it up in docker; that's a new workstation
also on this list is stuff where you authenticate, like github, aws cli, cloudflare

== install aws cli

npm's aws-cli is deprecated as of 6 years ago
it seems most people install aws-cli with python
https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html
amazon docs say for windows install that msi

$ aws --version
aws-cli/2.16.4 Python/3.11.8 Windows/10 exe/AMD64

(note on mac, you're trying $ brew install awscli)
$ brew --version
$ brew update
$ brew upgrade
$ xcode-select install
$ xcode-select -p

== give aws cli access to your aws account

$ aws configure

AWS Access Key ID [None]: YOUR_ACCESS_KEY_ID
AWS Secret Access Key [None]: YOUR_SECRET_ACCESS_KEY
Default region name [None]: us-east-1
Default output format [None]: json

dashboards at aws.amazon.com
iam dashboard
click the number of users
create user
net23dev2, example name
attach policies directly
check AdministratorAccess, second one
create user

click new user
access key 1, create access key
use case, command line interface, check understand reccomendation, next
no description, create access key

secret access key only shows once, paste it right into bash

$ aws sts get-caller-identity
$ aws s3 ls

confirm you're in, and list your buckets

== install serverless framework

https://www.serverless.com/

$ npm install -g serverless
$ serverless --version

== setup repository on github

made repo on github with node defaults
$ git clone https://github.com/zootella/net23

$ cd net23
move aside .gitignore as serverless create will also make one
$ serverless create --template aws-nodejs --path .

$ npm init
$ npm install -S aws-sdk

amazon access is configured, but make .env to mirror non-amazon secrets the lambdas will use
$ touch .env
and make sure .gitignore lists it
		 _                       _                                    
	__| | ___  _ __ ___   __ _(_)_ __    _ __   __ _ _ __ ___   ___ 
 / _` |/ _ \| '_ ` _ \ / _` | | '_ \  | '_ \ / _` | '_ ` _ \ / _ \
| (_| | (_) | | | | | | (_| | | | | | | | | | (_| | | | | | |  __/
 \__,_|\___/|_| |_| |_|\__,_|_|_| |_| |_| |_|\__,_|_| |_| |_|\___|
																																	

== domain name

registered net23.cc at a third party registrar

aws web console, route53 dashboard, hosted zones, create hosted zone
net23.cc, domain name
public already selected, create hosted zone
leads to page for new hosted zone, titled net23.cc, with two records
the NS record shows the name servers amazon has picked for us:

ns-1619.awsdns-10.co.uk.
ns-1281.awsdns-32.org.
ns-682.awsdns-21.net.
ns-211.awsdns-26.com.

go back to the registrar to put those in
nameservers, basic dns, changing that to custom dns
entering the four name servers above, *without* the periods at the ends

dnspropegation happens pretty quick, then check with nslookup

$ nslookup net23.cc
$ nslookup net23.cc 8.8.8.8
$ nslookup -type=NS net23.cc

first two won't work yet because you haven't made any A records
the 8s uses google's dns to check, rather than whatever is local

== ssl certificate

acm, list certificates, request
request a public certificate
add another name to this certificate to get two boxes, then enter
net23.cc
*.net23.cc
defaults of dns validation, rsa2048, request

at this point you can get the ARN
and status is pending validation

there's a button create records in route53
follow that flow, it'll make a cname record
in route53 you can see it made the record
still pending, taking a few minutes, then issued, green check

then put the ARN in .env and run the script:

$ serverless deploy

											_ _ 
	___ _ __ ___   __ _(_) |
 / _ \ '_ ` _ \ / _` | | |
|  __/ | | | | | (_| | | |
 \___|_| |_| |_|\__,_|_|_|
													

== email

goals and steps:
[x]review how you setup email forwarding already in your personal cloudflare account
[x]setup forwarding so support@cold3.cc forwards to another domain
[x]configure noreply@cold3.cc to correctly bounce or kill email
[x]configure DKIM and SPF for maximum security and correctness
[]get a lambda at api.net23.cc to send email from noreply@cold3.cc on surface and deep in headers

cloudflare dashboard for previous example domain, where you setup pages and email forwarding
websites, example.com
back on the left, dns, records
seeing these records as a result of setting up pages and email forwarding:

AAAA   www          100::                                         Proxied
CNAME  example.com  example-com.pages.dev                         Proxied
MX     example.com  route3.mx.cloudflare.net                      DNS only
MX     example.com  route2.mx.cloudflare.net                      DNS only
MX     example.com  route1.mx.cloudflare.net                      DNS only
TXT    example.com  "v=spf1 include:_spf.mx.cloudflare.net ~all"  DNS only

back on the left, email
configuration summary, custom addresses, 2 dns records, routing status enabled, email dns records configured

cloudflare dashboard for cold3.cc, where so far you've setup pages and workers but no email yet
websites, cold3.cc
back on the left, dns, records
there are only two:

AAAA   www       100::            Proxied
CNAME  cold3.cc  cold3.pages.dev  Proxied

back on the left, email, just a link to this documentation:
https://developers.cloudflare.com/email-routing/
and a big blue get started button, clicking it

breadcrumbs are
1 Create a custom address
2 Verify destination address
3 Configure your DNS

you're goign to set these two up now:

k----@cold3.cc   -> ke---@efa--.ll-
support@cold3.cc -> support@efa--.ll-

on the first one, step 2 is they send you an email link
and step 3 is they show you the extra dns records they'll add
then to add the second forwarding address click
email, email routing, routing rules, custom addresses, create address

you see how you can create an alias that drops emails, and will set that up now, as well
cloudflare dashboard, cold3.cc, email, email routing, routing rules, custom addresses, create address
custom address, noreply@cold3.cc
action, Drop

now the routing rules tab has a table with the three custom addresses
you can turn them on and off individually there, too
also, going back to dns, records, you can see the original AAAA and CNAME records
and the new MX and TXT ones that configuring email routing created

(...next, you leave the cloudflare dashboard and head over to aws)

remember, you have setup your domain ef---.ll- at namecheap and squarespace
aws has never heard of it outside of the pinpoint verification you just completed
you have working email forwarding like support@ef---.ll-
the domain you've setup at aws, net23.cc, the user should never see, not even in headers
and considering all this, your goal:
"a lambda at api.net23.cc can send an email from noreply@ef---.ll-"
should be possible

											_ _     ____  
	___ _ __ ___   __ _(_) |   |___ \ 
 / _ \ '_ ` _ \ / _` | | |     __) |
|  __/ | | | | | (_| | | |_   / __/ 
 \___|_| |_| |_|\__,_|_|_( ) |_____|
												 |/         

you started down the breadcrumbs with cold3.cc, but then read this on step 2:
"By adding your domain to Amazon SES it also allows your recipients to know that the emails coming from you. For your initial domain, we recommend verifying a domain that loads a webpage with identifiable information about your organization."
so, you need to use ef---.ll- not cold3.cc

aws dashboard labyrinth, ses homepage, get started button

here are the breadcrumbs:
Step 1 Add your email address
Step 2 Add your sending domain
Step 3 - optional Add MAIL FROM domain
Step 4 Review and get started with SES

Step 1

Add your email address, To get started with Amazon SES you must provide an email address so that we can send you a verification link. This verification process shows us you're the owner of the email address., A verification email will be sent to you at this address.

support@ef---.ll-

Step 2

Add your sending domain; A domain identity usually matches your website or business name. Amazon SES needs to be linked to your domain and verified in order to send emails to your recipients through SES. By adding your domain to Amazon SES it also allows your recipients to know that the emails coming from you. For your initial domain, we recommend verifying a domain that loads a webpage with identifiable information about your organization; To verify ownership of a domain, you must have access to its DNS settings to add the necessary records.

ef---.ll-

Step 3

Add MAIL FROM domain - optional; Configuring a custom MAIL FROM domain for messages sent from this identity enables the MAIL FROM address to align with the From address. Domain alignment must be achieved in order to be DMARC compliant; The MAIL FROM domain refers to the domain that appears in the 'From' field of an email message and is recommended for better deliverability, reputation management, and branding purposes. The MAIL FROM domain must be a subdomain of the verified identity from which you’re sending.

mail.ef---.ll-

Behavior on MX failure; Choose which action Amazon SES should take if your MAIL FROM domain's MX (Mail Exchange) record is not set up correctly

(x) Use default MAIL FROM domain; Will use a subdomain of amazonses.com instead of your custom MAIL FROM domain.
( ) Reject message; Will automatically reject the message without sending it.

Done

You are in Sandbox; In a sandbox environment, you can use all of the features offered by Amazon SES; however, certain sending limits and restrictions apply. When you're ready to move out of the sandbox, submit a request for production access. Before you submit a request for production access you must complete the tasks below.
[]Verify email address; To verify ownership of this email, check your inbox for a verification request email and click the link provided.
[]Send test email (Optional but recommended); Amazon SES mailbox simulator lets you test how your application handles different email sending scenarios.
[]Verify sending domain; Click on the button below and add the generated CNAME records to your domain's DNS provider.

Get DNS Records

there's a csv file to download with 3 CNAME, 2 TXT, and a MX record
going to namecheap to add them
already in namecheap are dns records for squarespace, and google workspace

(notes)

chatgpt suggested "mail" as the subdomain name
and explained there are four things i should configure here:
chat suggested dns records, as well, all of which are covered by amazon's instructions
SPF (Sender Policy Framework) Ensures AWS SES is authorized to send emails on behalf of your domain.
DKIM (DomainKeys Identified Mail) Adds email authentication to prevent spoofing.
DMARC (Domain-based Message Authentication, Reporting & Conformance) Helps manage and report on email authentication.
MAIL FROM Domain. Aligns the MAIL FROM domain with your From address for DMARC compliance and improved deliverability.

											_ _     _____ 
	___ _ __ ___   __ _(_) |   |___ / 
 / _ \ '_ ` _ \ / _` | | |     |_ \ 
|  __/ | | | | | (_| | | |_   ___) |
 \___|_| |_| |_|\__,_|_|_( ) |____/ 
												 |/         

2024jul3 going hard on dns for ef---.ll- to setup a net23 lambda that sends a message from ef---.ll-
first, some dns settings that are true:

amazon says, add these records to the dns for ef---.ll-:

CNAME  lzbihthz6dhrqvobiijpla77a4zfd5mo._domainkey.ef---.ll-  lzbihthz6dhrqvobiijpla77a4zfd5mo.dkim.amazonses.com
CNAME  ruhjtsmevec56krss4pvkw6b5kwalsmy._domainkey.ef---.ll-  ruhjtsmevec56krss4pvkw6b5kwalsmy.dkim.amazonses.com
CNAME  xhea6vwl7aw62ocquwlrerepecsrl6da._domainkey.ef---.ll-  xhea6vwl7aw62ocquwlrerepecsrl6da.dkim.amazonses.com
MX     mail.ef---.ll-                                         10 feedback-smtp.us-east-1.amazonses.com
TXT    mail.ef---.ll-                                         "v=spf1 include:amazonses.com ~all"
TXT    _dmarc.ef---.ll-                                       "v=DMARC1; p=none;"

remember that at this point, ef---.ll- is a squarespace with email through google workspace
namecheap says, these records are already current on ef---.ll-:

A      @                     198.185.159.144
A      @                     198.185.159.145
A      @                     198.49.23.144
A      @                     198.49.23.145
CNAME  www                   ext-cust.squarespace.com.
CNAME  3xwzwwzn2sx39xjmdp8w  verify.squarespace.com.

and doesn't show you the gmail ones, those they provide automatically
nslookup says, these are the current mx records for ef---.ll-:

MX  ef---.ll-  preference = 10, mail exchanger = aspmx3.googlemail.com
MX  ef---.ll-  preference = 5, mail exchanger = alt1.aspmx.l.google.com
MX  ef---.ll-  preference = 5, mail exchanger = alt2.aspmx.l.google.com
MX  ef---.ll-  preference = 1, mail exchanger = aspmx.l.google.com
MX  ef---.ll-  preference = 10, mail exchanger = aspmx2.googlemail.com

by the way, here are cloudflare's current dns records for cold3.cc:

AAAA   www       100::
CNAME  cold3.cc  cold3.pages.dev
MX     cold3.cc  route3.mx.cloudflare.net
MX     cold3.cc  route2.mx.cloudflare.net
MX     cold3.cc  route1.mx.cloudflare.net
TXT    cold3.cc  "v=spf1 include:_spf.mx.cloudflare.net ~all"

options for how to get this all combined:
(option 1) keep your dns at namecheap. less of a change, but more cowboy. website should stay up, but gmail could go down
(option 2) move dns to cloudflare. closer to eventual finish, squarespace could go down, saves $7/month on gmail
also, chatgpt says the mx records won't conflict because the amazon one is mail.ef---.ll- and the google ones are ef---.ll-
you're also noticing a txt record about spf1, but hopefully the mail.ef---.ll- and ef---.ll- will keep those from conflicting, too

you're picking option 2, if the squarespace goes down nobody's looking at it right now, less cowboy, closer to finish

steps for option 2:
[x]manually forward email you want from k----@ef---.ll- gmail to ef---ll-@gmail.com before you break squarespace google workspace
[x]namecheap stays the registrar, but move DNS from namecheap to cloudflare, like you did for cold3.cc
[x]get squarespace working again, first just put in the same records as before
[x]setup email routing in cloudflare, follow steps you just followed for cold3.cc for k----@, support@, noreply@
[x]add amazon records and complete domain verification on the ses dashboard
[x]apply to be let out of the sandbox

official steps above, some more notes within steps:
web records that point to squarespace will hopefully stay the same
intentonally break squarespace google workspace, unsignup to save $7/month, delete chrome profile
setup email routing in cloudflare, everything goes to ef---ll-@gmail.com

== 2024jul4 move ef---.ll- dns from namecheap to cloudflare

manually forwarded email from k----@ef---.ll- to ef---ll-@gmail.com
in squarespace, cancelled google workspace

cloudflare ef---ll- dashboard
websites, lists coldstart.cc, cold2.cc, cold3.cc
big blue button, add a site

link to documentation: https://developers.cloudflare.com/learning-paths/get-started
"This process sets up your web traffic to proxy through Cloudflare. Proxying speeds up and protects websites and services served by this domain."
https://developers.cloudflare.com/fundamentals/concepts/how-cloudflare-works
"If the domain’s status is active and the queried DNS record is set to proxied, then Cloudflare responds with an anycast IP address, instead of the value defined in your DNS table. This effectively re-routes the HTTP/HTTPS requests to the Cloudflare network, instead of directly reaching the targeted the origin server.
"In contrast, if the queried DNS record is set to DNS only, meaning the proxy is off, then Cloudflare responds with the value defined in your DNS table (that is, an IP address or CNAME record). This means HTTP/HTTPS requests route directly to the origin server and are not processed or protected by Cloudflare."
cold3.cc dashboards show DNS setup: full, and Proxied on

ef---.ll-, free plan
cloudflare says it's bringing in existing dns records, and it found
4 A, 1 CNAME, 5 MX
shoulda found 2 cname, but whatever

cloudflare says "Avoid DNS resolution issues caused by DNSSEC; Find the DNSSEC setting at your registrar (per-provider instructions). If it is on, you have two options: Turn DNSSEC off at least 24 hours before updating your nameservers. Most common; Migrate your existing DNS zone without turning off DNSSEC. More advanced; After your domain activates, we recommend turning DNSSEC on through Cloudflare."
namecheap dashboard, dnssec is off
is dnssec on for cold3.cc? no! you found the start of the flow in cloudflare, websites, cold3.cc, dns, settings, first box is dnssec
https://developers.cloudflare.com/dns/dnssec/
ttd july2024 add dnssec to cold3.cc as practice and then ever.fans as production, following steps in cloudflare and namecheap

namecheap, ef---.ll-, before changes:
nameservers is set to Namecheap BasicDNS
advanced dns tab shows the records above, the 4 A records and CNAME
dnssec is off
mail settings is Gmail, text says Gmail automatically configured for ef---.ll-

switching from Namecheap Basic DNS to Custom DNS
entering the nameservers from cloudflare:

carl.ns.cloudflare.com
maya.ns.cloudflare.com

2024jul4 1:13p finished flow on cloudflare, which says "Cloudflare is now checking the nameservers for ef---.ll-. Please wait a few hours for an update."
1:23p "Great news! Cloudflare is now protecting your site; Data about your site's usage will be here once available."
dnschecker.org also shows maya and carl for ef---.ll-, just like cold3.cc
your website site is down, ssl mismatch

cloudflare dashboard, quick start guide
automatic https rewrites, yes
always use https, yes

cloudflare dns records
4 A are correct
only one CNAME, [x]add the second
5 MX records all about gmail, [x]remove them, so now they again look like this:

A      @                     198.185.159.144
A      @                     198.185.159.145
A      @                     198.49.23.144
A      @                     198.49.23.145
CNAME  www                   ext-cust.squarespace.com
CNAME  3xwzwwzn2sx39xjmdp8w  verify.squarespace.com

you pasted a trailing period when adding the verify cname, but cloudflare isn't showing it

sqarespace dashboards, red text and boxes that says "DNS Error, We couldn't verify DNS settings with Namecheap."
and then they have a table of right and wrong settings, but it changes every time you refresh
cloudflare dashboard, ef---.ll-, dns records, turned proxy off on all records
at squarespace, refresh still is crazy
changed nothing, refreshed site, ef---.ll- is back online, ssl looks ok
squarespace dashboard, settings, domains and email, domains managed by third-party, ef---.ll-, red message changed to green Connected

interestingly(?) in firefox it's really easy to see who issued the certificate, and right now it shows
net23.cc: Verified by: Amazon (makes sense, as this ssl came from AWS ACM)
cold3.cc: Verified by: Google Trust Services (this is cloudflare pages and workers)
ef---.ll-: Verified by: Let's Encrypt (this is cloudflare dns, squarespace hosting, no idea where the ssl came from)

ok, on to setting up email forwarding
cloudflare dashboard, ef---.ll-, email, email routing, get started
destination already verified, so that step is faster

k----@ef---.ll-   -> ef---ll-@gmail.com
support@ef---.ll- -> ef---ll-@gmail.com
noreply@ef---.ll- -> Drop

all the tests worked, except nothere@ef---.ll- didn't bounce, but whatever

now at last you go back to the amazon steps
you configured aws at mail.ef---.ll-
you think later you can configure twilio at mail2.ef---.ll-, ttd july2024

aws dashboard, ses
yellow note saying im in the sandbox
verify sending domain, get dns records, download record set, got a csv file:

CNAME  lzbihthz6dhrqvobiijpla77a4zfd5mo._domainkey.ef---.ll-  lzbihthz6dhrqvobiijpla77a4zfd5mo.dkim.amazonses.com
CNAME  ruhjtsmevec56krss4pvkw6b5kwalsmy._domainkey.ef---.ll-  ruhjtsmevec56krss4pvkw6b5kwalsmy.dkim.amazonses.com
CNAME  xhea6vwl7aw62ocquwlrerepecsrl6da._domainkey.ef---.ll-  xhea6vwl7aw62ocquwlrerepecsrl6da.dkim.amazonses.com
MX     mail.ef---.ll-                                         10 feedback-smtp.us-east-1.amazonses.com
TXT    mail.ef---.ll-                                         "v=spf1 include:amazonses.com ~all"
TXT    _dmarc.ef---.ll-                                       "v=DMARC1; p=none;"

left proxying on entering the first cname, and got an error that says it can't be proxied

entering the mx record, there's a box that says, Priority (required)
you're going to enter just feedback-smtp.us-east-1.amazonses.com as the value, and separately dial in priority 10
also, you entered the name "mail.ef---.ll-" but cloudflare is showing it as just "mail"

for the txt record, cloudflare gives you a larger box, you're pasting in as above including the quotes
it saves with the quotes

ok, they're all in there, a mix of records about squarespace, about cloudflare email forwards, and about ses
only some things are proxyable, but you've turned off proxying on all of them

amazon doesn't have a check now button, rather it says
"Identity status: Verification pending; Last checked: July 4, 2024 at 14:21 (UTC-04:00)" and it's 14:56
you might have gotten the records with that button, and then not set them quick enough for the first check
15:02 without refresh, looking back at the page, green checkmark Verified

last of three boxes on ses page is to send test email, so let's see what's in there
"Send test email Info; The Amazon SES mailbox simulator lets you test how your application handles different email sending scenarios."
from is already set to support@ef---.ll-
there's no to box, it looks like messages go to your choice of options like success@simulator.amazonses.com
there is a cc box, so let's try that? ef---ll-@outlook.com
doesn't like it because not a verified identity

aws dashboards, ses, configuration, identities
already there are two listed, both green verified:

ef---.ll-, Domain
support@ef---.ll-, Email address

so you need to [x]turn noreply into a forward, [x]verify it, then [x]turn it back into a drop target
this is easy to do you just edit the existing row
verified that one, and also ef---ll-@outlook.com
and sent a test email. now the box in the ses dashboard says green check test email sent, but you haven't gotten it in outlook yet, but whatever

aws dashboard, ses, get set up
box on the top, you're still in the sandbox (but now the boxes below are all green), button request production access

== filled out the aws ses form to request production access

To help us evaluate your request for production access, fill out the following form outlining how you plan to use Amazon SES to send email once your account has moved out of the sandbox; Production access means you can send email to any recipient, regardless of whether the recipient's address or domain is verified. However, you must still verify all identities that you use as "From", "Source", "Sender", or "Return-Path" addresses.
https://docs.aws.amazon.com/ses/latest/dg/request-production-access.html
and they say approval takes just 24 hours, which is great

Mail type; Choose the option that best represents the types of messages you plan on sending. A marketing email promotes your products and services, while a transactional email is an immediate, trigger-based communication.

Transactional

Website URL; Provide the URL for your website to help us better understand the kind of content you plan on sending.

https://www.ef---.ll-/

Use case description: Explain how you plan to use Amazon SES to send email. Specifically, tell us:
-How do you plan to build or acquire your mailing list?
-How do you plan to handle bounces and complaints?
-How can recipients opt out of receiving email from you?

next day, approved: "Thank you for submitting your request to increase your sending limits. Your new sending quota is 50,000 messages per day. Your maximum send rate is now 14 messages per second. We have also moved your account out of the Amazon SES sandbox." huzzah. with great power comes great responsibility. and Network 23 will use it wisely

											_ _     _  _   
	___ _ __ ___   __ _(_) |   | || |  
 / _ \ '_ ` _ \ / _` | | |   | || |_ 
|  __/ | | | | | (_| | | |_  |__   _|
 \___|_| |_| |_|\__,_|_|_( )    |_|  
												 |/          

[]make a hello world lambda
[]make lambdas that send sms and email
[]secure them so only cold3.cc can call them















	 _                          _                 
	(_)___  __   _____ _ __ ___(_) ___  _ __  ___ 
	| / __| \ \ / / _ \ '__/ __| |/ _ \| '_ \/ __|
	| \__ \  \ V /  __/ |  \__ \ | (_) | | | \__ \
 _/ |___/   \_/ \___|_|  |___/_|\___/|_| |_|___/
|__/                                            

coding with big int literals, nuxt freaked out
it seems like it was building everything to es2019, and those are es2020
also, soon you're going to start using library functions from lambdas, which are node
so it's time to figure out what javascript version you want, and set it explicitly

here's what amazon supports for lambda right now, 2024jul:
https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html
Node.js 20, current
Node.js 18, current
Node.js 16, deprecation date 2024jun12
so from that, picking Node 20

you just updated node and npm on your development workstation:
$ node --version, 20.15.0
$ npm --version, 10.8.1
so that matches well

it seems there isn't a good way to match a node version to an esXXXX version
because platform vendors, like google making v8, implement different features from the spec at different times
this page is detailed, but useless:
https://v8.dev/features

so what version is cloudflare workers running?
https://developers.cloudflare.com/workers/runtime-apis/web-standards/
"The Workers runtime is updated at least once a week, to at least the version of V8 that is currently used by Google Chrome’s stable release. This means you can safely use the latest JavaScript features, with no need for transpilers."
ok, so that means way more recent than 2019 or 2020

$ nuxt build was the simplest command to hit the big int literal error:

	Kevin@ncom3 MINGW64 /.../repot1 (main)
	$ npm run build
	> nuxt build
	Nuxt 3.12.3 with Nitro 2.9.7
	i Building client...

	[nitro]  ERROR  Error: Transform failed with 2 errors:
	C:\Documents\code\code21site\repot1\server\api\myapi.ts:12:8: ERROR: Big integer literals are not available in the configured target environment ("es2019")

(as an aside: this doesn't really make sense because even if it is targeting es2019, shouldn't transplation correctly turn my big int literals into something that es2019 can understand? isn't that the whole point of transplilation?)
this configuration addition fixes it, in /nuxt.config.txt

	export default defineNuxtConfig({
		compatibilityDate: '2024-04-03',
		devtools: { enabled: true },
		nitro: {
			preset: "cloudflare-pages",
			esbuild: { options: { target: 'esnext' } }// <-- new line that fixes big int literal error
		},
		modules: ["nitro-cloudflare-dev"],
	})

so that's cool. "es2020" also fixes it, but you're using "esnext", because:
2020 was still four years ago,
es2023 and es2024 might not be real yet, and
esnext is also a good match for cloudflare's "what's our version? current!" thing



(bookmark)
then add more here about versioning net23 lambdas with babel transpilation to node20








										 _       _           
 _ __ ___   ___   __| |_   _| | ___  ___ 
| '_ ` _ \ / _ \ / _` | | | | |/ _ \/ __|
| | | | | | (_) | (_| | |_| | |  __/\__ \
|_| |_| |_|\___/ \__,_|\__,_|_|\___||___/
																				 

first, you want wrangler and serverless installed locally, not globally
commands to install, version, and uninstall globally:
$ npm install -g wrangler
$ npm list -g wrangler
$ npm uninstall -g wrangler
serverless 4.0.35 was installed globally, uninstalled that to install serverless 3 in project

commands to build up net23 package.json
note that you have to install serverless version 3, as current is 4 but even serverless-webpack, which has 235k weekly downloads, isn't ready for serverless 4
all of these modules are plenty popular, except perhaps for serverless-s3-sync
installing in this order, two of them were already there, but still installing to list in package.json

$ npm install -S aws-sdk                      8 million weekly downloads, added 37 packages

$ npm install -D serverless@3                 1 million, 550 packages

$ npm install -D babel-loader                15 million, 122 packages
$ npm install -D @babel/core                 45 million, this one is already there!
$ npm install -D @babel/preset-env           22 million, 112 packages

$ npm install -D webpack                     24 million, this one is already there!
$ npm install -D webpack-cli                  6 million, 32 packages

$ npm install -D serverless-webpack         235 thousand, 52 packages
$ npm install -D serverless-offline         519 thousand, 85 packages
$ npm install -D serverless-domain-manager  220 thousand, 62 packages
$ npm install -D serverless-s3-sync          32 thousand,  9 packages

as an aside, "$ serverless deploy" works, but doesn't notice changed files in www
you added a script to package.json so "$ npm run www" force syncs everything


(note later)
to avoid obscure modules, you replaced serverless-s3-sync with an aws cli command:
"www": "aws s3 sync ./www s3://www-net23-cc --delete"
$ yarn www
$ aws cloudfront create-invalidation --distribution-id E2J...KAR --paths "/*"
$ aws cloudfront list-invalidations --distribution-id E2J...KAR
it correctly deletes files in the bucket that aren't in the folder!
use create and list invalidations to get the cloudfront distribution in front of it to get new files from the bucket






(bookmark)
next three scopes:
1 hello world lambda, no custom domain, no webpack
2 lambdas as api.net23.cc
3 webpack and babel
try running local and deployed
can you see the bundle size before and after?
make an api endpoint to run tests, should be fine because local node 20 works great






 _          _ _         _                 _         _       
| |__   ___| | | ___   | | __ _ _ __ ___ | |__   __| | __ _ 
| '_ \ / _ \ | |/ _ \  | |/ _` | '_ ` _ \| '_ \ / _` |/ _` |
| | | |  __/ | | (_) | | | (_| | | | | | | |_) | (_| | (_| |
|_| |_|\___|_|_|\___/  |_|\__,_|_| |_| |_|_.__/ \__,_|\__,_|
																														

# lambda step 1: hello world and local development

notes getting started with lambda
$ npm run local
runs serverless offline, which emulates AWS Lambda and API Gateway
then you can hit your APIs at urls like:
http://localhost:3000/prod/hello1
http://localhost:3000/prod/hello2
live updates don't seem to work, but maybe that's not an intended feature

$ npm run deploy
first time, right now with two lambdas that aren't using a custom domain

# lambda step 2: manual steps to get custom domain

serverless.yml should be able to create api.net23.cc, but you're getting this error:
$ npm run deploy
Deploying net23 to stage prod (us-east-1)
Warning: V1 - 'api.net23.cc' does not exist.
Error: V1 - Make sure the 'api.net23.cc' exists.

aws dashboard, route 53, net23.cc, you see stuff for the root and www, but nothing yet for api.net23.cc
aws dashboard, api gateway, apis, there is a listing named prod-net23, and clicking into it you do see hello1 and hello2

ok, instead of trying to get this one-time configuration to be created by serverless.yml,
you're going to document steps here to set it up manually:

aws dashboard, api gateway, custom domain names
api.net23.cc
tls 1.2
regional (not edge optimized)
net23.cc, choose ACM certificate from drop down, only one there
create domain name button, green success banner

aws dashboard, api gateway, custom domain names, api.net23.cc
configurations, endpoint configuration, api gateway domain name
copy this as you'll paste it into route53 in a moment

aws dashboard, api gateway, custom domain names, api.net23.cc
api mappings, second tab
configure api mappings, add new mapping
api dropdown, one thing listed, what was created by serverless, prod-net23 (REST)
prod, stage, only option
path (optional), leave blank because you want root
save

aws dashboard, route53, hosted zones, net23.cc, create record
api, record name
A, record type, already set to this
switch on Alias
Route traffic to, dropdown with lots of options, Alias to API Gateway API
us-east-1, N. Virginia, region
choose endpoint, dropdown with one item, suggests the domain like blah.execute-api.us-east-1.amazonaws.com we just created
simple routing policy, yes evaluate target heath, leaving these defaults
create records

back in route23 for net23.cc, all your dns records from before are there, along with the new one for api.net23.cc
and it looks like it's directing to, and saying, the under-the-hood domain like:
blahblah.execute-api.us-east-1.amazonaws.com

# lambda step 3: turn off side door access, hard or impossible and gave up

related to the checks below, there's the www | root X http | https checks
the api subdomain means we don't have to worry about www | root here, which is by design
checking http, chrome takes a long time and then reports connection refused
firefox takes a long time, and then redirects to https
so all that looks ok for now

https://api.net23.cc/hello1 ~ works, correctly
https://jkXXXXXXo3.execute-api.us-east-1.amazonaws.com/prod/hello1 ~ works, wrongly, id from api gateway
https://d-1hXXXXXXyh.execute-api.us-east-1.amazonaws.com/prod/hello1 ~ blocked, correctly, value from route53

as always with aws, there are manual steps or you can try automatic
manual steps involve pasting in code
automatic steps involve a secret that includes your aws account number

as both include blocks of json, trying infrastructure as code
added ACCESS_API_GATEWAY_RESOURCE_ARN to .env and section that uses it to serverless.yml

before running, checked the dashboard:
aws dashboard, api gateway, prod-net23
halfway down on the left, resource policy
starting out seeing policy details, no resource policy

ok, spent a few hours going in circles with chatgpt trying to turn off the side door
chat repeatedly went to controlling who can call the function, not where the function is callable
using the referer header, which of course can be spoofed anyway
then got it to explain that aws doesn't have a way to turn off access at the amazonaws.com domain
"Unfortunately, AWS does not provide a direct option to completely disable the default amazonaws.com endpoint for API Gateway. ... Although you cannot completely disable the default domain, you can create a Lambda@Edge function or use API Gateway’s HTTP integration to redirect any traffic from the default domain to your custom domain."
so that's way too much work and complexity for this that likely doesn't even matter, giving up

# lambda step 4: let in webpack and babel to reduce bundle size and use modules

$ npm run pack
$ npm run deploy

pack just runs webpack, building to dist/hello1.js
deploy does that as part of the serverless framework upload to aws lambda
before adding webpack, each hello was 7.1 MB; now serverless framework says it's 157 bytes
on disk from pack its 494 bytes, though, but whatever

also, deploy seems to delete the dist folder!
it's there after pack, gone after deploy

fought an error about ES6 module syntax or not in the webpack configuration
generally, trying to pick ES6 module syntax everywhere because works in the browser and server, more recent and current, and helps tree shaking perform best
right now it's working with these choices:
-package.json includes "type": "module", keeping ES6 setting
-webpack.config.cjs ends .cjs not .js or .mjs, reverting to CommonJS setting
-serverless.yml custom.webpack.webpackConfig references the webpack config with that extension
-insde webpack.config.cjs, require, module.exports, and __dirname are all legacy Node that matches CommonJS
but also, there are two mentions of commonjs2, setting the output format, necessary for webpack to bundle for lambda's node environment

# lambda step 5: three horrible hacky workarounds

here are scripts in package.json:

	"scripts": {
(1) "www": "serverless s3sync"
(2) "deploy": "webpack && serverless deploy"
(3) "local": "serverless offline --config local.yml",

(1) sync the www bucket
serverless deploy should sync the www bucket
except it doesn't
so instead www calls s3sync directly

(2) pack and upload lambda functions
serverless deploy should run webpack and then upload the packed functions
except it like deletes them and then can't find them or something
so intead running webpack manually beforehand works
and npm run deploy does this in a single command that bundles two steps

(3) run lambda functions locally
serverless-offline can emulate api gateway and lambda locally
except somehow it can't find the named handler in the output bundle
so this hack uses a separate yml config that points at the source files
and that way, runs them fine

here are some paths related to all this:

on disk:
./src/hello1.js ~ source code file
./dist/hello1.js ~ bundled webpack output file

in settings in local.yml:
	handler: src/hello1.handler # Paths to src to run before webpack
in settings in serverless.yml:
	handler: dist/hello1.handler # Paths to dist to upload after webpack

in code in ./src/hello1.js:
	"... export const handler = async (event) => {..."
in code in ./dist/hello1.js:
	"...e.d(o,{handler:()=>t});const t=async..."

on the web:
	http://localhost:3000/prod/hello1 ~ path to function emulated locally
	https://api.net23.cc/hello1 ~ path to function deployed  

you've seen subfolders like src/handlers and dist/service appear, but for the moment at least, have managed to keep them away

so, this is working, but hacky as f**k with three workarounds, and you haven't even tested using a module or library code yet, ugh

# lambda step 6: switched to rollup

switched from webpack and babel to rollup, and everything works great
$ npm run local   is "local": "rollup -c && serverless offline",
$ npm run deploy  is "deploy": "rollup -c && serverless deploy",
and the serverless-webpack module is replaced with nothing

here is net23's package.json with rollup, looking at popularity and recency:

serverless                 1 million      1month
serverless-offline           536k         2months
serverless-domain-manager    198k         5months
serverless-s3-sync           31k          6months ~ a little too rare

rollup                        22 million  1month
@rollup/plugin-commonjs        3 million  1month
@rollup/plugin-node-resolve    6 million  9months
@rollup/plugin-terser          1 million  9months
@rollup/plugin-json            2 million  7months
rollup-plugin-node-polyfills     776k     5years ~ a little too old

# lambda step 7: great success

here's what we finished:
[x]hello lambda
[x]develop locally, using serverless framework's emulation of api gateway and lambda
[x]do some math big int style
[x]use a node module directly, nanoid probably
[x]import library code
[x]run all the tests

																						 _      
	__ _  ___ ___ ___  ___ ___    ___ ___   __| | ___ 
 / _` |/ __/ __/ _ \/ __/ __|  / __/ _ \ / _` |/ _ \
| (_| | (_| (_|  __/\__ \__ \ | (_| (_) | (_| |  __/
 \__,_|\___\___\___||___/___/  \___\___/ \__,_|\___|
																										

only cold3.cc should be able to call network 23 apis
in addition to cors and checking the origin header and so on,
the simplest way this is secured is with an api access key
this is just like how we access supabase and other third party apis
except here, we're coding both sides
here's an example:


when cold3 makes a request to net23, it includes this access key
cold3 gets the key from cloudflare secrets
net23 checks the key from lambda environment variables or aws secret manager or whatever

you need to store this secret a lot of places:
-env files for local development, both cold3 and net23
-cloudflare secrets
-lambda's equivalent of cloudflare secrets

.env:
ACCESS_NETWORK_23=VeryLongSecretCodeLikeDsqmB9YxbtseDuUHwyJnjDsqmB9YxbtseDuUHwyJnjAndSoOn

cloudflare:
cloudflare dashboard, workers and pages, cold3, settings tab, environment variables

serverless.yml:
provider:
	environment: # Name variables in .env file here so they are on process.env for the Lambda code
		ACCESS_NETWORK_23: ${env:ACCESS_NETWORK_23}

and then in lambda code like hello2.js:
	let access = 'not found'
	if (typeof process.env.ACCESS_NETWORK_23 == 'string') access = process.env.ACCESS_NETWORK_23.length

you don't have to mess with the aws dashboard, which is great

			 _             
 _ __ (_)_ __   __ _ 
| '_ \| | '_ \ / _` |
| |_) | | | | | (_| |
| .__/|_|_| |_|\__, |
|_|            |___/ 

2024sep setup ping pages and got service providers to hit them and generate nice graphs

cold3.cc/ping/ping1                                            -a nuxt page with no script tag
cold3.cc/ping/ping2                                            -a nuxt page with a trivial script tag
cold3.cc/ping/ping3 > cold3.cc/api/ping3                       -a nuxt page that fetches to a trivial nuxt api
cold3.cc/ping/ping4 > cold3.cc/api/ping4 > Supabase            -all that, but the server api code queries the global count from supabase
cold3.cc/ping/ping5 > cold3.cc/api/ping5 > api.net23.cc/ping5  -instead of calling supabase, fetches to our own trivial lambda

example output, deployed to cloud, curl and browser:

ping1: template says: ping1done (curl)
ping1: template says: ping1done (browser)

ping2: script setup says: CloudPageServer:Envi.Proc.Scri.Self.Serv.Zulu,           1725904851660, v2024sep8b, ping2done (curl)
ping2: script setup says: CloudPageClient:Achr.Asaf.Awin.Docu.Doma.Self.Stor.Wind, 1725904716570, v2024sep8b, ping2done (browser)

ping3: fetch to worker took 0ms to say: worker says: CloudNuxtServer:Aclo.Envi.Proc.Scri.Self.Zulu, 1725904908685, v2024sep8b, ping3done (curl)
ping3: fetch to worker took 2ms to say: worker says: CloudNuxtServer:Aclo.Envi.Proc.Scri.Self.Zulu, 1725904728918, v2024sep8b, ping3done (browser)

ping4: fetch to worker to database took 494ms to say: worker says: database took 494ms to get count 569, CloudNuxtServer:Aclo.Envi.Proc.Scri.Self.Zulu, 1725904949932, v2024sep8b, ping4done (curl)
ping4: fetch to worker to database took   2ms to say: worker says: database took 217ms to get count 569, CloudNuxtServer:Aclo.Envi.Proc.Scri.Self.Zulu, 1725904740498, v2024sep8b, ping4done (browser)

ping5: fetch to worker to lambda took 354ms to say: worker says: lambda took  354ms to say: lambda says: CloudLambda:Eigh.Envi.Glob.Lamb.Node.Proc.Regi.Zulu, 1725904979928, v2024sep8b, ping5done (curl)
ping5: fetch to worker to lambda took   2ms to say: worker says: lambda took 1277ms to say: lambda says: CloudLambda:Eigh.Envi.Glob.Lamb.Node.Proc.Regi.Zulu, 1725904754597, v2024sep8b, ping5done (browser)

notes making sense of these results
ping2, curl is from server rendering, so zulu time, and browser replaces that with client rendering, which has a window object
ping3, server render adds 0ms because it's all happening at once; senseEnvironment the same because it's always from a fetch to the nuxt api
ping4 and ping5, real times are on curl page, when re-rendered on the client it must cache or skip or something so you get 2ms instead of nearly half a second
and while supabase takes a half second, lambda frequently takes over a second

looked for service providers with these capabilities:
-simple request, don't emulate a rendering, code-running browser
-try from all over the world, randomly
-look for text in the page that you get
-say and show how long it took, average and higher percentile

trying out:
-pingdom.com - västerås now owned by solarwinds in austin, $33/month, strange duplication of uptime and speed, graphs good
-checklyhq.com - berlin, $0 hobby plan, graphs great, nicely formatted html result view
-uptime.com - nyc, $20/month, graphs ok
so you'll probably just go with checkly
they talk about complex stuff with playwright, but you found their simple thing

for each of their dashboards, you found and set:
-name
-url
-look for text like "ping5done" in page
-frequency, 1hr
-round robin or lots of random global locations

it'll be interesting to see if this affects your cloudflare and amazon bills :0
and this was a good trail, other than senseEnvironment() being a messy day in the bike shed

      _                 _    __                  _   _             
  ___| | ___  _   _  __| |  / _|_   _ _ __   ___| |_(_) ___  _ __  
 / __| |/ _ \| | | |/ _` | | |_| | | | '_ \ / __| __| |/ _ \| '_ \ 
| (__| | (_) | |_| | (_| | |  _| |_| | | | | (__| |_| | (_) | | | |
 \___|_|\___/ \__,_|\__,_| |_|  \__,_|_| |_|\___|\__|_|\___/|_| |_|
                                                                   
                       _ _      _ _          _   _                                       
 _ __   __ _ _ __ __ _| | | ___| (_)______ _| |_(_) ___  _ __   __      _____   ___  ___ 
| '_ \ / _` | '__/ _` | | |/ _ \ | |_  / _` | __| |/ _ \| '_ \  \ \ /\ / / _ \ / _ \/ __|
| |_) | (_| | | | (_| | | |  __/ | |/ / (_| | |_| | (_) | | | |  \ V  V / (_) |  __/\__ \
| .__/ \__,_|_|  \__,_|_|_|\___|_|_/___\__,_|\__|_|\___/|_| |_|   \_/\_/ \___/ \___||___/
|_|                                                                                      

in the eternal triangular tug of war between simplicity, reliability, and speed
you've decided for initial launch to keep cloud functions synchronous
what follows is a discussion of fancier things you tried, and why they were difficult, worrysome, or didn't work at all

[I.] you can fetch in parallel in your own code while handling an api request

these will run one after the other:

await fetch('https://datadog.com/api/log', ...)
await fetch('https://datacat.com/api/log', ...)

and these will run both at the same time:

let promiseDog = fetch('https://datadog.com/api/log', ...)
let promiseCat = fetch('https://datacat.com/api/log', ...)
await Promise.all([promiseDog, promiseCat])

and that code is fine in workers and lambda
the difficulty comes in when you have everything you need to get back to the client,
and want to do that right away, with cleanup tasks like logging happening in parallel or afterwards
it doesn't work because cloudflare and lambda see the request sent, think we're done here, and tear down the environment

[II.] performing tasks in a worker after responding to the client is possible but more complicated

cloudflare pins a method to the event object:

event.waitUntil(p)

you can pass it a promise, and then the worker will delay teardown,
even after you've responded to the client and returned from everything
so that's great. there is no lambda equivalent you can find

[III.] performing tasks in a lambda after responding to the client is either not possible or so rare you can't find it

the return from a lambda handler is the response back up to the client
you can't find a way to talk to the client before returning

the common pattern for lambda functions starts like this:
	exports.handler = async (event, context) => {

and an alternative documented form that looked promising (get it?) is this:
	exports.handler = (event, context, callback) => {

but to do what you want, you'd have to combine them,
which either won't work or is so weird no one else has thought of it:
exports.handler = async (event, context, callback) => {

	//first, we process the request, using steps that we do have to await
	let messageForClient = await processRequest({event, context})

	//second, we get back to the client quickly
	const response = {
		statusCode: 200,
		body: JSON.stringify({ message: 'OK', messageForClient }),
	}
	callback(null, response)

	//third, perform tasks like logging. the response has been sent, but the lambda must keep running!
	let p = performLoggingTasks()
	return p//by returning this promise, we're telling lambda to not tear down until it resolves!
}

looking more shed even more doubt on this--the returned promise p needs to resolve into the response body
at which point we're all the way back where we started

chat says lambda doesn't even support streaming a response back to a client
the whole thing, headers and complete body, needs to go out once at the end

and there's also something called context.succeed() as an alternative to callback
but it seems like that's an older api that has been replaced by the callback parameter
not a different thing that could help you here

and hours later you found callbackWaitsForEmptyEventLoop, which is already set to true
which could keep the lambda alive long enough to finish logging
but might also delay sending a returned response back to the client

so, you found a lot of complexity and very little certainty in this exploration, and are backing out

[IV.] so here are some features you're removing

const defaultFetchTimeLimit = 5*Time.second
async function ashFetchum(c, q) {//early nickname to wrapped fetch; gotta fetch 'em all
	if (!q.timeLimit) q.timeLimit = defaultFetchTimeLimit
	let response, bodyText, body, error, success = true

	let a = new AbortController()//js way of telling fetch to give up after a time limit
	let m = setTimeout(() => a.abort(), q.timeLimit)
	let o = {method: q.method, headers: q.headers, body: q.body, signal: a.signal}
	q.tick = Now()

	try {
		if (q.skipResponse) {
			let p = fetch(q.resource, o)             //get the promise instead of awaiting here for it to resolve
			if (event?.waitUntil) event.waitUntil(p) //tell cloudflare to not tear down the worker until p resolves
			p.then(() => { clearTimeout(m) })
		} else {
			response = await fetch(q.resource, o); clearTimeout(m)

in your wrapped server side fetch, there used to be
-a fire and forget option to skip waiting for the response entirely; the call isn't even async anymore
-an option to skip reading the body
-an upper time limit imposed by an abort controller

to provide the fire and forget feature, you'd have to save the promise and
on cloudflare, feed it to event.waitUntil()
on lambda, manually await it before returning
this is possible, but far less simple and likely less reliable,
either because you make a coding error due to the more complex flow
or because it causes cloudflare or amazon to act in an unusual way
also there's no benefit on lambda, only cloudflare

you could do the option to not read the body, but
the body is probably short,
has probably already arrived,
and you probably always want to log it to have a full audit of api behavior

abort controller is is a feature of fetch, and interesting, but
workers have a not configurable 30 second maximum lifetime, and
you've set your lambdas to match, from a default of 3 seconds and upper limit of 15 minutes

[V.] the big surrounding picture here

you're picking servlerless offerings like cloudflare workers, lambda, and supabase for simplicity, reliability, and scalability
you don't have to maintain a server, and if things get popular overnight, nothing should break

but this also means that every read, write, and log is another ~150ms
many user interactions will involve a handful of these, back to back
and that means the experience isn't fast anymore

a traditional architecture--with web server, database, and log file all on the same virtual instance--avoids this problem
also on a regular server, you can absolutely stream a response piece by piece back up to a client
and you can finish a response, and stay alive to perform some clean up tasks afterwards

your hope is that the site still seems remarkable to users even with some steps causing second-long pauses
the first navigation will still happen in like 35ms
clicking around will be instantaneous because vue is just showing and hiding components that already have cached data
and when the user is changing their email or uploading a file, it makes sense that those tasks would take seconds

[VI.] the next day, a map if you get back to this trail

maybe this would work for lambda, or maybe its an ai hallucination, you're not sure:
exports.handler = async (event, context) => {
	context.callbackWaitsForEmptyEventLoop = false//change away from default true
	setTimeout(() => {//do 0ms later, but in the next turn of the event loop
		console.log("Performing background task like logging...")
	}, 0)
	return {//return response to client right away
		statusCode: 200,
		headers: { 'Content-Type': 'application/json' },
		body: JSON.stringify({ message: 'Response to client' })
	}
}

and then on the cloudflare side, it's context.waitUntil(), and you just found tail workers:
https://developers.cloudflare.com/workers/observability/logging/tail-workers/
specifically designed for logging and https://developers.cloudflare.com/queues/ is in beta

also, you can call console.log and console.error in both
in lambda, they automatically go into a cloudwatch log you can see in the dashboard

[VII.] the next month, reports from the field


with lambda, it never works
with cloudflare, it works most of the time, but there are missing logs
so, for reliability you also get simplicity, and never touch this


to eliminate setWorkerEvent entirely, also load the secrets explicitly
just load their names, from process.env
this also works on worker and lambda same code
because both can dereference them
even if cloudflare can't list them


simplicity and reliabi

no, here's the plan
simple, isomorphic, reliable

instead of listing secrets from process or worker event
parse what you need to redact into words, look for words that end _SECRET, then dereference them from process

and here's how you make dog and all logs reliable but not await async
implement your own promise wait until
and then wait on it before returning the result from the handler

reliability should be unaffected
things are slightly faster because fire and forget events can proceed in parallel
although the response is still delayed by the longest one, it's not delayed by the sum duration of all of them

thre is that theoretical problem that this keeps lambdas, called quickly, from ever returning
so maybe build in a tick of first addition, and if that's too old, just return--like four seconds too old
and also put in the alert for detected overlapping lambda or worker, put that in, too

cloudPromise(p)

dog
awaitDog
logAlert
awaitLogAlert
















     _       _        _                    
  __| | __ _| |_ __ _| |__   __ _ ___  ___ 
 / _` |/ _` | __/ _` | '_ \ / _` / __|/ _ \
| (_| | (_| | || (_| | |_) | (_| \__ \  __/
 \__,_|\__,_|\__\__,_|_.__/ \__,_|___/\___|
                                           

bookmark january
here's where you talk about
-what you learned
-choices you've made
-concessions you've made
-conventions you've set
about
SQL, PostgreSQL, and Supabase
as well as manual steps to create a database
and backup and restore the database, cloud and local postgres

https://supabase.com/docs/guides/platform/backups
"All Pro, Team and Enterprise Plan Supabase projects are backed up automatically on a daily basis."
pro costs $25/month



2025jan19
went into supabase dashboard to make test database
and check settings

https://supabase.com/dashboard/projects
New project

Organization, only one there
test1, Project Name
use their generate link to create a strong password; don't seem to need it later
east us north virginia, region

security options
data api and connection string
public schema for data api
both of these are default, not sure what they mean, note that says that you can change these afterwards

advanced configuration
postgres, default, not the other one which is orioledb alpha, whatever that is

top bar, Connect
App Frameworks, Nuxt
copy out the url and key
check out the code for createClient in the next two tabs, also

now you can see
SUPABASE_URL
SUPABASE_KEY
and a code example
import { createClient } from "@supabase/supabase-js";
export const supabase = createClient(supabaseUrl, supabaseKey);

Settings, Database, SSL Configuration
Enforce SSL on incoming connections
Reject non-SSL connections to your database
turned on

Settings, configuration, API, Data API Settings
Max rows 1000
The maximum number of rows returned from a view, table, or stored procedure. Limits payload size for accidental or malicious requests.

Settings, configuration Authentication, User Signups
turned off
Allow new users to sign up
If this is disabled, new users will not be able to sign up to your application.
correct, because what does that even mean





 _        _ _          _           _ 
| |_ __ _(_) |_      _(_)_ __   __| |
| __/ _` | | \ \ /\ / / | '_ \ / _` |
| || (_| | | |\ V  V /| | | | | (_| |
 \__\__,_|_|_| \_/\_/ |_|_| |_|\__,_|
                                     

https://tailwindcss.nuxtjs.org/getting-started/installation
$ yarn add -D @nuxtjs/tailwindcss
$ yarn list --pattern tailwindcss
├─ @nuxtjs/tailwindcss@6.14.0
└─ tailwindcss@3.4.17
so as expected, we get tailwind 3, even though the current version of tailwind is 4

//and added to nuxt.config.js:
configuration.modules.push('@nuxtjs/tailwindcss')
configuration.tailwindcss = {cssPath: '~/assets/css/style.css'}



 _                         _   
| | __ _ _   _  ___  _   _| |_ 
| |/ _` | | | |/ _ \| | | | __|
| | (_| | |_| | (_) | |_| | |_ 
|_|\__,_|\__, |\___/ \__,_|\__|
         |___/                 

inner to outer, top to bottom, here's how page layout and the component tree works:

./components/MyComponent.vue
./components/MyPage.vue

limit component styles to setting interior structure, not page layout

./pages/page2.vue
./pages/index.vue

pages define routes
index isn't fundamental like app.vue is, it just defines the root route
every page should choose a layout with script starting definePageMeta({layout: 'some-layout'})

./layouts/default.vue
./layouts/blank-layout.vue
./layouts/column-layout.vue

<template>
	<div class="page-container">
		<NavigationBar :note="route.meta.note" />
		<main><slot /></main>
	</div>
</template>

layouts must all contain main/slot/main
a page that doesn't choose a layout gets default
in layouts, you can style outer page structure with div class page-container
you can also include persistant components, like NavigationBar

./app.vue - universal structural wrapper, tags NuxtLayout/NuxtPage, top and bottom bar components like notifications, turnstile

<template>
	<TopBar />
	<NuxtLayout><NuxtPage /></NuxtLayout>
	<BottomBar />
</template>

app.vue is the only place where NuxtLayout and NuxtPage appear
both layout files and app.vue can have the same stuff
the difference is a page chooses one layout or another, while app.vue is always at the bottom, everywhere

./assets/css/style.css

tailwind's global stylesheet with custom utilities, component styles, or base resets
don't put layout styles in here, you think
rather it's good for styles common to parts of many components

./tailwind.config.js
./nuxt.config.js

configuration files that also don't do anything with structural layout





  __             _       
 / _| ___  _ __ | |_ ___ 
| |_ / _ \| '_ \| __/ __|
|  _| (_) | | | | |_\__ \
|_|  \___/|_| |_|\__|___/
                         

in the example below,
- Roboto is linked from the web
- Lemon is packaged
and we want to use both kinds
- to set the face of fundamental html tags like <p> and <code>
- upon request with a tailwind class like "font-myname"

[./public/fonts/Lemon-Wide.woff2] put your packaged fonts here

[./nuxt.config.js] link web hosted fonts here

configuration.app.head.link.push({
	rel: 'stylesheet',
	href: (
		'https://fonts.googleapis.com/css2?'
		+ '&family=Roboto:ital,wght@0,400;1,400;0,500;1,500'
		+ '&display=swap'
		+ '&subset=latin,latin-ext'
	)
})

[./assets/css/style.css] define packaged fonts here

@font-face {
	font-family: "Lemon Wide";
	src: url('/fonts/Lemon-Wide.woff2') format('woff2');
	font-weight: 400;
	font-display: swap;
}

[./tailwind.config.js] style default tags and create tailwind classes here

theme: {
	extend: {
		fontFamily: {
			sans:   ['"Noto Sans"',      ...defaultTheme.fontFamily.sans],
			mono:   ['"Noto Sans Mono"', ...defaultTheme.fontFamily.mono],

			roboto: ['"Roboto"',         ...defaultTheme.fontFamily.sans],
		},
	},
},



notes on getting the .woff2 files

https://abcdinamo.com/typefaces/diatype-rounded
entered name and email to download the free trial zip

https://www.behance.net/gallery/182528277/Lemon-Typeface-18-Styles-Variable-Regular-Free
https://rajputrajesh-448.gumroad.com/l/Lemon9
entered email to download the free trial zip
inside the email, picked the woff2 in the web-tt folder, chat says this is the one optimized for web rendering

./assets/fonts/Lemon-Wide.woff2 is where you put the file
./assets/css/style.css is where you bring it in

ttd july2025, right now just trying to bring two in; more to do with type later:
- non english alphabet characters
- italic face, avoid the browser's default slant
- maybe variable width; you've gotten working, but the file is 300kb instead of 60kb
- variants, like lemon's superwides and diatype's alternate alternate y, a, q, 4, and others
- licensing, purchase, watermark of identity in the file, those issues






                                                  _       
  ___ ___  _ __ ___  _ __   ___  _ __   ___ _ __ | |_ ___ 
 / __/ _ \| '_ ` _ \| '_ \ / _ \| '_ \ / _ \ '_ \| __/ __|
| (_| (_) | | | | | | |_) | (_) | | | |  __/ | | | |_\__ \
 \___\___/|_| |_| |_| .__/ \___/|_| |_|\___|_| |_|\__|___/
                    |_|                                   

ttd july2025, when you choose a component library,
how does that interact with what you've already done with tailwind, layout classes, and style classes






																				_            
 ___  ___ _ __ __ _ _ __    _ __   ___ | |_ ___  ___ 
/ __|/ __| '__/ _` | '_ \  | '_ \ / _ \| __/ _ \/ __|
\__ \ (__| | | (_| | |_) | | | | | (_) | ||  __/\__ \
|___/\___|_|  \__,_| .__/  |_| |_|\___/ \__\___||___/
									 |_|                               


(This page intentionally left blank.)





2024oct8
after updating wrangler, you got supabase working again
in the cloudflare dashboard
cloudflare dashboard, workers and pages, cold3
there's an Integrations tab, on it are a bunch of third party services in boxes, including Supabase
you went through this flow
you let cloudflare oauth into your supabase account
at the end, it had made two new cloudflare secrets
and all of that was unnecessary--you can just copy and paste the endpoint url and api key from supabase yourself
you deleted the integration, and supabase still works fine
so this note is for, in the future, if you're fixing supabase again, the integrations tab probably won't help







beyond aws services, another nice thing about net23 is that
if you do run into some node module or third party service
that you cannot get working in a cloudflare worker
then you just call out to a lambda at net23 which does it there
and you've got that capability at easy reach
without making every page load slow and expensive




2024aug30 turn off default email tracking pixel in sendgrid
https://app.sendgrid.com/settings/tracking
dashboard, settings, tracking
-Enabled; Open Tracking; An invisible image is being appended to HTML emails to track if they have been opened.
-Enabled; Click Tracking; Every link is being overwritten to track every click in emails.
-Disabled; Subscription Tracking; Allows every link to be overwritten to track every Subscription in emails.
-Disabled; Google Analytics Tracking; Allows tracking of your conversion rates and ROI with Google Analytics.
turned them all off








your stupid goal of total isomorphism
like, every environment can include and call into every library file
you accomplished this by doing two things:

(1) lazy and on demand loading, following this pattern

let _twilio,
async function loadTwilio() {
	if (!_twilio) _twilio = await import('twilio')
	return _twilio
}

and then the idea is that only code that should be able to use twilio calls down in here
and if code erroneously does, then you just get a run time exception
this alone should be enough, but it's not
nitro sees the mention, and freaks out with a build time error

chat suggested nuxt.config.ts vite.build.rollupOptions.external
but this is for hiding nuxt server only modules from nuxt client
not hiding not for nuxt at all modules

you previously somehow accidentally had aws-sdk as a cloud3 development module
and this is a working solution:

(2) install as a development dependency
here are your root nuxt package.json, and serverless framework lambda net23/package.json

./package.json
	"dependencies": {
		"@supabase/supabase-js": "^2.39.8",
	},
	"devDependencies": {
		"@sendgrid/mail": "^8.1.4",
		"aws-sdk": "^2.1691.0",
		"twilio": "^5.3.4",
	}

./net23/package.json
	"dependencies": {
		"@supabase/supabase-js": "^2.39.8",
		"@sendgrid/mail": "^8.1.4",
		"aws-sdk": "^2.1691.0",
		"twilio": "^5.3.4"
	},

both actually use supabase
amazon, twilio, and sendgrid have to be listed in both
but you hide these three in devDependencies just to calm nuxt down

and your hope is that it won't get bundled, but you're not sure about this
chat suggets 
$ npm install -D rollup-plugin-visualizer
$ npm install -D source-map-explorer
as tools to do this

(3) if that hadn't worked
you considered, and probably still should, make a separate library file:
./library/lambda.js
not imported by grand; not imported by nuxt
and then the nuxt package.json doesn't install amazon, twilio, sendgrid modules at all
imported directly by the lambda endpoints, alongside their imports of library stuff through grand
it's possible this will make the nuxt build faster
if you run into problems in this area again, quickly switch to this more straightforward design

note also this doesn't mean you make a library file called worker.js alongside lambda.js
lambda.js is by itself because it's where you can install and use any node module

oh, the note entrypoint might mean that you separate the 
when looking at real monorepo options, you tried moving the nuxt project into ./site/
but cloudflare didn't know what to do with it
so instead you could keep nuxt as the root, and move the pure node tasks into ./library
so then there's a new ./library/package.json













2024oct24

you've given up trying to get node modules like twilio working in a worker
none of these node modules were written imagining a web worker future
and their service providers don't mention web workers at all
even if you got one to work, it might be less reliable in a worker

also, even though javascript is full stack, there are big differences between client and server modules, tools and patterns
client js is all about es6, include, tree shaking, and small modules with few dependencies
server js is all about commonjs, require, node, and big modules with hundreds of dependencies
building server modules for the client is a neat demo for a meetup, but not a fruitful practice for a workday

so, you'll just try the lambda first now, instead of last
thank the bezos lambda exists--otherwise you'd be securing a virtual instance with chatgpt right now!

and even with this hybrid model, cloudflare is still great at:
-loading really fast, including as a cold start to make a good first impression to a brand new prospective user
-running trusted code that can't be tampered with
-using supabase, with their module, and datadog, with fetch
don't try to get it to do more than that

this hybrid model still keeps all the application logic in the worker
the lambda code is just "send this email" or "upload this file"--individual atomic commands,
and notes in database tables as to what happened afterwards










2024nov4

you've switched to yarn, using yarn 1.22.22 on both mac and windows
deleting and then git ignoring yarn.lock, which isn't standard
but as you checkout and build on mac, windows, and linux (through docker)
you want yarn to get stuff specific to that new platform, not a yarn.lock generated on a different os













																								_                         _     
	___  _ __   ___ _ __     __ _ _ __ __ _ _ __ | |__     ___ __ _ _ __ __| |___ 
 / _ \| '_ \ / _ \ '_ \   / _` | '__/ _` | '_ \| '_ \   / __/ _` | '__/ _` / __|
| (_) | |_) |  __/ | | | | (_| | | | (_| | |_) | | | | | (_| (_| | | | (_| \__ \
 \___/| .__/ \___|_| |_|  \__, |_|  \__,_| .__/|_| |_|  \___\__,_|_|  \__,_|___/
			|_|                 |___/          |_|                                    

open graph cards appear easy, but are hard--the server, instead of the browser, must render
they're also important--they're a new user's first experience with the site, right before they first click in

[i] scaffolding

https://nuxt.com/modules/og-image
https://nuxtseo.com/docs/og-image/getting-started/introduction

https://github.com/nuxt-modules/og-image
https://www.npmjs.com/package/nuxt-og-image - 41k weekly downloads, low

$ yarn run nuxi module add og-image

+++ b/nuxt.config.ts
-  modules: ["nitro-cloudflare-dev"]
+  modules: ["nitro-cloudflare-dev", "nuxt-og-image"]

+++ b/package.json
	"dependencies": {
+		"nuxt-og-image": "5.1.6",  https://www.npmjs.com/package/nuxt-og-image  ⚠️ 42k weekly downloads 
+		"@unhead/vue": "^2.0.5",   https://www.npmjs.com/package/@unhead/vue    🆗 986k 
+		"unstorage": "^1.15.0",    https://www.npmjs.com/package/unstorage      🆗 2mm
	},

used the nuxi module install instead of installing manually
edits nuxt config to add the module
pins the module version, only place we've got no carrot!
and also brings in those two peer dependencies, unhead and unstorage

[ii] code configuration

./nuxt.config.ts
push the module
have to tell it the site name and url
name and connect the KV store

./wrangler.jsonc
paste in the KV store id from the dashboard

[iii] dashboard configuration

dashboard clicking steps were:
cloudflare dashboard, workers and pages, KV, create a namespace
OG_IMAGE_CACHE, name
then the dashboard generates an id, pasted that into wrangler.toml

to see current keys and values in the dashboard:
Account Home, Storage & Databases, KV
KV stores are listed top level in the cloudflare account,
rather than within a Domains or Workers & Pages listing

[iv] notes, observations, issues, and things to do later

## how this works

1 whatsapp GETs the link to our site that has appeared in the user's chat
2 nuxt renders the page on the server side, including the meta tags that have the image url
3 whatsapp requests the image url
4 nuxt renders the image on the server side, painting and compressing a png file

nuxt is the cloudflare worker running nuxt, nuxt og image, and our code written in that framework
whatsapp is either that app on the user's phone, or likely a backend server running on behalf of the whatsapp user

## example

example URLs, meta tags, KV key, and KV value:

http://localhost:3000/__og-image__/image/og.png
https://cold3.cc/__og-image__/image/og.png

<meta property="og:image" content="https://cold3.cc/__og-image__/image/card/name1174/og.png">
<meta property="og:image:type" content="image/png">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://cold3.cc/__og-image__/image/card/name1174/og.png">
<meta name="twitter:image:src" content="https://cold3.cc/__og-image__/image/card/name1174/og.png">
<meta property="og:image:width" content="1200">
<meta name="twitter:image:width" content="1200">
<meta property="og:image:height" content="600">
<meta name="twitter:image:height" content="600">

5.1.6:card:name4557:gfIYtMskgaEe-yswRu2I4daKEdykNZs3S37l1UwuwZg

{
	"value": "iVBORw0KGgoAAAANS ... 1TNcAAAAASUVORK5CYII=",//around 160 KB of base64 text
	"headers": {
		"Vary": "accept-encoding, host",
		"etag": "W/\"cfXhjA5NzE0681Cn-hbLoVBsvpGl933mVb8G4fWtFVI\"",
		"last-modified": "Sat, 19 Jul 2025 19:14:32 GMT",
		"cache-control": "public, s-maxage=1200, stale-while-revalidate"
	},
	"expiresAt": 1752953672593
}

## image generation on the edge

surprisingly, cloudflare workers gives nuxt og image all it needs to make a PNG
SVG is also an option, but client apps like whatsapp can't uniformly show that format
JPG is also an option for the module, but the worker can't make those

but how does the worker make the png? some early chatting identified maybe
https://github.com/vercel/satori renders the SVG, and then
https://github.com/thx/resvg-js converts SVG -> PNG, all on the worker

in the test cards, there's a huge gradient which looks hard for PNG to compress
but you haven't tried getting a source JPG to render into the card, which you'll need
it may be that net23 will have to save a PNG of a picture,
and then hopefully nuxt og image running still entirely in the worker can compose the PNG into the card

ttd july2025, test out getting a ug image like a photograph from net23 into a card

## observed speeds

you're seeing three speeds:
3244ms new image the worker had to generate
552ms that same route in a new browser; here you think it's coming from the cloudflare KV cache
2ms a browser refresh quickly gets the image from the browser cache

if a post is very popular, you want to deliver the card instantly to thousands of followers
but also, if a user edits their post, they want the card to immediately show the edited information

## observed generation behavior from initial server render only

the card is new and correct only for the first hit, not later if you navigate around the site
this is ok, you suppose, as these cards appear for non-browser clients, which only do a first hit
essentially, you're observing that it works like this:
when the tab loads, the meta tags that lead to the card are set
so if you start on the home page, you get the home card, even if you then navigate to the card page
if you do a browser reload, that's the tab loading, so you get a new card
you're seeing this behavior running both locally and deployed, also

## site routes versus user routes

for v1, you want three cards:
- home card, on the root route and other site-specific settings
- user card, about the user's profile as a whole
- post card, about a user's single post

to begin, you've just cards in these two code files:

./pages/index.vue
./pages/card/[more].vue

so you need to figure out how to get module use and nuxt routing together propertly to make the 3 cards
can you define a global card, which then gets overridden with a user page or post card?

ttd july2025 configure routes and cards to generate site, profile, and post cards

## light and dark color modes

defineOgImageComponent accepts the property:
	colorMode: 'light'
but pretty sure that there's no way to know the user's phone's light/dark mode setting
the request to get the card doesn't come from the phone's browser
nor even from the whatsapp running on the user's phone
but rather from a meta server running the whatsapp backend for that user!

## hashing and caching

how does the module decide when to generate a new image?
you can see the card hash in the KV store key, at the end
and chat says that's a hash of all the properties defineOgImageComponent() got to make the card
but the URL has just the route, and it works

also, early on, you tried the ?purge flag, like
https://cold3.cc/__og-image__/image/og.png?purge
but it doesn't always regenerate the image

## cache expiration woes

even though you've set 20 minute expiration
and see that in the KV store values
you can also see that the KV store is keeping everything forever

chat is saying the fix is to monkey patch the nitro driver:

export default defineNitroPlugin((nitroApp) => {
	const kvDriver = nitroApp.storage.getDriver('cloudflare-kv-binding')
	const originalPut = kvDriver.put.bind(kvDriver)
	kvDriver.put = (key, body) =>
		// now passes a 1200s TTL to KV
		originalPut(key, body, { expirationTtl: 60 * 20 })
})

but before you do that, perhaps
- an updated version of nuxt og image will do this
- cloudflare will add an expiration setting on the dashboard

or, to just kill the old ones more easily manually, perhaps
- wrangler will allow a delete * you can just periodically run
- or there will be a way to do that in the dashboard

right now the dashboard only has a button to delete the whole store
which is still useful to clear out a big old one,
but you'd have to do a new deployment right after with the replacement KV store's id in wrangler.jsonc

wrangler's kv api is really minimal, but you can do this to count them:
$ npx wrangler kv key list --binding=OG_IMAGE_CACHE --remote | jq 'length'
https://developers.cloudflare.com/kv/reference/kv-commands/

even with the cache being eternal, you can still edit a post and get a new card
because when you change any of the define og image component properties
you get a new hash, and a new png, and a new KV store item
which also of course sticks around forever

ttd july2025 get the kv store to throw away old cards instead of keeping them all forever

                     _
  ___ _ ____   _____| | ___  _ __   ___
 / _ \ '_ \ \ / / _ \ |/ _ \| '_ \ / _ \
|  __/ | | \ V /  __/ | (_) | |_) |  __/
 \___|_| |_|\_/ \___|_|\___/| .__/ \___|
                            |_|

the encrypted envelope, holding a pojo letter, is a useful pattern
Key('envelope, secret') produces the symmetric key, which is available on
all three servers (nuxt worker on cloudflare, sveltekit worker on cloudflare, serverless framework lambda on amazon)

servers send envelopes to each other to pass self-authenticated messages
additionally, the page holds an envelope to continue server context (totp and wallet flows)

the standard symmetric encryption means there aren't concerns  about authenticity, secrecy, or tampering
envelope replay is something each use of them needs to consider carefully, though!

encryption is faster than database calls
and envelopes are stateless and standalone
without them, we'd have to have sveltekit write to the database
and have database tables with provisional notes during totp and wallet enrollment (as we did have to do with the complex and derpy email and sms one time codes, regretably)

envelope data is encoded in base62, where the compact, only letters and numerals format really shines
short strings move through content bodies, the query string, and cookies
and we never have to worry about escaping and unescaping!

code is like this:

let symmetric = encryptSymmetric(Key('')
let letter = {
	action: 'EnvelopeTitle.',//unique titles prevent replaying a valid envelope about something else
	expires: Now() + Limit.handoff,//required, and always set and checked by a trusted server clock
}
let envelope = await symmetric.encryptObject(letter)
let letter = await openEnvelope(envelope)//which uses symmetric.decrytpObject(envelope)
if (isExpired(letter.expiration) toss('expired')

places to see (and check) envelope in action:

(1) lambda (envelope sealed by nuxt worker, opened by lambda)
in level2, doorWorkerOpen and doorLambdaOpen call openEnvelope if the body or query string has one
doorLambdaCheck requires a letter in the body with action "Network23."
calling net23, the worker sets an expiration Limit.handoffLambda 14 seconds later, allowing for the cold start

(2) wallet (envelope sealed by nuxt worker, held by page, opened by worker)
wallet.js seals an envelope with the nonce for the user's browser's ethereum wallet to sign
and in the second step, the same nuxt endpoint reopens the envelope to pick up where it left off
here, expiration is Limit.expirationUser 20 minutes, allowing the user to walk away and return between manual steps
front end Vue components get response.envelope, and return it in the next fetch request

(3) totp (envelope sealed by nuxt worker, held by page, kept in cookie, opened by worker)
similar flow in totp, with same 20 minute user task expiration
here, the front end Vue component also keeps the envelope in a temporary_envelope_totp same site cookie, as the user or device might refresh the page while dealing with their authenticator app

(4) oauth (first envelope nuxt worker to sveltekit worker; second envelope sveltekit back to nuxt)
the nuxt site must take the user to the sveltekit site, which does the oauth dance, and then must get results back to nuxt

1 (nuxt) ./components/OauthDemo.vue
2 (nuxt) ./server/api/oauth.js
3 (sveltekit) ./src/routes/continue/[provider]/+page.server.js
4 (sveltekit) ./src/auth.js
5 (nuxt) ./server/routes/oauth-done.get.js

- page component 1 posts to nuxt endpoint 2, requesting an envelope to begin the flow as browserHash
- endpoint 2 seals the envelope "OauthContinue." with a shorter 2 second expiration, as this is worker to worker (no user interaction or cold-starting lambdas involved)
- back in 1, the page redirects the browser to 3, including the envelope in the query string
3 calls openEnvelope directly (no door system for sveltekit), checking the action and expiration
...the oauth dance happens...
- in 5, signIn() seals the user's oauth {account, profile, user} in a second envelope "OauthDone." with the same 2 second Limit.handoffWorker, and with redirect() the browser, given that envelope, gives it to 6, navigating there
- back in nuxt, 6 gets passed the decrypted letter from the door system, and checks the action and expiration

demo code will get replaced with final code, and filenames will change
in summary, make sure you're always setting and checking letter.action and letter.expiration
and don't worry about authenticity or secrecy, but do think about implications of quick replay scenarios
like the same envelope, again, or a an unrelated envelope when you're expecting one with the appropriate action

                   _   _
  ___   __ _ _   _| |_| |__
 / _ \ / _` | | | | __| '_ \
| (_) | (_| | |_| | |_| | | |
 \___/ \__,_|\__,_|\__|_| |_|


2025jun12
steps to get oauth.cold3.cc
up on cloudflare
and as a new workspace in this monorepo that imports icarus

https://developers.cloudflare.com/pages/get-started/c3/
"We recommend using Cloudflare Workers for new projects. For existing Pages projects, see our migration guide and compatibility matrix."
https://developers.cloudflare.com/workers/static-assets/migration-guides/migrate-from-pages/
sure, starting this without the platform pages flag
https://developers.cloudflare.com/pages/framework-guides/deploy-a-svelte-kit-site/
https://developers.cloudflare.com/workers/framework-guides/web-apps/svelte/

in the project root folder
$ yarn create cloudflare
oauth, in which directory do you wish to create
framework starter
sveltekit
minimal, barebones scaffolding for your app
no typescript
all unchecked, add to project
yarn, package manager
no git, you're in an existing git repository
no deploy, we'll do that in a moment

$ yarn local
$ yarn build
$ yarn deploy

cloudflare dashboard
workers and pages, oauth, settings
domains and routes
here, by the way, is where you can see and disable the subdomains cloudflare gives your site that end workers.dev 
add, custom domain
oauth.cold3.cc
now the box has three rows, Type are:
workers.dev    oauth. ... .workers.dev    Disable
Preview URLs   *-oauth. ... .workers.dev  Disable
Custom domain  oauth.cold3.cc             Edit, Delete
and the domain works:
https://oauth.cold3.cc/
also confirmed that http -> https in chrome and firefox

from there it's all in the code files checked into github, you think




ttd november, going to move oauth notes here now that you've got the flow working with sveltekit and discord
bookmark


















      _                 _  __                 _      __                  _   _             
  ___| | ___  _   _  __| |/ _|_ __ ___  _ __ | |_   / _|_   _ _ __   ___| |_(_) ___  _ __  
 / __| |/ _ \| | | |/ _` | |_| '__/ _ \| '_ \| __| | |_| | | | '_ \ / __| __| |/ _ \| '_ \ 
| (__| | (_) | |_| | (_| |  _| | | (_) | | | | |_  |  _| |_| | | | | (__| |_| | (_) | | | |
 \___|_|\___/ \__,_|\__,_|_| |_|  \___/|_| |_|\__| |_|  \__,_|_| |_|\___|\__|_|\___/|_| |_|
                                                                                           

after not being able to get Lambda@Edge to work in serverless.yml,
you found amazon's weird CloudFront Functions:
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html
it's javascript, sorta; there are a lot of weird limitations:
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/functions-javascript-runtime-20.html
for instance, you can't comment at the end of a line; comments have to be on separate lines

$ node vhs.cjs

is the edit-test-debug loop you made
deployment is pasting code into a box in the aws dashboards:

aws dashboard, cloudfront
functions, third one down on the left
create function
vhs1, name
cloudfront-js-2.0, runtime

paste secrets into the top of the box
paste code into the box below the secrets, save changes

publish tab, publish function
then at the bottom, associated distributions, add association
vhs.net23.cc, distribution
viewer request, event type
cache behavior, clicked down and selected default, not sure what this is or if there's a use here for you
add association

in the code, console.log works (console.error isn't there, though!)
and logs started going into CloudWatch without you having to configure anything

if you change the code, a new deploy takes 5-10min, so use vhs.cjs









 _                        _   _ _      
| |_ _   _ _ __ _ __  ___| |_(_) | ___ 
| __| | | | '__| '_ \/ __| __| | |/ _ \
| |_| |_| | |  | | | \__ \ |_| | |  __/
 \__|\__,_|_|  |_| |_|___/\__|_|_|\___|
                                       

cloudflare turnstile is better than recaptcha because not google, and no clicking traffic lights
https://developers.cloudflare.com/turnstile/get-started/
we're also on cloudflare pages, a site hosted anywhere can use turnstile the same way we do

start in the dashboard
widget name: turnstile1
hostname management: cold3.cc
widget mode: invisible
pre-clearance: no

and then get the site key and secret key, which in our code will be:
"turnstile site key, public", available to untrusted front-end code, revealed to users
"turnstile, secret", securely and secretly stored in the cloudflare worker

app.vue loads Turnstile site-wide as soon as the user navigates to any route
if later the user navigates to a turnstile-protected form, turnstile already has signals that indicate human behavior

in the dashboard, Widget Mode choices are:
-Managed
-Non-interactive
-Invisible
and in code, window.turnstile.render size options are:
-invisible
-normal
-compact
-flexible

running configured invisible+invisible, widget doesn't show up, 2.3 second delay
running configured noninteractive+normal, widget shows up, then sometimes disappears, 2.3 second delay, and shows a spinner, or a green check in a circle
you haven't seen the checkbox the user must check, but officially, that's a possibility you can't rule out
coding turnstile you configured it noninteractive+normal
[]but to use it later you'll change back to invisible+invisible

so, because the time delay is noticeably long, and there's no user interaction, your user experience desigin plan is this:
when the user navigates to the form, turnstile executes to get a token invisibly
the user won't notice any delay unless they fill out and submit the form faster than 2.3 seconds, or longer than 4 minutes
a really fast user will get the remaining delay hidden into the submit button being orange
a really slow user will get the full delay baked
so even if our api endpoint gets back in milliseconds, it will look like the server took a few seconds
this is a normal and expected experience on the consumer web today

with this design, we're never showing turnstile feedback,
and hiding the time delay in the user's typing in the form and the submit button
the submit button already needs a good visual "working on it" appearance (like pink hearts bubbling up or something)
and that spinner will work for both delays
here, we've also grouped two progress bars into a single progress bar, à la Office Space

even configured invisible, turnstile documentation says that there's a possibility it will show up for the user to check a box
if it does this, the experience will be:
while the button is orange, beneath it, the widget appears
the user checks the box, and then the orange button finishes normally
this experience is below our standard for user experience on the site, but is totally normal for average websites
also, it may never happen if "non-interactive" and "invisible" really mean those things

additional features you're not using yet, but which may be good
https://developers.cloudflare.com/turnstile/concepts/pre-clearance-support/
designed for SPAs, which we are, so that you can include turnstile on every POST but not get turnstile bothering the user

also there's stats about turnstile in the dashboard
see if you can see how long the browser hashing delay is for real useres, ideally p50 and p95
and if the widget has ever shown anyone the checkbox









                  _                 _   _             
  __ _  ___  ___ | | ___   ___ __ _| |_(_) ___  _ __  
 / _` |/ _ \/ _ \| |/ _ \ / __/ _` | __| |/ _ \| '_ \ 
| (_| |  __/ (_) | | (_) | (_| (_| | |_| | (_) | | | |
 \__, |\___|\___/|_|\___/ \___\__,_|\__|_|\___/|_| |_|
 |___/                                                

cloudflare comes with really good geolocation
official docs:
https://developers.cloudflare.com/network/ip-geolocation/
https://developers.cloudflare.com/rules/transform/managed-transforms/reference/#add-visitor-location-headers
https://developers.cloudflare.com/rules/transform/managed-transforms/configure/
you have to turn it on
there are two checkboxes, and they're hard to find

first one, just the country
cloudflare dashboard, domain name
Network, on the left; options on the right like IPv6 Compatibility, gRPC, WebSockets
IP Geolocation, seems to be enabled by default; "Include the country code of the visitor location with all requests to your website. Note: You must retrieve the IP Geolocation information from the CF-IPCountry HTTP header."

second one, city and more
cloudflare dashboard, domain name
Rules, on the left
click to the second page of cards to find one that has the linked category Request Header Transform Rules
Managed Transforms tab
"Add visitor location headers. Adds HTTP request headers with location information for the visitor's IP address, including city, country, continent, longitude, and latitude."
turn on, not on by default

earlier, you setup http->https and www->apex, documented above
the cards in Rules, Overview may be new, and may be an easier way to configure this
cards include:
"Redirect from HTTP to HTTPS: Always redirect HTTP requests to HTTPS based on hostname.
Redirect from WWW to Root: Always redirect HTTP requests from the WWW subdomain to the root.
Redirect to a New URL: Redirect visitors requesting one page to another page URL."






              _    _____
__      _____| |__|___ /
\ \ /\ / / _ \ '_ \ |_ \
 \ V  V /  __/ |_) |__) |
  \_/\_/ \___|_.__/____/

Party like it's 2021



[i] module roster

viem
https://www.npmjs.com/package/viem
1.9mm weekly downloads
** Low-level Ethereum interface providing pure JS functions for blockchain operations - the foundation everything else builds on

@wagmi/core
https://www.npmjs.com/package/@wagmi/core
366k weekly downloads
** Adds wallet connection abstractions and multi-chain support on top of viem - handles account management without UI framework dependencies

@wagmi/connectors
https://www.npmjs.com/package/@wagmi/connectors
364k weekly downloads
** Pre-built wallet connectors for MetaMask, WalletConnect, Coinbase Wallet, etc - without this you'd have to implement each wallet's connection protocol yourself

(this is probably all you'll end up using, but also trying out)

@wagmi/vue
https://www.npmjs.com/package/@wagmi/vue
5k ⚠️ warning, low download count!
** Vue composables that make wagmi/core reactive - provides useAccount(), useBalance(), etc. Warning: low adoption suggests most Vue devs use wagmi/core directly

@tanstack/vue-query
https://www.npmjs.com/package/@tanstack/vue-query
272k weekly downloads
** Required dependency for @wagmi/vue - handles async state management, caching, and request deduplication. You're adding this solely because @wagmi/vue depends on it

other early decisions:
- coding in component but will move to composable or store
- importing modules on margin (for tree shaking) but only using on mounted (alchemy key won't work ssr)
- single alchemy key revealed but protected by domain name (won't work ssr)




https://dashboard.alchemy.com/
create new app
cold3page, Name
Identity, use case
Ethereum, picked only one chain in the choose chains step
Node API, picked only one servive in the next step, claude says this is all that's necessary for erc20 and nfts
now you get the dashboard, fourth tab Settings
Allowlist, Domain, Restrict to specific domains
add two, note there's no https or port, Add, Save
cold3.cc
localhost














										 _                       
 _ __ ___   __ _  __| |_ __ ___   __ _ _ __  
| '__/ _ \ / _` |/ _` | '_ ` _ \ / _` | '_ \ 
| | | (_) | (_| | (_| | | | | | | (_| | |_) |
|_|  \___/ \__,_|\__,_|_| |_| |_|\__,_| .__/ 
																			|_|    

== high level

>milestone 1, done

domain name
ssl certificate

www bucket
www distribution
www upload

>milestone 2

api functions
only callable from https://cold3.cc
that use imports
and library code
and survived webpack tree shaking

>milestone 3

vhs bucket
vhs distribution
vhs lambda@edge un-get-round-able gatekeeper
so also only callable from cold3

>milestone 4

send an email
send a sms
