
 _   _      _                      _      ____  _____ 
| \ | | ___| |___      _____  _ __| | __ |___ \|___ / 
|  \| |/ _ \ __\ \ /\ / / _ \| '__| |/ /   __) | |_ \ 
| |\  |  __/ |_ \ V  V / (_) | |  |   <   / __/ ___) |
|_| \_|\___|\__| \_/\_/ \___/|_|  |_|\_\ |_____|____/ 
																											

Network 23 ~ "Beta to the Max!" -ChatGPT

 _________________________________
/\                                \
\_| --y'know what? There's only   |
	| four things we do better than |
	| anyone else:                  |
	|                               |
	|   music                       |
	|   movies                      |
	|   microcode (software)        |
	|   high-speed pizza delivery   |
	|   ____________________________|_
	 \_/______________________________/

some notes building the megamultimedia empire:
       _      _                   
__   _(_)_ __| |_ _   _  ___  ___ 
\ \ / / | '__| __| | | |/ _ \/ __|
 \ V /| | |  | |_| |_| |  __/\__ \
  \_/ |_|_|   \__|\__,_|\___||___/
                                  
start out up here with design document
the requirements i set, the choices i made, and the reasoning behind them
TODO, but notes follow

values, requirements

in order, they are
1 reliable: never down, not for five minutes or five users
2 simple: written at boot camp level, all the code fits on a floppy disk
3 fast: most user tasks complete in the blink of an eye

serverless, because a server would mean you have to
restart it when it goes down
update it
secure it, know when it's been compromised, and recover from such an attack
pay too much when it's too big (cinderella's problem)
change configuration and software when it's too small (evil stepsisters' problem)

code, best to worst
pure javascript
npm modules with millions of weekly downloads
npm modules with hundreds of thousands of weekly downloads


factored downwards
biggest: .js files in the library
smallest: .vue files

100% isomorphic
the exact same function checkEmail() corrects user input on the page,
and keeps malformed data from being written to the database
execution from ten different environments could call into any file
local/deployed x
node/nuxt/serverless framework/vite tdd runner icarus
cloudflare workers/amazon lambda
page/server api
page code pre-rendering on server/page code rendering on client




we love vue. we hate react
jsx lets you mix everything, invalidating the wonderous code separation of the web stack
react wins when it's complex enough professionals base their careers around it
we win when it's simple enough we're focused on features and users, not platform patterns and what's new this year


configuration as code
do as much configuration in computer-readable as possible
where that's not possible, configuration is in english, step by step, in configuration.txt
and be wary of very difficult configuration code being replacable by a pretty simple and one time step by step



and the requirements list you presented to rf goes here, too
and before that, the slides for sp



Requirements
1. Simple, full-stack JavaScript
2. Really fast: <200ms
3. Really cheap: <$5/month
4. Minimal vendor-specific code
5. Ready for millions of users overnight
6. Secure even from sophisticated attacks





computer and software are made of instructions
since the dawn of software, there have been three kinds of instructions:

I. configuration (mental, or readable)
II. configuration as code (machine executable)
III. code (machine executable)

traditionally, instrutions in category I are kept and versioned in the human brains of the development team
the rule here is that can't be the only place: all instructions in category I must also be codified here, in human readable if not machine executable form
the test is: could you hand this file to a brand new team member, and could they use it to set everything up, without asking your human brain any questions

the second value related to this is to keep instructions as far down that list as possible
code is better than configuration as code
configuration as code is better than configuration (this idea has gained recent popularity in software engineering)

but exceptions to this guideline exist!: like maybe all the domain name stuff in serverless.yml
it took a week to craft screenfuls of potentially brittle configuration
and maybe all that could be done just once at the start of the project, and documented here, as a few human-readable instructions on what to click on amazon's website dashboard




When selecting an open-source component from a leaderboard of popular options, a compelling pattern emerges: the second-most popular option often strikes the best balance. These components are widely adopted enough that AI tools and community resources can reliably address questions, yet they frequently benefit from more modern construction than their top-ranked counterparts. Their designs tend to emphasize simplicity and efficiency, often making them smaller and easier to work with.

The contrast in positioning is also notable. The #1-ranked component often commands its spot by framing itself as the "industry standard" or "default choice," appealing to developers with its dominance and perceived authority. In contrast, #2 typically targets users who are dissatisfied with complexity, positioning itself as a simpler, more approachable alternative. This focus on ease of use can lead to faster adoption and smoother integration, especially for teams valuing efficiency over exhaustive feature sets.

there's even a cheap cosmetic beneift: unstyled or default-styled #1 looks looks generic because it is widely recognized as an unmodified default. unstyled or default styled #2 can be misinterpreted as a custom look for the app!

example:
instead of react, vue
instead of videojs, vidstack formerly plyr



that's one of three module aphorisms:
- pick the second most popular module for the task
- find a module, unless you could code it yourself in just a screenful
- don't try to get node modules to work in a worker

on the third one:
workers are great at running client-side modules, but trusted!
javascript touts isomorphism, but the dream is not quite real yet
so if a module was written imagining it would be run on a server in node
don't spend time trying to fit it into a worker
isntead, just get it in persephone in the lambda, which will work, first try








the floppy disk fill percent, as a metric, is designed to measure developer attention-load
any files that a human could need to read or write are included: for instance, package.json is included, while yarn.lock is not
it's not a measure of how little code must be delivred to the user: for that, radio brotli in stats.html
"you say government should be small," asked a voter to a libertarian. "well then, *how* small should government be?"
"small enough to drown in a bathtub," answered the candidate
like a calf on a farm, first, you can kill it, but just weeks later, it can kill you
software projects brag about a million lines of code, an expert team of a dozen crafting for three years--these should be criticisms, not compliments
"small enough to fit on a floppy disk" means that
if everything needed to be reviewed (such as, for security), it wouldn't be that difficult
if everything needed to be rewritten (such as, for porting to a new platform), it wouldn't take that long
and you could do these things quickly, and with a small team

the goal here is not to write a lot of code, or to write complex or inventive code
the goal is to complete the user stories and epics
writing a small amount of simple code
to keep that code standard and common, avoiding, except where necessary, extra modules
avoiding, except where absolutely necessary, platform or provider specific details, and in those cases, factoring them away

"a short script that fits on a floppy disk"









     _         _                    _     _      
 ___| |_ _   _| | ___    __ _ _   _(_) __| | ___ 
/ __| __| | | | |/ _ \  / _` | | | | |/ _` |/ _ \
\__ \ |_| |_| | |  __/ | (_| | |_| | | (_| |  __/
|___/\__|\__, |_|\___|  \__, |\__,_|_|\__,_|\___|
         |___/          |___/                    

code at an 8th grade level
The rule "Simple is better than complex" comes from "The Zen of Python," a set of guiding principles for the language. Emphasized by Python's creator, Guido van Rossum, it promotes writing straightforward, readable code, making Python especially beginner-friendly and easy to maintain.

ideas to write the highly opinionated style guide

use complete words as names, avoid abbreviations and acronyms
for example, avoid "dev", as this is an abbreviation for developer or development
$ yarn local
where an acronym is not avoidable, treat it as a word
Html - good
HTML - bad
avoid all caps

tag names are title case, and end with a period
SomeTag.

comment nearly every line of code
write comments at the end of lines so comments do not lengthen the number of lines
comment not what the line of code does, but rather explain why it is doing that as part of what the surrounding function or larger scope accomplishes
comments should be nearly so complete and robust that you could give the function name and parameters, with the body stripped to be only comments, only, to a 2024-25 LLM and it could write a functional equivalent

do not format code to wrap onto multiple lines
your editor should be configured to wrap long lines
where lines of code are similar, use spaces to align similiarities vertically

use const to be explicit that a reference cannot or must not change
do not use it everywhere a reference happens to not change or not need to change
let is fine most of the time
use const for factory presets, imported module and platform APIs, Vue refs, and similar special cases

blocks of code have { spaces } while {objectProperty: 'literals'} do not

omit semicolons that are not necessary to enforce proper code flow
so if you have two statements on the same line, then you have to have a semicolon between them

string literals in single quotes, unless double or backtick necessary
in comments, double quotes

comments in conversational lowercase





 _   _                     _                           
| |_| |__  _ __ ___  ___  | | __ _ _   _  ___ _ __ ___ 
| __| '_ \| '__/ _ \/ _ \ | |/ _` | | | |/ _ \ '__/ __|
| |_| | | | | |  __/  __/ | | (_| | |_| |  __/ |  \__ \
 \__|_| |_|_|  \___|\___| |_|\__,_|\__, |\___|_|  |___/
                                   |___/               

the club sandwich
cloudflare workers are great at
1 really fast with no cold start time
2 running trusted code that can't be tampered with and has access to secrets
3 npm imports designed for client side web
workers really cannot
4 run a node module that imagines a node server (require, commonjs, long-lived events, deep tree of other node-like
lambda can do those
but lambda is slow:
avg    p95   results from checkly, no-cookie GET and read of static returned page, no client side code runs in these
1.55s  2.57  ssr page from worker including one supabase query
4.13s  5.91  ssr page from worker including one supabase query, and two warm requests to lambda
why not move the whole thing to cloudflare? workers can't run all the modules we need
why not move the whole thing to amazon? making one out of every 20 first time visitors wait 5.91 seconds to get a brochure page is not acceptable
hence, the three layer stack, the club sandwich






 _                  _                     _                            
| |_ _ __ _   _ ___| |_    __ _ _ __   __| |   ___ _ __ _ __ ___  _ __ 
| __| '__| | | / __| __|  / _` | '_ \ / _` |  / _ \ '__| '__/ _ \| '__|
| |_| |  | |_| \__ \ |_  | (_| | | | | (_| | |  __/ |  | | | (_) | |   
 \__|_|   \__,_|___/\__|  \__,_|_| |_|\__,_|  \___|_|  |_|  \___/|_|   
                                                                       

essay about security, trust, errors, and exceptions

talk about
- Task as a wrapper
- unwrapping before passing up to not let things go deeper and deeper
- exceptions as truly exceptional (summarized below)

where should things be brittle? where should they be resiliant?
how are errors caught and dealt with, up and down the stack, within and without our code, within and without our control?

treat unexpected exceptions as fatal issues that indicate a bug in the code rather than as a part of normal control flow—the recommended Nuxt 3 configuration and practices shift toward a “fail fast” approach. This means that rather than attempting to recover from errors or provide graceful degradation, you let exceptions bubble up to a global error handler, log them appropriately, and present a clear, interruptive error page to users.

we don't follow nuxt patterns precisely, because we're vendor isomorphic between workers and lambdas

but reading
https://nuxt.com/docs/getting-started/error-handling
https://nuxt.com/docs/getting-started/error-handling#error-page
https://nuxt.com/docs/api/utils/create-error


Nuxt 3—with its underlying Nitro engine—mainly expects you to handle server-side errors by letting them bubble up and then catching them using Nitro’s built‐in hooks (like the render:errorMiddleware hook). This is the recommended and most common approach because it captures a wide range of errors that occur during request processing, including those that arise from internal configuration or runtime code that you might not directly control.

design ideas for the errors and exceptions essay

for exceptions specifically, what if you did this
put try {} catch (e) {} around
small: fetch to, and use of, unreliable can be made redundant third party APIs, like twilio
big: the entry point to request handlers for worker and lambda, global error handlers in nuxt, framework-level
and then, when you catch a small exception, you keep running, and work around it
and when you catch a big exception, you log it to datadog, and tear down the whole page, error.vue, interrupting the user
done this way, if you find yourself coding more try blocks, or rethrowing an error, if you find yourself doing any of that as part of expected logic flow, you're doing it wrong. rather, toss exceptions as soon as you encounter an exceptional circumstance, and let them fly all the way up the stack, getting logged to datadog, and closing out the page with error.vue

groups of causes and remedies for errors
1 (a) it's staff's fault, and the fix will be correcting code and deploying a new version
2 (a) it's a third party provider's fault, and that provider is critical, and cannot be made redundant, like cloudflare pages, turnstile, amazon lambda, supabase, they should be waking up people on pager duty, and should fix it in a few hours, and send an apology email to us
3 (b) it's a third party provider's fault, and that provider is non-critical, and can be made redundant, like twilio, amazon SES and SNS, the fix is the round robin code seeing the interruption and sending use to redundant providers, automatically and immediately, and manually and hours later, staff getting their support on the line, or finding out they have deplatformed us
3 (c) it's the user's fault, and the fix is them not clicking or typing the wrong thing
4 (a) it's the user's fault, adn the fix is they need to stop trying to hack us, or remove that malicious chrome extension
ok, so on these, when do you:
(a) tear down the whole page to an error message,
(b) catch an exception and code decides what to do next, routing around it, or
(c) this shouldn't raise an exception anywhere to begin with at all!


trust is in these three zones
1 page: information script on the page is telling us; least trustworthy
2 browser: information the browser is telling us; more trustworthy
3 worker: information cloudflare is telling us; trustworthy
what does it mean for each to be compromised?
1 page compromised: the user has a malicious desktop browser extension, or has been coached on reddit to dig into the developer tools, or is a script kiddie themselves, this is likely but should not be widespread
2 browser compromised: the attacker is using curl or a botnet, this is possible
3 amazon, cloudflare, or supabase compromised: this should not happen, as these public companies have their share price son the line. also, if it does happen, it is outside of our control


what if you do try catch like this
tight, around every fetch or use of a third party api, like AWS, twilio
broad, around the whole 


pay attention to the cause and the remedy of the error, here are three
1 a user, 





       _ _          _                _         _               _   
  __ _(_) |_    ___| |__   ___  __ _| |_   ___| |__   ___  ___| |_ 
 / _` | | __|  / __| '_ \ / _ \/ _` | __| / __| '_ \ / _ \/ _ \ __|
| (_| | | |_  | (__| | | |  __/ (_| | |_  \__ \ | | |  __/  __/ |_ 
 \__, |_|\__|  \___|_| |_|\___|\__,_|\__| |___/_| |_|\___|\___|\__|
 |___/                                                             

$ git clone git@github.com:username/repository.git

$ git checkout -b name1             # Create a new branch and switch to it
$ git push -u origin name1          # Push the new branch to github and set the upstream

$ git fetch origin                  # Later, on another computer, find out about new branches up at origin
$ git branch -vv                    # List branches and their origins
$ git checkout name1                # And switch to one of the new ones there
$ git checkout main                 # Switch back to the main branch

$ git log --graph --oneline --decorate --all
$ git log --graph --oneline --decorate --all --pretty=format:"%h %ad %d %s" --date=format:"%Y%b%d"
$ git diff ec2023c..9edded6
$ git diff name1 name2 > diff.diff  #see the subway graph, and do a diff between two commits or branches; where you are doesn't matter

$ git checkout main                 #make sure main is a direct ancestor of name1 for an easy fast-forward merge
$ git merge name1
$ git push origin main

$ git branch -d name1               # Delete 'name1' after merging it (safe deletion)
$ git push origin --delete name1    # And, you should delete it from GitHub, the origin



$ git diff main feature1 > feature1.diff
$ git checkout main
$ git merge feature1
$ git push origin main









below are first and once for the project steps
group them into these categories
- setup a new computer, like get windows or mac ready to check out and build the site and net23
- configure and authenticate services, like install the aws cli and sign it in, cloudflare, serverless framework
- make a new similar shell project from scratch, useful to try out a new module or test something on the side, the clean room, this is like nuxt's setup, cloudflare create cli, that stuff
- not sure whre in there is make a github repo


          _                                                 _            
 ___  ___| |_ _   _ _ __     ___ ___  _ __ ___  _ __  _   _| |_ ___ _ __ 
/ __|/ _ \ __| | | | '_ \   / __/ _ \| '_ ` _ \| '_ \| | | | __/ _ \ '__|
\__ \  __/ |_| |_| | |_) | | (_| (_) | | | | | | |_) | |_| | ||  __/ |   
|___/\___|\__|\__,_| .__/   \___\___/|_| |_| |_| .__/ \__,_|\__\___|_|   
                   |_|                         |_|                       

new box, who dis?
steps to setup a new windows or mac workstation for development with the network 23 stack
mac you can really only experience these steps upon unboxing
windows you can always repeat to check with vmware

minimal steps to get to yarn local, build, deploy for worker and lambda
includes stuff like get git, node, node version manager, place secrets

== mac, 2025feb, unboxing, apple silicon, macOS 15 Sequoia

install iterm2
https://iterm2.com/
Download

install git
$ git --version
install box for command line developer tools appears, Install
used storage goes from 57->62gb
$ git --version, 2.39.5

install homebrew
https://brew.sh/
$ /bin/bash -c "$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)"
and then follow steps to add to path by editing ~/.zprofile
$ brew --version, 4.4.21
$ brew update, do before installing new packages

install nvm, node, and npm
used homebrew to install nvm, and it worked fine,
but homebrew shows output that says the nvm people want us to say they don't like it
$ brew install nvm    $ nvm --version, 0.40.1
$ nvm install 20      $ node --version, 20.18.3
$ npm install -g npm  $ npm --version, 11.1.0, the update changed the version from 10->11

install yarn version 1 with npm (not brew)
$ npm install -g yarn@1
$ yarn --version, 1.22.22

install the aws cli with brew (not npm, also notice we're not using yarn to install anything globally)
$ brew install awscli
$ aws --version, 2.24.10

install cloudflare wrangler globally, which is also referenced in individual projects' package.json files
$ npm install -g wrangler
$ wrangler --version, 3.109.2

install serverless framework globally with npm
$ npm install -g serverless
$ serverless --version, 4.6.4, and running that also causes it to update

authenticate git with github
instructions:
https://docs.github.com/en/authentication/connecting-to-github-with-ssh
control panel:
https://github.com/settings/keys
choosing ed25519 instead of rsa
choosing no passphrase, which means you also don't have to deal with the ssh agent
on the new mac, there is no .ssh folder at all yet
$ ssh-keygen -t ed25519 -C "name@example.com"
using the email address github knows about, which will also match the git global config step in a moment
now you have these two files:
~/.ssh/id_ed_25519
~/.ssh/id_ed_25519.pub
the public one goes into the github control panel; the private one stays private on your new box

set name and email in git
$ git config --global --list
user.name=First Last
user.email=name@example.com
git config --global user.name "First Last"
git config --global user.email "name@example.com"

now you can check out private repositories, notice this new form that doesn't start https and ends .git
$ git clone git@github.com:username/repository.git

chrome, Add New Profile, sign in with google credentials for project profile, yes sync
then sign into web dashboards:
https://cloudflare.com/
http://aws.amazon.com/, Sign in using root user email
https://app.serverless.com/
https://supabase.com/
https://app.datadoghq.com/logs
keep that chrome profile forward so when the command line pops to web to authenticate, it finds it

authenticate wrangler to be able to deploy to cloudflare pages
$ npm install -g wrangler, you already did this
$ wrangler --version, 3.109.2
$ wrangler whoami
$ wrangler login, pops to browser so make sure you've got the right tab signed in and forward!
$ wrangler logout, if you need to switch accounts

authenticate the aws cli
dashboards at aws.amazon.com
iam dashboard
click the number of users
create user
net23dev2, example name
attach policies directly
check AdministratorAccess, second one
create user
click new user
access key 1, create access key
use case, command line interface, check understand reccomendation, next
no description, create access key
secret access key only shows once, paste it right into bash

AWS Access Key ID [None]: YOUR_ACCESS_KEY_ID
AWS Secret Access Key [None]: YOUR_SECRET_ACCESS_KEY (paste in these two)
Default region name [None]: us-east-1
Default output format [None]: json (type these two)

commands to sign in, check sign in, and test sign in by listing your buckets
$ aws configure
$ aws sts get-caller-identity
$ aws s3 ls

now you've got this box set up enough you can checkout and build
$ git clone git@github.com:zootella/cold3.git
$ cd cold3
$ yarn install
$ yarn test

copy in secrets and stowaway
cold3/.env
cold3/.env.local, place the two secrets files in the project root
stowaway, and the stowaway folder beside the project root

$ yarn seal, which copies the secrets files you just placed
$ cd site
$ yarn build, works before you've authenticated wrangler
$ cd ../net23
$ yarn stowaway, which copies the linux binaries you placed
$ yarn build
the net23 build enters flow to authenticate serverless framework cli

and those are the steps to get a brand new mac ready to develop, build and deploy

== windows

(next, do all the same steps in vmware)






                      _     _   _                  _             _   
 _ __ ___ _ __   ___ | |_  | |_| |__   ___   _ __ | | __ _ _ __ | |_ 
| '__/ _ \ '_ \ / _ \| __| | __| '_ \ / _ \ | '_ \| |/ _` | '_ \| __|
| | |  __/ |_) | (_) | |_  | |_| | | |  __/ | |_) | | (_| | | | | |_ 
|_|  \___| .__/ \___/ \__|  \__|_| |_|\___| | .__/|_|\__,_|_| |_|\__|
				 |_|                                |_|                      

(these steps are about recreating the project, not the computer you're building the project on)

code rot happens faster than ever
six months later, several modules will be on new full number versions
and the task of updating them and confirming that the stack fits together and works together as a whole is significant

to keep the project current in today's reality of rapid code rot,
without spending the time and effort of updating modules individually, testing everything, and then moving on to the next one
the Network 23 Software Engineering Division instead employs a manœuvre we call "repot the plan"

here's how you do that
(step 1)
after updating platform things like node, yarn, git, wrangler, modules outside any project
(step 2)
in a completely separate folder,
following current getting started docs like
https://developers.cloudflare.com/pages/get-started/c3/
and
https://nuxt.com/docs/getting-started/installation
make a new 'Hello, World!' project from scratch, bringing in the same modules (but new versions)
essentially, follow the path from the very start that you used the last time to create the project
(step 3)
confirm hello world in the new pot is working, local and deployed
look at the configuration files you get, like wrangler.toml and nuxt.config.ts--the starting default contents will be different
merge previous project configurations into the new configuration files, make sure this does not break hello world
(step 4)
move code files from the old project into the new one




quick repot1

$ npm create cloudflare@latest
repot1
website or web app
nuxt
yes git for version control
yes deploy to cloudflare
(quickly get the right chrome profile forward, and already signed in to cloudflare there!)
yeah it's up at
https://repot1.pages.dev/

$ npx nuxi add mypage
$ npx nuxi add mycomponent
$ npx nuxi add myapi


2025feb trying to find or make new repot steps
https://developers.cloudflare.com/pages/get-started/c3/
$ yarn create cloudflare

ttd february

get just enough to generate stats.html
and to deploy to a pages.dev domain
use this to find small modules for qr codes and totp codes that works in cloudflare
and have good notes so you can return to it later, also



notes repotting for nuxt based static site start, 2025mar
https://developers.cloudflare.com/pages/get-started/c3/

$ wrangler whoami (make sure you're signed into the cloudflare account you want to use)
$ yarn create cloudflare (with yarn 1 installed globally)
example-com, sets the local directory and pages project name in the dashboard; use hyphen, not space
Framework Starter, category
Nuxt, framework
y, yes to install nuxi@3; didn't get asked the second time through these steps
no, don't use git for version control; going to drag created files into this existing repository
yes, deploy to cloudflare; add domain after that in the dashboard

github website
navigate to organization or user, repository list, New green button
example.com, repository name, ok to use a period here
wrote description
Public
no readme, gitignore, license, to start out with a completely empty repository
Create repository

$ git clone git@github.com:username/example-com.git
warning: You appear to have cloned an empty repository.

then, drag contents from example-com folder to example.com folder
set README.md, LICENSE, and .gitignore
in package.json, rename dev to local
git add, commit, push
yarn build, local, and deploy







										_            _       
 _ __ _   _ _ __   | |_ ___  ___| |_ ___ 
| '__| | | | '_ \  | __/ _ \/ __| __/ __|
| |  | |_| | | | | | ||  __/\__ \ |_\__ \
|_|   \__,_|_| |_|  \__\___||___/\__|___/
																				 

tiny tests, in level0.js, exports test(), ok(), and runTests()
all calls to test() are in the library
and, as much as possible, factor all *code* into the library as well!
pages, components, and api handlers should be really short

run tests in...

(1) Node 20
$ cd ./library
$ node test.js
test results are logged out

(2) Vite
$ cd ./icarus
$ npm run icarus
and then see test results on the home route

(3) Nuxt
$ cd ./
$ npm run local
and then see test results on the route /tests, TODO!

(and add more)
nuxt really is local/deployed x client/server--can you run tests in all four environments?
lambda serverless framework is local/deployed--run in those two environments







 _                          
(_) ___ __ _ _ __ _   _ ___ 
| |/ __/ _` | '__| | | / __|
| | (_| (_| | |  | |_| \__ \
|_|\___\__,_|_|   \__,_|___/
                            
(moved here from ./icarus/icarus.txt - but maybe it should be there, or at least a note about it in the readme)



Icarus is a vite little tdd system and test runner for the library functions

commands you used to make this:

https://vitejs.dev/guide/
$ npm create vite@latest
chose vue, rather than vanilla or react
chose javascript, rather than customize with create-vue or nuxt
$ cd hivite
$ npm install
$ npm run dev
http://localhost:5173

commands to use icarus:

$ cd icarus
$ npm run icarus
http://localhost:5173

it works! superfast refresh on:
-not just this folder, but the library folder alongside
-not just code changes, but comment and whitespace changes

next steps for this bike shed:
[]get the test results on the page
[]get the log output on the page
refresh replaces those there, instead of scrolling forever downwards
and if that works, can you not open the dev tools until you want to walk an object with >

[]move these notes up into cold3 readme










										 _                         _           _   
	___ _ __ ___  __ _| |_ ___   _ __  _ __ ___ (_) ___  ___| |_ 
 / __| '__/ _ \/ _` | __/ _ \ | '_ \| '__/ _ \| |/ _ \/ __| __|
| (__| | |  __/ (_| | ||  __/ | |_) | | | (_) | |  __/ (__| |_ 
 \___|_|  \___|\__,_|\__\___| | .__/|_|  \___// |\___|\___|\__|
															|_|           |__/               

[]todo january, in vmware windows and a new mac silicon user, set up project from scratch to check and minimize these notes. divide "create project" into two sections: "prepare workstation" which has stuff like brew and yarn and wrangler and serverless, all the global stuff, and "repot the plant" with stuff like cloudflare create and serverless' create
sorta also in this section is how to set it up in docker; that's a new workstation
also on this list is stuff where you authenticate, like github, aws cli, cloudflare

== install aws cli

npm's aws-cli is deprecated as of 6 years ago
it seems most people install aws-cli with python
https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html
amazon docs say for windows install that msi

$ aws --version
aws-cli/2.16.4 Python/3.11.8 Windows/10 exe/AMD64

(note on mac, you're trying $ brew install awscli)
$ brew --version
$ brew update
$ brew upgrade
$ xcode-select install
$ xcode-select -p

== give aws cli access to your aws account

$ aws configure

AWS Access Key ID [None]: YOUR_ACCESS_KEY_ID
AWS Secret Access Key [None]: YOUR_SECRET_ACCESS_KEY
Default region name [None]: us-east-1
Default output format [None]: json

dashboards at aws.amazon.com
iam dashboard
click the number of users
create user
net23dev2, example name
attach policies directly
check AdministratorAccess, second one
create user

click new user
access key 1, create access key
use case, command line interface, check understand reccomendation, next
no description, create access key

secret access key only shows once, paste it right into bash

$ aws sts get-caller-identity
$ aws s3 ls

confirm you're in, and list your buckets

== install serverless framework

https://www.serverless.com/

$ npm install -g serverless
$ serverless --version

== setup repository on github

made repo on github with node defaults
$ git clone https://github.com/zootella/net23

$ cd net23
move aside .gitignore as serverless create will also make one
$ serverless create --template aws-nodejs --path .

$ npm init
$ npm install -S aws-sdk

amazon access is configured, but make .env to mirror non-amazon secrets the lambdas will use
$ touch .env
and make sure .gitignore lists it
		 _                       _                                    
	__| | ___  _ __ ___   __ _(_)_ __    _ __   __ _ _ __ ___   ___ 
 / _` |/ _ \| '_ ` _ \ / _` | | '_ \  | '_ \ / _` | '_ ` _ \ / _ \
| (_| | (_) | | | | | | (_| | | | | | | | | | (_| | | | | | |  __/
 \__,_|\___/|_| |_| |_|\__,_|_|_| |_| |_| |_|\__,_|_| |_| |_|\___|
																																	

== domain name

registered net23.cc at a third party registrar

aws web console, route53 dashboard, hosted zones, create hosted zone
net23.cc, domain name
public already selected, create hosted zone
leads to page for new hosted zone, titled net23.cc, with two records
the NS record shows the name servers amazon has picked for us:

ns-1619.awsdns-10.co.uk.
ns-1281.awsdns-32.org.
ns-682.awsdns-21.net.
ns-211.awsdns-26.com.

go back to the registrar to put those in
nameservers, basic dns, changing that to custom dns
entering the four name servers above, *without* the periods at the ends

dnspropegation happens pretty quick, then check with nslookup

$ nslookup net23.cc
$ nslookup net23.cc 8.8.8.8
$ nslookup -type=NS net23.cc

first two won't work yet because you haven't made any A records
the 8s uses google's dns to check, rather than whatever is local

== ssl certificate

acm, list certificates, request
request a public certificate
add another name to this certificate to get two boxes, then enter
net23.cc
*.net23.cc
defaults of dns validation, rsa2048, request

at this point you can get the ARN
and status is pending validation

there's a button create records in route53
follow that flow, it'll make a cname record
in route53 you can see it made the record
still pending, taking a few minutes, then issued, green check

then put the ARN in .env and run the script:

$ serverless deploy

											_ _ 
	___ _ __ ___   __ _(_) |
 / _ \ '_ ` _ \ / _` | | |
|  __/ | | | | | (_| | | |
 \___|_| |_| |_|\__,_|_|_|
													

== email

goals and steps:
[x]review how you setup email forwarding already in your personal cloudflare account
[x]setup forwarding so support@cold3.cc forwards to another domain
[x]configure noreply@cold3.cc to correctly bounce or kill email
[x]configure DKIM and SPF for maximum security and correctness
[]get a lambda at api.net23.cc to send email from noreply@cold3.cc on surface and deep in headers

cloudflare dashboard for previous example domain, where you setup pages and email forwarding
websites, example.com
back on the left, dns, records
seeing these records as a result of setting up pages and email forwarding:

AAAA   www          100::                                         Proxied
CNAME  example.com  example-com.pages.dev                         Proxied
MX     example.com  route3.mx.cloudflare.net                      DNS only
MX     example.com  route2.mx.cloudflare.net                      DNS only
MX     example.com  route1.mx.cloudflare.net                      DNS only
TXT    example.com  "v=spf1 include:_spf.mx.cloudflare.net ~all"  DNS only

back on the left, email
configuration summary, custom addresses, 2 dns records, routing status enabled, email dns records configured

cloudflare dashboard for cold3.cc, where so far you've setup pages and workers but no email yet
websites, cold3.cc
back on the left, dns, records
there are only two:

AAAA   www       100::            Proxied
CNAME  cold3.cc  cold3.pages.dev  Proxied

back on the left, email, just a link to this documentation:
https://developers.cloudflare.com/email-routing/
and a big blue get started button, clicking it

breadcrumbs are
1 Create a custom address
2 Verify destination address
3 Configure your DNS

you're goign to set these two up now:

k----@cold3.cc   -> ke---@efa--.ll-
support@cold3.cc -> support@efa--.ll-

on the first one, step 2 is they send you an email link
and step 3 is they show you the extra dns records they'll add
then to add the second forwarding address click
email, email routing, routing rules, custom addresses, create address

you see how you can create an alias that drops emails, and will set that up now, as well
cloudflare dashboard, cold3.cc, email, email routing, routing rules, custom addresses, create address
custom address, noreply@cold3.cc
action, Drop

now the routing rules tab has a table with the three custom addresses
you can turn them on and off individually there, too
also, going back to dns, records, you can see the original AAAA and CNAME records
and the new MX and TXT ones that configuring email routing created

(...next, you leave the cloudflare dashboard and head over to aws)

remember, you have setup your domain ef---.ll- at namecheap and squarespace
aws has never heard of it outside of the pinpoint verification you just completed
you have working email forwarding like support@ef---.ll-
the domain you've setup at aws, net23.cc, the user should never see, not even in headers
and considering all this, your goal:
"a lambda at api.net23.cc can send an email from noreply@ef---.ll-"
should be possible

											_ _     ____  
	___ _ __ ___   __ _(_) |   |___ \ 
 / _ \ '_ ` _ \ / _` | | |     __) |
|  __/ | | | | | (_| | | |_   / __/ 
 \___|_| |_| |_|\__,_|_|_( ) |_____|
												 |/         

you started down the breadcrumbs with cold3.cc, but then read this on step 2:
"By adding your domain to Amazon SES it also allows your recipients to know that the emails coming from you. For your initial domain, we recommend verifying a domain that loads a webpage with identifiable information about your organization."
so, you need to use ef---.ll- not cold3.cc

aws dashboard labyrinth, ses homepage, get started button

here are the breadcrumbs:
Step 1 Add your email address
Step 2 Add your sending domain
Step 3 - optional Add MAIL FROM domain
Step 4 Review and get started with SES

Step 1

Add your email address, To get started with Amazon SES you must provide an email address so that we can send you a verification link. This verification process shows us you're the owner of the email address., A verification email will be sent to you at this address.

support@ef---.ll-

Step 2

Add your sending domain; A domain identity usually matches your website or business name. Amazon SES needs to be linked to your domain and verified in order to send emails to your recipients through SES. By adding your domain to Amazon SES it also allows your recipients to know that the emails coming from you. For your initial domain, we recommend verifying a domain that loads a webpage with identifiable information about your organization; To verify ownership of a domain, you must have access to its DNS settings to add the necessary records.

ef---.ll-

Step 3

Add MAIL FROM domain - optional; Configuring a custom MAIL FROM domain for messages sent from this identity enables the MAIL FROM address to align with the From address. Domain alignment must be achieved in order to be DMARC compliant; The MAIL FROM domain refers to the domain that appears in the 'From' field of an email message and is recommended for better deliverability, reputation management, and branding purposes. The MAIL FROM domain must be a subdomain of the verified identity from which you’re sending.

mail.ef---.ll-

Behavior on MX failure; Choose which action Amazon SES should take if your MAIL FROM domain's MX (Mail Exchange) record is not set up correctly

(x) Use default MAIL FROM domain; Will use a subdomain of amazonses.com instead of your custom MAIL FROM domain.
( ) Reject message; Will automatically reject the message without sending it.

Done

You are in Sandbox; In a sandbox environment, you can use all of the features offered by Amazon SES; however, certain sending limits and restrictions apply. When you're ready to move out of the sandbox, submit a request for production access. Before you submit a request for production access you must complete the tasks below.
[]Verify email address; To verify ownership of this email, check your inbox for a verification request email and click the link provided.
[]Send test email (Optional but recommended); Amazon SES mailbox simulator lets you test how your application handles different email sending scenarios.
[]Verify sending domain; Click on the button below and add the generated CNAME records to your domain's DNS provider.

Get DNS Records

there's a csv file to download with 3 CNAME, 2 TXT, and a MX record
going to namecheap to add them
already in namecheap are dns records for squarespace, and google workspace

(notes)

chatgpt suggested "mail" as the subdomain name
and explained there are four things i should configure here:
chat suggested dns records, as well, all of which are covered by amazon's instructions
SPF (Sender Policy Framework) Ensures AWS SES is authorized to send emails on behalf of your domain.
DKIM (DomainKeys Identified Mail) Adds email authentication to prevent spoofing.
DMARC (Domain-based Message Authentication, Reporting & Conformance) Helps manage and report on email authentication.
MAIL FROM Domain. Aligns the MAIL FROM domain with your From address for DMARC compliance and improved deliverability.

											_ _     _____ 
	___ _ __ ___   __ _(_) |   |___ / 
 / _ \ '_ ` _ \ / _` | | |     |_ \ 
|  __/ | | | | | (_| | | |_   ___) |
 \___|_| |_| |_|\__,_|_|_( ) |____/ 
												 |/         

2024jul3 going hard on dns for ef---.ll- to setup a net23 lambda that sends a message from ef---.ll-
first, some dns settings that are true:

amazon says, add these records to the dns for ef---.ll-:

CNAME  lzbihthz6dhrqvobiijpla77a4zfd5mo._domainkey.ef---.ll-  lzbihthz6dhrqvobiijpla77a4zfd5mo.dkim.amazonses.com
CNAME  ruhjtsmevec56krss4pvkw6b5kwalsmy._domainkey.ef---.ll-  ruhjtsmevec56krss4pvkw6b5kwalsmy.dkim.amazonses.com
CNAME  xhea6vwl7aw62ocquwlrerepecsrl6da._domainkey.ef---.ll-  xhea6vwl7aw62ocquwlrerepecsrl6da.dkim.amazonses.com
MX     mail.ef---.ll-                                         10 feedback-smtp.us-east-1.amazonses.com
TXT    mail.ef---.ll-                                         "v=spf1 include:amazonses.com ~all"
TXT    _dmarc.ef---.ll-                                       "v=DMARC1; p=none;"

remember that at this point, ef---.ll- is a squarespace with email through google workspace
namecheap says, these records are already current on ef---.ll-:

A      @                     198.185.159.144
A      @                     198.185.159.145
A      @                     198.49.23.144
A      @                     198.49.23.145
CNAME  www                   ext-cust.squarespace.com.
CNAME  3xwzwwzn2sx39xjmdp8w  verify.squarespace.com.

and doesn't show you the gmail ones, those they provide automatically
nslookup says, these are the current mx records for ef---.ll-:

MX  ef---.ll-  preference = 10, mail exchanger = aspmx3.googlemail.com
MX  ef---.ll-  preference = 5, mail exchanger = alt1.aspmx.l.google.com
MX  ef---.ll-  preference = 5, mail exchanger = alt2.aspmx.l.google.com
MX  ef---.ll-  preference = 1, mail exchanger = aspmx.l.google.com
MX  ef---.ll-  preference = 10, mail exchanger = aspmx2.googlemail.com

by the way, here are cloudflare's current dns records for cold3.cc:

AAAA   www       100::
CNAME  cold3.cc  cold3.pages.dev
MX     cold3.cc  route3.mx.cloudflare.net
MX     cold3.cc  route2.mx.cloudflare.net
MX     cold3.cc  route1.mx.cloudflare.net
TXT    cold3.cc  "v=spf1 include:_spf.mx.cloudflare.net ~all"

options for how to get this all combined:
(option 1) keep your dns at namecheap. less of a change, but more cowboy. website should stay up, but gmail could go down
(option 2) move dns to cloudflare. closer to eventual finish, squarespace could go down, saves $7/month on gmail
also, chatgpt says the mx records won't conflict because the amazon one is mail.ef---.ll- and the google ones are ef---.ll-
you're also noticing a txt record about spf1, but hopefully the mail.ef---.ll- and ef---.ll- will keep those from conflicting, too

you're picking option 2, if the squarespace goes down nobody's looking at it right now, less cowboy, closer to finish

steps for option 2:
[x]manually forward email you want from k----@ef---.ll- gmail to ef---ll-@gmail.com before you break squarespace google workspace
[x]namecheap stays the registrar, but move DNS from namecheap to cloudflare, like you did for cold3.cc
[x]get squarespace working again, first just put in the same records as before
[x]setup email routing in cloudflare, follow steps you just followed for cold3.cc for k----@, support@, noreply@
[x]add amazon records and complete domain verification on the ses dashboard
[x]apply to be let out of the sandbox

official steps above, some more notes within steps:
web records that point to squarespace will hopefully stay the same
intentonally break squarespace google workspace, unsignup to save $7/month, delete chrome profile
setup email routing in cloudflare, everything goes to ef---ll-@gmail.com

== 2024jul4 move ef---.ll- dns from namecheap to cloudflare

manually forwarded email from k----@ef---.ll- to ef---ll-@gmail.com
in squarespace, cancelled google workspace

cloudflare ef---ll- dashboard
websites, lists coldstart.cc, cold2.cc, cold3.cc
big blue button, add a site

link to documentation: https://developers.cloudflare.com/learning-paths/get-started
"This process sets up your web traffic to proxy through Cloudflare. Proxying speeds up and protects websites and services served by this domain."
https://developers.cloudflare.com/fundamentals/concepts/how-cloudflare-works
"If the domain’s status is active and the queried DNS record is set to proxied, then Cloudflare responds with an anycast IP address, instead of the value defined in your DNS table. This effectively re-routes the HTTP/HTTPS requests to the Cloudflare network, instead of directly reaching the targeted the origin server.
"In contrast, if the queried DNS record is set to DNS only, meaning the proxy is off, then Cloudflare responds with the value defined in your DNS table (that is, an IP address or CNAME record). This means HTTP/HTTPS requests route directly to the origin server and are not processed or protected by Cloudflare."
cold3.cc dashboards show DNS setup: full, and Proxied on

ef---.ll-, free plan
cloudflare says it's bringing in existing dns records, and it found
4 A, 1 CNAME, 5 MX
shoulda found 2 cname, but whatever

cloudflare says "Avoid DNS resolution issues caused by DNSSEC; Find the DNSSEC setting at your registrar (per-provider instructions). If it is on, you have two options: Turn DNSSEC off at least 24 hours before updating your nameservers. Most common; Migrate your existing DNS zone without turning off DNSSEC. More advanced; After your domain activates, we recommend turning DNSSEC on through Cloudflare."
namecheap dashboard, dnssec is off
is dnssec on for cold3.cc? no! you found the start of the flow in cloudflare, websites, cold3.cc, dns, settings, first box is dnssec
https://developers.cloudflare.com/dns/dnssec/
TODO add dnssec to cold3.cc as practice and then ever.fans as production, following steps in cloudflare and namecheap

namecheap, ef---.ll-, before changes:
nameservers is set to Namecheap BasicDNS
advanced dns tab shows the records above, the 4 A records and CNAME
dnssec is off
mail settings is Gmail, text says Gmail automatically configured for ef---.ll-

switching from Namecheap Basic DNS to Custom DNS
entering the nameservers from cloudflare:

carl.ns.cloudflare.com
maya.ns.cloudflare.com

2024jul4 1:13p finished flow on cloudflare, which says "Cloudflare is now checking the nameservers for ef---.ll-. Please wait a few hours for an update."
1:23p "Great news! Cloudflare is now protecting your site; Data about your site's usage will be here once available."
dnschecker.org also shows maya and carl for ef---.ll-, just like cold3.cc
your website site is down, ssl mismatch

cloudflare dashboard, quick start guide
automatic https rewrites, yes
always use https, yes

cloudflare dns records
4 A are correct
only one CNAME, [x]add the second
5 MX records all about gmail, [x]remove them, so now they again look like this:

A      @                     198.185.159.144
A      @                     198.185.159.145
A      @                     198.49.23.144
A      @                     198.49.23.145
CNAME  www                   ext-cust.squarespace.com
CNAME  3xwzwwzn2sx39xjmdp8w  verify.squarespace.com

you pasted a trailing period when adding the verify cname, but cloudflare isn't showing it

sqarespace dashboards, red text and boxes that says "DNS Error, We couldn't verify DNS settings with Namecheap."
and then they have a table of right and wrong settings, but it changes every time you refresh
cloudflare dashboard, ef---.ll-, dns records, turned proxy off on all records
at squarespace, refresh still is crazy
changed nothing, refreshed site, ef---.ll- is back online, ssl looks ok
squarespace dashboard, settings, domains and email, domains managed by third-party, ef---.ll-, red message changed to green Connected

interestingly(?) in firefox it's really easy to see who issued the certificate, and right now it shows
net23.cc: Verified by: Amazon (makes sense, as this ssl came from AWS ACM)
cold3.cc: Verified by: Google Trust Services (this is cloudflare pages and workers)
ef---.ll-: Verified by: Let's Encrypt (this is cloudflare dns, squarespace hosting, no idea where the ssl came from)

ok, on to setting up email forwarding
cloudflare dashboard, ef---.ll-, email, email routing, get started
destination already verified, so that step is faster

k----@ef---.ll-   -> ef---ll-@gmail.com
support@ef---.ll- -> ef---ll-@gmail.com
noreply@ef---.ll- -> Drop

all the tests worked, except nothere@ef---.ll- didn't bounce, but whatever

now at last you go back to the amazon steps
you configured aws at mail.ef---.ll-
you think later you can configure twilio at mail2.ef---.ll-, TODO

aws dashboard, ses
yellow note saying im in the sandbox
verify sending domain, get dns records, download record set, got a csv file:

CNAME  lzbihthz6dhrqvobiijpla77a4zfd5mo._domainkey.ef---.ll-  lzbihthz6dhrqvobiijpla77a4zfd5mo.dkim.amazonses.com
CNAME  ruhjtsmevec56krss4pvkw6b5kwalsmy._domainkey.ef---.ll-  ruhjtsmevec56krss4pvkw6b5kwalsmy.dkim.amazonses.com
CNAME  xhea6vwl7aw62ocquwlrerepecsrl6da._domainkey.ef---.ll-  xhea6vwl7aw62ocquwlrerepecsrl6da.dkim.amazonses.com
MX     mail.ef---.ll-                                         10 feedback-smtp.us-east-1.amazonses.com
TXT    mail.ef---.ll-                                         "v=spf1 include:amazonses.com ~all"
TXT    _dmarc.ef---.ll-                                       "v=DMARC1; p=none;"

left proxying on entering the first cname, and got an error that says it can't be proxied

entering the mx record, there's a box that says, Priority (required)
you're going to enter just feedback-smtp.us-east-1.amazonses.com as the value, and separately dial in priority 10
also, you entered the name "mail.ef---.ll-" but cloudflare is showing it as just "mail"

for the txt record, cloudflare gives you a larger box, you're pasting in as above including the quotes
it saves with the quotes

ok, they're all in there, a mix of records about squarespace, about cloudflare email forwards, and about ses
only some things are proxyable, but you've turned off proxying on all of them

amazon doesn't have a check now button, rather it says
"Identity status: Verification pending; Last checked: July 4, 2024 at 14:21 (UTC-04:00)" and it's 14:56
you might have gotten the records with that button, and then not set them quick enough for the first check
15:02 without refresh, looking back at the page, green checkmark Verified

last of three boxes on ses page is to send test email, so let's see what's in there
"Send test email Info; The Amazon SES mailbox simulator lets you test how your application handles different email sending scenarios."
from is already set to support@ef---.ll-
there's no to box, it looks like messages go to your choice of options like success@simulator.amazonses.com
there is a cc box, so let's try that? ef---ll-@outlook.com
doesn't like it because not a verified identity

aws dashboards, ses, configuration, identities
already there are two listed, both green verified:

ef---.ll-, Domain
support@ef---.ll-, Email address

so you need to [x]turn noreply into a forward, [x]verify it, then [x]turn it back into a drop target
this is easy to do you just edit the existing row
verified that one, and also ef---ll-@outlook.com
and sent a test email. now the box in the ses dashboard says green check test email sent, but you haven't gotten it in outlook yet, but whatever

aws dashboard, ses, get set up
box on the top, you're still in the sandbox (but now the boxes below are all green), button request production access

== filled out the aws ses form to request production access

To help us evaluate your request for production access, fill out the following form outlining how you plan to use Amazon SES to send email once your account has moved out of the sandbox; Production access means you can send email to any recipient, regardless of whether the recipient's address or domain is verified. However, you must still verify all identities that you use as "From", "Source", "Sender", or "Return-Path" addresses.
https://docs.aws.amazon.com/ses/latest/dg/request-production-access.html
and they say approval takes just 24 hours, which is great

Mail type; Choose the option that best represents the types of messages you plan on sending. A marketing email promotes your products and services, while a transactional email is an immediate, trigger-based communication.

Transactional

Website URL; Provide the URL for your website to help us better understand the kind of content you plan on sending.

https://www.ef---.ll-/

Use case description: Explain how you plan to use Amazon SES to send email. Specifically, tell us:
-How do you plan to build or acquire your mailing list?
-How do you plan to handle bounces and complaints?
-How can recipients opt out of receiving email from you?

next day, approved: "Thank you for submitting your request to increase your sending limits. Your new sending quota is 50,000 messages per day. Your maximum send rate is now 14 messages per second. We have also moved your account out of the Amazon SES sandbox." huzzah. with great power comes great responsibility. and Network 23 will use it wisely

											_ _     _  _   
	___ _ __ ___   __ _(_) |   | || |  
 / _ \ '_ ` _ \ / _` | | |   | || |_ 
|  __/ | | | | | (_| | | |_  |__   _|
 \___|_| |_| |_|\__,_|_|_( )    |_|  
												 |/          

[]make a hello world lambda
[]make lambdas that send sms and email
[]secure them so only cold3.cc can call them















	 _                          _                 
	(_)___  __   _____ _ __ ___(_) ___  _ __  ___ 
	| / __| \ \ / / _ \ '__/ __| |/ _ \| '_ \/ __|
	| \__ \  \ V /  __/ |  \__ \ | (_) | | | \__ \
 _/ |___/   \_/ \___|_|  |___/_|\___/|_| |_|___/
|__/                                            

coding with big int literals, nuxt freaked out
it seems like it was building everything to es2019, and those are es2020
also, soon you're going to start using library functions from lambdas, which are node
so it's time to figure out what javascript version you want, and set it explicitly

here's what amazon supports for lambda right now, 2024jul:
https://docs.aws.amazon.com/lambda/latest/dg/lambda-runtimes.html
Node.js 20, current
Node.js 18, current
Node.js 16, deprecation date 2024jun12
so from that, picking Node 20

you just updated node and npm on your development workstation:
$ node --version, 20.15.0
$ npm --version, 10.8.1
so that matches well

it seems there isn't a good way to match a node version to an esXXXX version
because platform vendors, like google making v8, implement different features from the spec at different times
this page is detailed, but useless:
https://v8.dev/features

so what version is cloudflare workers running?
https://developers.cloudflare.com/workers/runtime-apis/web-standards/
"The Workers runtime is updated at least once a week, to at least the version of V8 that is currently used by Google Chrome’s stable release. This means you can safely use the latest JavaScript features, with no need for transpilers."
ok, so that means way more recent than 2019 or 2020

$ nuxt build was the simplest command to hit the big int literal error:

	Kevin@ncom3 MINGW64 /.../repot1 (main)
	$ npm run build
	> nuxt build
	Nuxt 3.12.3 with Nitro 2.9.7
	i Building client...

	[nitro]  ERROR  Error: Transform failed with 2 errors:
	C:\Documents\code\code21site\repot1\server\api\myapi.ts:12:8: ERROR: Big integer literals are not available in the configured target environment ("es2019")

(as an aside: this doesn't really make sense because even if it is targeting es2019, shouldn't transplation correctly turn my big int literals into something that es2019 can understand? isn't that the whole point of transplilation?)
this configuration addition fixes it, in /nuxt.config.txt

	export default defineNuxtConfig({
		compatibilityDate: '2024-04-03',
		devtools: { enabled: true },
		nitro: {
			preset: "cloudflare-pages",
			esbuild: { options: { target: 'esnext' } }// <-- new line that fixes big int literal error
		},
		modules: ["nitro-cloudflare-dev"],
	})

so that's cool. "es2020" also fixes it, but you're using "esnext", because:
2020 was still four years ago,
es2023 and es2024 might not be real yet, and
esnext is also a good match for cloudflare's "what's our version? current!" thing



(bookmark)
then add more here about versioning net23 lambdas with babel transpilation to node20








										 _       _           
 _ __ ___   ___   __| |_   _| | ___  ___ 
| '_ ` _ \ / _ \ / _` | | | | |/ _ \/ __|
| | | | | | (_) | (_| | |_| | |  __/\__ \
|_| |_| |_|\___/ \__,_|\__,_|_|\___||___/
																				 

first, you want wrangler and serverless installed locally, not globally
commands to install, version, and uninstall globally:
$ npm install -g wrangler
$ npm list -g wrangler
$ npm uninstall -g wrangler
serverless 4.0.35 was installed globally, uninstalled that to install serverless 3 in project

commands to build up net23 package.json
note that you have to install serverless version 3, as current is 4 but even serverless-webpack, which has 235k weekly downloads, isn't ready for serverless 4
all of these modules are plenty popular, except perhaps for serverless-s3-sync
installing in this order, two of them were already there, but still installing to list in package.json

$ npm install -S aws-sdk                      8 million weekly downloads, added 37 packages

$ npm install -D serverless@3                 1 million, 550 packages

$ npm install -D babel-loader                15 million, 122 packages
$ npm install -D @babel/core                 45 million, this one is already there!
$ npm install -D @babel/preset-env           22 million, 112 packages

$ npm install -D webpack                     24 million, this one is already there!
$ npm install -D webpack-cli                  6 million, 32 packages

$ npm install -D serverless-webpack         235 thousand, 52 packages
$ npm install -D serverless-offline         519 thousand, 85 packages
$ npm install -D serverless-domain-manager  220 thousand, 62 packages
$ npm install -D serverless-s3-sync          32 thousand,  9 packages

as an aside, "$ serverless deploy" works, but doesn't notice changed files in www
you added a script to package.json so "$ npm run www" force syncs everything


(note later)
to avoid obscure modules, you replaced serverless-s3-sync with an aws cli command:
"www": "aws s3 sync ./www s3://www-net23-cc --delete"
$ yarn www
$ aws cloudfront create-invalidation --distribution-id E2J...KAR --paths "/*"
$ aws cloudfront list-invalidations --distribution-id E2J...KAR
it correctly deletes files in the bucket that aren't in the folder!
use create and list invalidations to get the cloudfront distribution in front of it to get new files from the bucket






(bookmark)
next three scopes:
1 hello world lambda, no custom domain, no webpack
2 lambdas as api.net23.cc
3 webpack and babel
try running local and deployed
can you see the bundle size before and after?
make an api endpoint to run tests, should be fine because local node 20 works great






 _          _ _         _                 _         _       
| |__   ___| | | ___   | | __ _ _ __ ___ | |__   __| | __ _ 
| '_ \ / _ \ | |/ _ \  | |/ _` | '_ ` _ \| '_ \ / _` |/ _` |
| | | |  __/ | | (_) | | | (_| | | | | | | |_) | (_| | (_| |
|_| |_|\___|_|_|\___/  |_|\__,_|_| |_| |_|_.__/ \__,_|\__,_|
																														

# lambda step 1: hello world and local development

notes getting started with lambda
$ npm run local
runs serverless offline, which emulates AWS Lambda and API Gateway
then you can hit your APIs at urls like:
http://localhost:3000/prod/hello1
http://localhost:3000/prod/hello2
live updates don't seem to work, but maybe that's not an intended feature

$ npm run deploy
first time, right now with two lambdas that aren't using a custom domain

# lambda step 2: manual steps to get custom domain

serverless.yml should be able to create api.net23.cc, but you're getting this error:
$ npm run deploy
Deploying net23 to stage prod (us-east-1)
Warning: V1 - 'api.net23.cc' does not exist.
Error: V1 - Make sure the 'api.net23.cc' exists.

aws dashboard, route 53, net23.cc, you see stuff for the root and www, but nothing yet for api.net23.cc
aws dashboard, api gateway, apis, there is a listing named prod-net23, and clicking into it you do see hello1 and hello2

ok, instead of trying to get this one-time configuration to be created by serverless.yml,
you're going to document steps here to set it up manually:

aws dashboard, api gateway, custom domain names
api.net23.cc
tls 1.2
regional (not edge optimized)
net23.cc, choose ACM certificate from drop down, only one there
create domain name button, green success banner

aws dashboard, api gateway, custom domain names, api.net23.cc
configurations, endpoint configuration, api gateway domain name
copy this as you'll paste it into route53 in a moment

aws dashboard, api gateway, custom domain names, api.net23.cc
api mappings, second tab
configure api mappings, add new mapping
api dropdown, one thing listed, what was created by serverless, prod-net23 (REST)
prod, stage, only option
path (optional), leave blank because you want root
save

aws dashboard, route53, hosted zones, net23.cc, create record
api, record name
A, record type, already set to this
switch on Alias
Route traffic to, dropdown with lots of options, Alias to API Gateway API
us-east-1, N. Virginia, region
choose endpoint, dropdown with one item, suggests the domain like blah.execute-api.us-east-1.amazonaws.com we just created
simple routing policy, yes evaluate target heath, leaving these defaults
create records

back in route23 for net23.cc, all your dns records from before are there, along with the new one for api.net23.cc
and it looks like it's directing to, and saying, the under-the-hood domain like:
blahblah.execute-api.us-east-1.amazonaws.com

# lambda step 3: turn off side door access, hard or impossible and gave up

related to the checks below, there's the www | root X http | https checks
the api subdomain means we don't have to worry about www | root here, which is by design
checking http, chrome takes a long time and then reports connection refused
firefox takes a long time, and then redirects to https
so all that looks ok for now

https://api.net23.cc/hello1 ~ works, correctly
https://jkXXXXXXo3.execute-api.us-east-1.amazonaws.com/prod/hello1 ~ works, wrongly, id from api gateway
https://d-1hXXXXXXyh.execute-api.us-east-1.amazonaws.com/prod/hello1 ~ blocked, correctly, value from route53

as always with aws, there are manual steps or you can try automatic
manual steps involve pasting in code
automatic steps involve a secret that includes your aws account number

as both include blocks of json, trying infrastructure as code
added ACCESS_API_GATEWAY_RESOURCE_ARN to .env and section that uses it to serverless.yml

before running, checked the dashboard:
aws dashboard, api gateway, prod-net23
halfway down on the left, resource policy
starting out seeing policy details, no resource policy

ok, spent a few hours going in circles with chatgpt trying to turn off the side door
chat repeatedly went to controlling who can call the function, not where the function is callable
using the referer header, which of course can be spoofed anyway
then got it to explain that aws doesn't have a way to turn off access at the amazonaws.com domain
"Unfortunately, AWS does not provide a direct option to completely disable the default amazonaws.com endpoint for API Gateway. ... Although you cannot completely disable the default domain, you can create a Lambda@Edge function or use API Gateway’s HTTP integration to redirect any traffic from the default domain to your custom domain."
so that's way too much work and complexity for this that likely doesn't even matter, giving up

# lambda step 4: let in webpack and babel to reduce bundle size and use modules

$ npm run pack
$ npm run deploy

pack just runs webpack, building to dist/hello1.js
deploy does that as part of the serverless framework upload to aws lambda
before adding webpack, each hello was 7.1 MB; now serverless framework says it's 157 bytes
on disk from pack its 494 bytes, though, but whatever

also, deploy seems to delete the dist folder!
it's there after pack, gone after deploy

fought an error about ES6 module syntax or not in the webpack configuration
generally, trying to pick ES6 module syntax everywhere because works in the browser and server, more recent and current, and helps tree shaking perform best
right now it's working with these choices:
-package.json includes "type": "module", keeping ES6 setting
-webpack.config.cjs ends .cjs not .js or .mjs, reverting to CommonJS setting
-serverless.yml custom.webpack.webpackConfig references the webpack config with that extension
-insde webpack.config.cjs, require, module.exports, and __dirname are all legacy Node that matches CommonJS
but also, there are two mentions of commonjs2, setting the output format, necessary for webpack to bundle for lambda's node environment

# lambda step 5: three horrible hacky workarounds

here are scripts in package.json:

	"scripts": {
(1) "www": "serverless s3sync"
(2) "deploy": "webpack && serverless deploy"
(3) "local": "serverless offline --config local.yml",

(1) sync the www bucket
serverless deploy should sync the www bucket
except it doesn't
so instead www calls s3sync directly

(2) pack and upload lambda functions
serverless deploy should run webpack and then upload the packed functions
except it like deletes them and then can't find them or something
so intead running webpack manually beforehand works
and npm run deploy does this in a single command that bundles two steps

(3) run lambda functions locally
serverless-offline can emulate api gateway and lambda locally
except somehow it can't find the named handler in the output bundle
so this hack uses a separate yml config that points at the source files
and that way, runs them fine

here are some paths related to all this:

on disk:
./src/hello1.js ~ source code file
./dist/hello1.js ~ bundled webpack output file

in settings in local.yml:
	handler: src/hello1.handler # Paths to src to run before webpack
in settings in serverless.yml:
	handler: dist/hello1.handler # Paths to dist to upload after webpack

in code in ./src/hello1.js:
	"... export const handler = async (event) => {..."
in code in ./dist/hello1.js:
	"...e.d(o,{handler:()=>t});const t=async..."

on the web:
	http://localhost:3000/prod/hello1 ~ path to function emulated locally
	https://api.net23.cc/hello1 ~ path to function deployed  

you've seen subfolders like src/handlers and dist/service appear, but for the moment at least, have managed to keep them away

so, this is working, but hacky as f**k with three workarounds, and you haven't even tested using a module or library code yet, ugh

# lambda step 6: switched to rollup

switched from webpack and babel to rollup, and everything works great
$ npm run local   is "local": "rollup -c && serverless offline",
$ npm run deploy  is "deploy": "rollup -c && serverless deploy",
and the serverless-webpack module is replaced with nothing

here is net23's package.json with rollup, looking at popularity and recency:

serverless                 1 million      1month
serverless-offline           536k         2months
serverless-domain-manager    198k         5months
serverless-s3-sync           31k          6months ~ a little too rare

rollup                        22 million  1month
@rollup/plugin-commonjs        3 million  1month
@rollup/plugin-node-resolve    6 million  9months
@rollup/plugin-terser          1 million  9months
@rollup/plugin-json            2 million  7months
rollup-plugin-node-polyfills     776k     5years ~ a little too old

# lambda step 7: great success

here's what we finished:
[x]hello lambda
[x]develop locally, using serverless framework's emulation of api gateway and lambda
[x]do some math big int style
[x]use a node module directly, nanoid probably
[x]import library code
[x]run all the tests

																						 _      
	__ _  ___ ___ ___  ___ ___    ___ ___   __| | ___ 
 / _` |/ __/ __/ _ \/ __/ __|  / __/ _ \ / _` |/ _ \
| (_| | (_| (_|  __/\__ \__ \ | (_| (_) | (_| |  __/
 \__,_|\___\___\___||___/___/  \___\___/ \__,_|\___|
																										

only cold3.cc should be able to call network 23 apis
in addition to cors and checking the origin header and so on,
the simplest way this is secured is with an api access key
this is just like how we access supabase and other third party apis
except here, we're coding both sides
here's an example:


when cold3 makes a request to net23, it includes this access key
cold3 gets the key from cloudflare secrets
net23 checks the key from lambda environment variables or aws secret manager or whatever

you need to store this secret a lot of places:
-env files for local development, both cold3 and net23
-cloudflare secrets
-lambda's equivalent of cloudflare secrets

.env:
ACCESS_NETWORK_23=VeryLongSecretCodeLikeDsqmB9YxbtseDuUHwyJnjDsqmB9YxbtseDuUHwyJnjAndSoOn

cloudflare:
cloudflare dashboard, workers and pages, cold3, settings tab, environment variables

serverless.yml:
provider:
	environment: # Name variables in .env file here so they are on process.env for the Lambda code
		ACCESS_NETWORK_23: ${env:ACCESS_NETWORK_23}

and then in lambda code like hello2.js:
	let access = 'not found'
	if (typeof process.env.ACCESS_NETWORK_23 == 'string') access = process.env.ACCESS_NETWORK_23.length

you don't have to mess with the aws dashboard, which is great

			 _             
 _ __ (_)_ __   __ _ 
| '_ \| | '_ \ / _` |
| |_) | | | | | (_| |
| .__/|_|_| |_|\__, |
|_|            |___/ 

2024sep setup ping pages and got service providers to hit them and generate nice graphs

cold3.cc/ping/ping1                                            -a nuxt page with no script tag
cold3.cc/ping/ping2                                            -a nuxt page with a trivial script tag
cold3.cc/ping/ping3 > cold3.cc/api/ping3                       -a nuxt page that fetches to a trivial nuxt api
cold3.cc/ping/ping4 > cold3.cc/api/ping4 > Supabase            -all that, but the server api code queries the global count from supabase
cold3.cc/ping/ping5 > cold3.cc/api/ping5 > api.net23.cc/ping5  -instead of calling supabase, fetches to our own trivial lambda

example output, deployed to cloud, curl and browser:

ping1: template says: ping1done (curl)
ping1: template says: ping1done (browser)

ping2: script setup says: CloudPageServer:Envi.Proc.Scri.Self.Serv.Zulu,           1725904851660, v2024sep8b, ping2done (curl)
ping2: script setup says: CloudPageClient:Achr.Asaf.Awin.Docu.Doma.Self.Stor.Wind, 1725904716570, v2024sep8b, ping2done (browser)

ping3: fetch to worker took 0ms to say: worker says: CloudNuxtServer:Aclo.Envi.Proc.Scri.Self.Zulu, 1725904908685, v2024sep8b, ping3done (curl)
ping3: fetch to worker took 2ms to say: worker says: CloudNuxtServer:Aclo.Envi.Proc.Scri.Self.Zulu, 1725904728918, v2024sep8b, ping3done (browser)

ping4: fetch to worker to database took 494ms to say: worker says: database took 494ms to get count 569, CloudNuxtServer:Aclo.Envi.Proc.Scri.Self.Zulu, 1725904949932, v2024sep8b, ping4done (curl)
ping4: fetch to worker to database took   2ms to say: worker says: database took 217ms to get count 569, CloudNuxtServer:Aclo.Envi.Proc.Scri.Self.Zulu, 1725904740498, v2024sep8b, ping4done (browser)

ping5: fetch to worker to lambda took 354ms to say: worker says: lambda took  354ms to say: lambda says: CloudLambda:Eigh.Envi.Glob.Lamb.Node.Proc.Regi.Zulu, 1725904979928, v2024sep8b, ping5done (curl)
ping5: fetch to worker to lambda took   2ms to say: worker says: lambda took 1277ms to say: lambda says: CloudLambda:Eigh.Envi.Glob.Lamb.Node.Proc.Regi.Zulu, 1725904754597, v2024sep8b, ping5done (browser)

notes making sense of these results
ping2, curl is from server rendering, so zulu time, and browser replaces that with client rendering, which has a window object
ping3, server render adds 0ms because it's all happening at once; senseEnvironment the same because it's always from a fetch to the nuxt api
ping4 and ping5, real times are on curl page, when re-rendered on the client it must cache or skip or something so you get 2ms instead of nearly half a second
and while supabase takes a half second, lambda frequently takes over a second

looked for service providers with these capabilities:
-simple request, don't emulate a rendering, code-running browser
-try from all over the world, randomly
-look for text in the page that you get
-say and show how long it took, average and higher percentile

trying out:
-pingdom.com - västerås now owned by solarwinds in austin, $33/month, strange duplication of uptime and speed, graphs good
-checklyhq.com - berlin, $0 hobby plan, graphs great, nicely formatted html result view
-uptime.com - nyc, $20/month, graphs ok
so you'll probably just go with checkly
they talk about complex stuff with playwright, but you found their simple thing

for each of their dashboards, you found and set:
-name
-url
-look for text like "ping5done" in page
-frequency, 1hr
-round robin or lots of random global locations

it'll be interesting to see if this affects your cloudflare and amazon bills :0
and this was a good trail, other than senseEnvironment() being a messy day in the bike shed

      _                 _    __                  _   _             
  ___| | ___  _   _  __| |  / _|_   _ _ __   ___| |_(_) ___  _ __  
 / __| |/ _ \| | | |/ _` | | |_| | | | '_ \ / __| __| |/ _ \| '_ \ 
| (__| | (_) | |_| | (_| | |  _| |_| | | | | (__| |_| | (_) | | | |
 \___|_|\___/ \__,_|\__,_| |_|  \__,_|_| |_|\___|\__|_|\___/|_| |_|
                                                                   
                       _ _      _ _          _   _                                       
 _ __   __ _ _ __ __ _| | | ___| (_)______ _| |_(_) ___  _ __   __      _____   ___  ___ 
| '_ \ / _` | '__/ _` | | |/ _ \ | |_  / _` | __| |/ _ \| '_ \  \ \ /\ / / _ \ / _ \/ __|
| |_) | (_| | | | (_| | | |  __/ | |/ / (_| | |_| | (_) | | | |  \ V  V / (_) |  __/\__ \
| .__/ \__,_|_|  \__,_|_|_|\___|_|_/___\__,_|\__|_|\___/|_| |_|   \_/\_/ \___/ \___||___/
|_|                                                                                      

in the eternal triangular tug of war between simplicity, reliability, and speed
you've decided for initial launch to keep cloud functions synchronous
what follows is a discussion of fancier things you tried, and why they were difficult, worrysome, or didn't work at all

[I.] you can fetch in parallel in your own code while handling an api request

these will run one after the other:

await fetch('https://datadog.com/api/log', ...)
await fetch('https://datacat.com/api/log', ...)

and these will run both at the same time:

let promiseDog = fetch('https://datadog.com/api/log', ...)
let promiseCat = fetch('https://datacat.com/api/log', ...)
await Promise.all([promiseDog, promiseCat])

and that code is fine in workers and lambda
the difficulty comes in when you have everything you need to get back to the client,
and want to do that right away, with cleanup tasks like logging happening in parallel or afterwards
it doesn't work because cloudflare and lambda see the request sent, think we're done here, and tear down the environment

[II.] performing tasks in a worker after responding to the client is possible but more complicated

cloudflare pins a method to the event object:

event.waitUntil(p)

you can pass it a promise, and then the worker will delay teardown,
even after you've responded to the client and returned from everything
so that's great. there is no lambda equivalent you can find

[III.] performing tasks in a lambda after responding to the client is either not possible or so rare you can't find it

the return from a lambda handler is the response back up to the client
you can't find a way to talk to the client before returning

the common pattern for lambda functions starts like this:
	exports.handler = async (event, context) => {

and an alternative documented form that looked promising (get it?) is this:
	exports.handler = (event, context, callback) => {

but to do what you want, you'd have to combine them,
which either won't work or is so weird no one else has thought of it:
exports.handler = async (event, context, callback) => {

	//first, we process the request, using steps that we do have to await
	let messageForClient = await processRequest({event, context})

	//second, we get back to the client quickly
	const response = {
		statusCode: 200,
		body: JSON.stringify({ message: 'OK', messageForClient }),
	}
	callback(null, response)

	//third, perform tasks like logging. the response has been sent, but the lambda must keep running!
	let p = performLoggingTasks()
	return p//by returning this promise, we're telling lambda to not tear down until it resolves!
}

looking more shed even more doubt on this--the returned promise p needs to resolve into the response body
at which point we're all the way back where we started

chat says lambda doesn't even support streaming a response back to a client
the whole thing, headers and complete body, needs to go out once at the end

and there's also something called context.succeed() as an alternative to callback
but it seems like that's an older api that has been replaced by the callback parameter
not a different thing that could help you here

and hours later you found callbackWaitsForEmptyEventLoop, which is already set to true
which could keep the lambda alive long enough to finish logging
but might also delay sending a returned response back to the client

so, you found a lot of complexity and very little certainty in this exploration, and are backing out

[IV.] so here are some features you're removing

const defaultFetchTimeLimit = 5*Time.second
async function ashFetchum(c, q) {//early nickname to wrapped fetch; gotta fetch 'em all
	if (!q.timeLimit) q.timeLimit = defaultFetchTimeLimit
	let response, bodyText, body, error, success = true

	let a = new AbortController()//js way of telling fetch to give up after a time limit
	let m = setTimeout(() => a.abort(), q.timeLimit)
	let o = {method: q.method, headers: q.headers, body: q.body, signal: a.signal}
	q.tick = Now()

	try {
		if (q.skipResponse) {
			let p = fetch(q.resource, o)             //get the promise instead of awaiting here for it to resolve
			if (event?.waitUntil) event.waitUntil(p) //tell cloudflare to not tear down the worker until p resolves
			p.then(() => { clearTimeout(m) })
		} else {
			response = await fetch(q.resource, o); clearTimeout(m)

in your wrapped server side fetch, there used to be
-a fire and forget option to skip waiting for the response entirely; the call isn't even async anymore
-an option to skip reading the body
-an upper time limit imposed by an abort controller

to provide the fire and forget feature, you'd have to save the promise and
on cloudflare, feed it to event.waitUntil()
on lambda, manually await it before returning
this is possible, but far less simple and likely less reliable,
either because you make a coding error due to the more complex flow
or because it causes cloudflare or amazon to act in an unusual way
also there's no benefit on lambda, only cloudflare

you could do the option to not read the body, but
the body is probably short,
has probably already arrived,
and you probably always want to log it to have a full audit of api behavior

abort controller is is a feature of fetch, and interesting, but
workers have a not configurable 30 second maximum lifetime, and
you've set your lambdas to match, from a default of 3 seconds and upper limit of 15 minutes

[V.] the big surrounding picture here

you're picking servlerless offerings like cloudflare workers, lambda, and supabase for simplicity, reliability, and scalability
you don't have to maintain a server, and if things get popular overnight, nothing should break

but this also means that every read, write, and log is another ~150ms
many user interactions will involve a handful of these, back to back
and that means the experience isn't fast anymore

a traditional architecture--with web server, database, and log file all on the same virtual instance--avoids this problem
also on a regular server, you can absolutely stream a response piece by piece back up to a client
and you can finish a response, and stay alive to perform some clean up tasks afterwards

your hope is that the site still seems remarkable to users even with some steps causing second-long pauses
the first navigation will still happen in like 35ms
clicking around will be instantaneous because vue is just showing and hiding components that already have cached data
and when the user is changing their email or uploading a file, it makes sense that those tasks would take seconds

[VI.] the next day, a map if you get back to this trail

maybe this would work for lambda, or maybe its an ai hallucination, you're not sure:
exports.handler = async (event, context) => {
	context.callbackWaitsForEmptyEventLoop = false//change away from default true
	setTimeout(() => {//do 0ms later, but in the next turn of the event loop
		console.log("Performing background task like logging...")
	}, 0)
	return {//return response to client right away
		statusCode: 200,
		headers: { 'Content-Type': 'application/json' },
		body: JSON.stringify({ message: 'Response to client' })
	}
}

and then on the cloudflare side, it's context.waitUntil(), and you just found tail workers:
https://developers.cloudflare.com/workers/observability/logging/tail-workers/
specifically designed for logging and https://developers.cloudflare.com/queues/ is in beta

also, you can call console.log and console.error in both
in lambda, they automatically go into a cloudwatch log you can see in the dashboard

[VII.] the next month, reports from the field


with lambda, it never works
with cloudflare, it works most of the time, but there are missing logs
so, for reliability you also get simplicity, and never touch this


to eliminate setWorkerEvent entirely, also load the secrets explicitly
just load their names, from process.env
this also works on worker and lambda same code
because both can dereference them
even if cloudflare can't list them


simplicity and reliabi

no, here's the plan
simple, isomorphic, reliable

instead of listing secrets from process or worker event
parse what you need to redact into words, look for words that end _SECRET, then dereference them from process

and here's how you make dog and all logs reliable but not await async
implement your own promise wait until
and then wait on it before returning the result from the handler

reliability should be unaffected
things are slightly faster because fire and forget events can proceed in parallel
although the response is still delayed by the longest one, it's not delayed by the sum duration of all of them

thre is that theoretical problem that this keeps lambdas, called quickly, from ever returning
so maybe build in a tick of first addition, and if that's too old, just return--like four seconds too old
and also put in the alert for detected overlapping lambda or worker, put that in, too

cloudPromise(p)

dog
awaitDog
logAlert
awaitLogAlert
















     _       _        _                    
  __| | __ _| |_ __ _| |__   __ _ ___  ___ 
 / _` |/ _` | __/ _` | '_ \ / _` / __|/ _ \
| (_| | (_| | || (_| | |_) | (_| \__ \  __/
 \__,_|\__,_|\__\__,_|_.__/ \__,_|___/\___|
                                           

bookmark january
here's where you talk about
-what you learned
-choices you've made
-concessions you've made
-conventions you've set
about
SQL, PostgreSQL, and Supabase
as well as manual steps to create a database
and backup and restore the database, cloud and local postgres

https://supabase.com/docs/guides/platform/backups
"All Pro, Team and Enterprise Plan Supabase projects are backed up automatically on a daily basis."
pro costs $25/month



2025jan19
went into supabase dashboard to make test database
and check settings

https://supabase.com/dashboard/projects
New project

Organization, only one there
test1, Project Name
use their generate link to create a strong password; don't seem to need it later
east us north virginia, region

security options
data api and connection string
public schema for data api
both of these are default, not sure what they mean, note that says that you can change these afterwards

advanced configuration
postgres, default, not the other one which is orioledb alpha, whatever that is

top bar, Connect
App Frameworks, Nuxt
copy out the url and key
check out the code for createClient in the next two tabs, also

now you can see
SUPABASE_URL
SUPABASE_KEY
and a code example
import { createClient } from "@supabase/supabase-js";
export const supabase = createClient(supabaseUrl, supabaseKey);

Settings, Database, SSL Configuration
Enforce SSL on incoming connections
Reject non-SSL connections to your database
turned on

Settings, configuration, API, Data API Settings
Max rows 1000
The maximum number of rows returned from a view, table, or stored procedure. Limits payload size for accidental or malicious requests.

Settings, configuration Authentication, User Signups
turned off
Allow new users to sign up
If this is disabled, new users will not be able to sign up to your application.
correct, because what does that even mean













																				_            
 ___  ___ _ __ __ _ _ __    _ __   ___ | |_ ___  ___ 
/ __|/ __| '__/ _` | '_ \  | '_ \ / _ \| __/ _ \/ __|
\__ \ (__| | | (_| | |_) | | | | | (_) | ||  __/\__ \
|___/\___|_|  \__,_| .__/  |_| |_|\___/ \__\___||___/
									 |_|                               


(This page intentionally left blank.)





2024oct8
after updating wrangler, you got supabase working again
in the cloudflare dashboard
cloudflare dashboard, workers and pages, cold3
there's an Integrations tab, on it are a bunch of third party services in boxes, including Supabase
you went through this flow
you let cloudflare oauth into your supabase account
at the end, it had made two new cloudflare secrets
and all of that was unnecessary--you can just copy and paste the endpoint url and api key from supabase yourself
you deleted the integration, and supabase still works fine
so this note is for, in the future, if you're fixing supabase again, the integrations tab probably won't help







beyond aws services, another nice thing about net23 is that
if you do run into some node module or third party service
that you cannot get working in a cloudflare worker
then you just call out to a lambda at net23 which does it there
and you've got that capability at easy reach
without making every page load slow and expensive




2024aug30 turn off default email tracking pixel in sendgrid
https://app.sendgrid.com/settings/tracking
dashboard, settings, tracking
-Enabled; Open Tracking; An invisible image is being appended to HTML emails to track if they have been opened.
-Enabled; Click Tracking; Every link is being overwritten to track every click in emails.
-Disabled; Subscription Tracking; Allows every link to be overwritten to track every Subscription in emails.
-Disabled; Google Analytics Tracking; Allows tracking of your conversion rates and ROI with Google Analytics.
turned them all off








your stupid goal of total isomorphism
like, every environment can include and call into every library file
you accomplished this by doing two things:

(1) lazy and on demand loading, following this pattern

let _twilio,
async function loadTwilio() {
	if (!_twilio) _twilio = await import('twilio')
	return _twilio
}

and then the idea is that only code that should be able to use twilio calls down in here
and if code erroneously does, then you just get a run time exception
this alone should be enough, but it's not
nitro sees the mention, and freaks out with a build time error

chat suggested nuxt.config.ts vite.build.rollupOptions.external
but this is for hiding nuxt server only modules from nuxt client
not hiding not for nuxt at all modules

you previously somehow accidentally had aws-sdk as a cloud3 development module
and this is a working solution:

(2) install as a development dependency
here are your root nuxt package.json, and serverless framework lambda net23/package.json

./package.json
	"dependencies": {
		"@supabase/supabase-js": "^2.39.8",
	},
	"devDependencies": {
		"@sendgrid/mail": "^8.1.4",
		"aws-sdk": "^2.1691.0",
		"twilio": "^5.3.4",
	}

./net23/package.json
	"dependencies": {
		"@supabase/supabase-js": "^2.39.8",
		"@sendgrid/mail": "^8.1.4",
		"aws-sdk": "^2.1691.0",
		"twilio": "^5.3.4"
	},

both actually use supabase
amazon, twilio, and sendgrid have to be listed in both
but you hide these three in devDependencies just to calm nuxt down

and your hope is that it won't get bundled, but you're not sure about this
chat suggets 
$ npm install -D rollup-plugin-visualizer
$ npm install -D source-map-explorer
as tools to do this

(3) if that hadn't worked
you considered, and probably still should, make a separate library file:
./library/lambda.js
not imported by grand; not imported by nuxt
and then the nuxt package.json doesn't install amazon, twilio, sendgrid modules at all
imported directly by the lambda endpoints, alongside their imports of library stuff through grand
it's possible this will make the nuxt build faster
if you run into problems in this area again, quickly switch to this more straightforward design

note also this doesn't mean you make a library file called worker.js alongside lambda.js
lambda.js is by itself because it's where you can install and use any node module

oh, the note entrypoint might mean that you separate the 
when looking at real monorepo options, you tried moving the nuxt project into ./site/
but cloudflare didn't know what to do with it
so instead you could keep nuxt as the root, and move the pure node tasks into ./library
so then there's a new ./library/package.json













2024oct24

you've given up trying to get node modules like twilio working in a worker
none of these node modules were written imagining a web worker future
and their service providers don't mention web workers at all
even if you got one to work, it might be less reliable in a worker

also, even though javascript is full stack, there are big differences between client and server modules, tools and patterns
client js is all about es6, include, tree shaking, and small modules with few dependencies
server js is all about commonjs, require, node, and big modules with hundreds of dependencies
building server modules for the client is a neat demo for a meetup, but not a fruitful practice for a workday

so, you'll just try the lambda first now, instead of last
thank the bezos lambda exists--otherwise you'd be securing a virtual instance with chatgpt right now!

and even with this hybrid model, cloudflare is still great at:
-loading really fast, including as a cold start to make a good first impression to a brand new prospective user
-running trusted code that can't be tampered with
-using supabase, with their module, and datadog, with fetch
don't try to get it to do more than that

this hybrid model still keeps all the application logic in the worker
the lambda code is just "send this email" or "upload this file"--individual atomic commands,
and notes in database tables as to what happened afterwards










2024nov4

you've switched to yarn, using yarn 1.22.22 on both mac and windows
deleting and then git ignoring yarn.lock, which isn't standard
but as you checkout and build on mac, windows, and linux (through docker)
you want yarn to get stuff specific to that new platform, not a yarn.lock generated on a different os









   ____  _                    __              _ _                  
  / __ \(_)_ __ ___   __ _   / _| ___  _ __  | (_)_ __  _   ___  __
 / / _` | | '_ ` _ \ / _` | | |_ / _ \| '__| | | | '_ \| | | \ \/ /
| | (_| | | | | | | | (_| | |  _| (_) | |    | | | | | | |_| |>  < 
 \ \__,_|_|_| |_| |_|\__, | |_|  \___/|_|    |_|_|_| |_|\__,_/_/\_\
  \____/             |___/                                         


network 23 needs to do some image manipulation tasks, like:
-convert an open graph protocol card from svg generated by nuxt to png so whatsapp can display it
-resize and image and add watermark text
we've found the node module sharp to do this
while sharp is a node module, it's fast because native libraries do the image processing

getting yarn to install the right libraries for amazon linux was challenging
after long journeys on many dead-end trails sherpa'd by chatgpt, we found two working solutions:
(1) a "stowaway" method, detailed below
(2) a clean build in Docker, detailed in the next section

the stowaway method:

manually chuck the linux binaries into net23's node modules @img folder
yarn add, remove, install will find them and chuck them overboard,
so after running yarn, so you have to stow them away again
serverless.yml's exclude keeps the mac or windows binaries out of the zip,
but lets the stowaways through

more info and example commands in net23/stowaway.mjs


     _            _             
  __| | ___   ___| | _____ _ __ 
 / _` |/ _ \ / __| |/ / _ \ '__|
| (_| | (_) | (__|   <  __/ |   
 \__,_|\___/ \___|_|\_\___|_|   
                                
to be able to get the files to stow away, you learned docker
and, docker will always be the most correct, clean, and secure way to build and deploy
perhaps the stowaway method will break,
or some new node module will need to actually compile binaries for amazon linux
if so, you've got docker ready to (again) save the day

here are the commands you learned to use docker generally,
and build in amazon linux to make a serverless package with a sharp that works on lambda
dollar sign on the left is your host computer; hashtag indented are commands inside the container

==== docker setup

$ brew update
$ brew install --cask docker

on mac, run Docker.app to start Docker Desktop for the command line docker to work

$ docker --version
$ docker-compose --version
$ docker run hello-world

==== docker cleanup

$ docker images
$ docker rmi image9
$ docker ps -a
$ docker rm container9

==== docker setup

use the platform attribute to specify what processor architecture we want
switching from default amd64 x86 to faster better cheaper amazon graviton arm64
notice the subtle aMD versus aRM below!

$ docker run -it --platform linux/amd64 --name container23    amazonlinux:2023 /bin/bash
$ docker run -it --platform linux/arm64 --name container23arm amazonlinux:2023 /bin/bash
$ exit
$ docker start -ai container23arm

$ uname -m
arm64  ~host apple silicon mac mini
x86_64  ~container made with platform linux/amd64 to match amazon lambda default
aarch64  ~container made with platform linux/arm64 to match amazon graviton

		# yum update -y
		# yum install -y which git tar zip
		# curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.5/install.sh | bash
		# source ~/.nvm/nvm.sh
		# nvm install 20
		# npm install -g yarn@1
		# node --version
		# yarn --version

		# mkdir code
		# cd code
		# git clone https://github.com/zootella/cold3
		# cd cold3
		# git checkout graviton1  ~branch name if you're working on a branch right now
		# rm yarn.lock
		# yarn install

$ docker cp ../.env       container23arm:/code/cold3/.env
$ docker cp ../.env.local container23arm:/code/cold3/.env.local

		# git pull
		# yarn install
		# yarn seal
		# yarn test
		# cd net23
		# yarn build

$ rm -rf .serverless
$ docker cp container23arm:/code/cold3/net23/.serverless .serverless
$ docker cp container23arm:/code/cold3/wrapper.txt       ../wrapper.txt
$ docker cp container23arm:/code/cold3/icarus/wrapper.js ../icarus/wrapper.js

$ yarn justdeploy (have serverless deploy the .serverless folder without building it first)
$ cd ../site (then also deploy the nuxt site to cloudflare)
$ yarn deploy
$ cd ..
$ git status (and commit and push to github the sealed version you deployed)

==== docker loop

$ docker start -ai container23arm

		# source ~/.nvm/nvm.sh
		# nvm install 20
		^when you go back in, you have to repeat just these two; afterwards yarn is there

		# cd /code/cold3
		# git stash
		# git stash clear
		# git pull

		# yarn install
		# yarn seal
		# yarn test
		# cd net23
		# yarn build

$ rm -rf .serverless
$ docker cp container23arm:/code/cold3/net23/.serverless .serverless
$ docker cp container23arm:/code/cold3/wrapper.txt ../wrapper.txt
$ docker cp container23arm:/code/cold3/icarus/wrapper.js ../icarus/wrapper.js

$ yarn justdeploy
$ cd ../site
$ yarn deploy
$ cd ..
$ git status (and so on to commit and push)

==== docker end

from this experimentation and use, here's what you've got on your mac:

% docker images
REPOSITORY    TAG       IMAGE ID       CREATED       SIZE
amazonlinux   2023      6c2c3bb2ce48   6 days ago    248MB
<none>        <none>    5bf4cf420ef7   4 weeks ago   214MB  ~you're seeing this remnant from amazonlinux a month ago but leaving it alone

% docker ps -a
CONTAINER ID   IMAGE              COMMAND       CREATED             STATUS                   PORTS     NAMES
270df468a535   amazonlinux:2023   "/bin/bash"   About an hour ago   Up About an hour                   container23arm
c65859f86d9a   5bf4cf420ef7       "/bin/bash"   2 weeks ago         Exited (0) 2 weeks ago             container23
0c25625e7aaf   5bf4cf420ef7       "/bin/bash"   2 weeks ago         Exited (0) 2 weeks ago             layer1

enter container23arm to do the loop tasks above
the other container, layer1, is where you made lambda layers for sharp and twilio that you could not get working

using these notes, you were building the zip inside docker,
then copying it back out, and deploying it from the host computer
this is because you didn't authenticate the container to be able to deploy directly
but if you return to docker later to do clean secure deployments, you should
and it shouldn't be hard to get github, cloudflare, and amazon authenticated
so then you'd seal and deploy and commit all from inside the container

chat kept on encouraging you to make a Dockerfile,
but you just used it manually like a virtual computer
you also worked on scripting the loop tasks, using a bash .sh script
the hard part there was being able to tell on the outside when the build on the inside finished
you planned a little done.txt file inside that outside would watch
you never ran the script draft now that stowaway is the fast way, and all inside docker is the good way

                  _    
 _ __   __ _  ___| | __
| '_ \ / _` |/ __| |/ /
| |_) | (_| | (__|   < 
| .__/ \__,_|\___|_|\_\
|_|                    

you had configured a custom rollup bundle step for the lambda functions,
but just removed it
it wasn't working--it was packing the library code, which is small to begin with
and then ignoring the node modules folder, which serverless would zip up entirely

also, with it gone, you can start the local server nearly instantly,
and error stacks have findable line numbers

you got net23.zip from 50mb down to 20mb just by chasing away aws-sdk as a dev dependency,
and clumsily, line item excluding the largest node modules that came in as icarus dev dependencies

but, you are still drawn to the JavaScript dream--that since all code can run everywhere,
and transpilation can turn anything into anything,
and tree shaking can remove entire modules and individual functions in a file,
everything is awesome

<rant>
none of that worked, however--
there have been days of fighting to get CommonJS and ESM modules to work with each other,
weaving the boundary means that you have to await import, spreading async upwards and everywhere,
modules like twilio written for require() don't work well when you load them not with require(),
jimp, pure javascript, is prohibitively slower than sharp, which is all native,
and even though cloudflare has a node compatibility flag,
that absolutely does not mean that a worker can load or run a node module
</rant>

but, you could still mess around with the code that gets zipped for lambda
if you do, these are the tools that look promising:

https://parceljs.org/
2017, zero configuration, can struggle with complex mixtures of commonjs and esm

https://www.npmjs.com/package/serverless-bundle
2018, for serverless framework, uses webpack, may handle mixture well

https://esbuild.github.io/
2020, extremely fast, should be able to handle the mixture










                                                _     
  ___  _ __   ___ _ __     __ _ _ __ __ _ _ __ | |__  
 / _ \| '_ \ / _ \ '_ \   / _` | '__/ _` | '_ \| '_ \ 
| (_) | |_) |  __/ | | | | (_| | | | (_| | |_) | | | |
 \___/| .__/ \___|_| |_|  \__, |_|  \__,_| .__/|_| |_|
      |_|                 |___/          |_|          

you got open graph images working by adding nuxt-og-image:
https://www.npmjs.com/package/nuxt-og-image
https://nuxt.com/modules/og-image
https://nuxtseo.com/docs/og-image/getting-started/introduction

dashboard clicking steps were:
cloudflare dashboard, workers and pages, KV, create a namespace
OG_IMAGE_CACHE, name
then the dashboard generates an id, pasted that into wrangler.toml

this was enough to get image caching working; you can see them in the dashboard
keys have nuxt og image module version:route:not sure, like:
3.0.8:index:SQxRoEWOgn
3.0.8:card:whatever-you-put-here:a04axs96VI
values like:
{
	"value":"iVBORw0KGgoAAAANSUhEUgAABLA... (whole thing is 159 KB when downloaded) ...NW84AAAAASUVORK5CYII=",
	"headers":{
		"Vary":"accept-encoding, host",
		"etag":"W/\"fvJ5qGVPK5\"",
		"last-modified":"Sat, 09 Nov 2024 02:39:05 GMT",
		"cache-control":"public, s-maxage=604800, stale-while-revalidate"
	},
	"expiresAt":1731724745242
}

urls like:

http://localhost:3000/__og-image__/image/og.png
http://localhost:3000/__og-image__/image/og.svg

https://cold3.cc/__og-image__/image/og.png
https://cold3.cc/__og-image__/image/og.svg (deployed, svg doesn't work; not sure why but doesn't matter)

still to investigate further:

how is this actually working? it's amazing that it doesn't need a lambda after all!
https://github.com/vercel/satori renders the SVG, and then
https://github.com/thx/resvg-js converts SVG -> PNG, all on the worker

how does the cache expiration work?
if a post is very popular, you want to deliver the card instantly to thousands of followers
but also, if a user edits their post, they want the card to immediately show the edited information

and you've tried the ?purge flag, like
https://cold3.cc/__og-image__/image/og.png?purge
but it doesn't always regenerate the image

you've set cards with defineOgImageComponent() in index.vue and card/[more].vue
but what if someone sends a link to cold3.cc/settings
you want the same card as the home page
can you define a global card, which then gets overridden with a user page or post card?









      _                 _  __                 _      __                  _   _             
  ___| | ___  _   _  __| |/ _|_ __ ___  _ __ | |_   / _|_   _ _ __   ___| |_(_) ___  _ __  
 / __| |/ _ \| | | |/ _` | |_| '__/ _ \| '_ \| __| | |_| | | | '_ \ / __| __| |/ _ \| '_ \ 
| (__| | (_) | |_| | (_| |  _| | | (_) | | | | |_  |  _| |_| | | | | (__| |_| | (_) | | | |
 \___|_|\___/ \__,_|\__,_|_| |_|  \___/|_| |_|\__| |_|  \__,_|_| |_|\___|\__|_|\___/|_| |_|
                                                                                           

after not being able to get Lambda@Edge to work in serverless.yml,
you found amazon's weird CloudFront Functions:
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/edge-functions.html
it's javascript, sorta; there are a lot of weird limitations:
https://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/functions-javascript-runtime-20.html
for instance, you can't comment at the end of a line; comments have to be on separate lines

$ node vhs.cjs

is the edit-test-debug loop you made
deployment is pasting code into a box in the aws dashboards:

aws dashboard, cloudfront
functions, third one down on the left
create function
vhs1, name
cloudfront-js-2.0, runtime

paste secrets into the top of the box
paste code into the box below the secrets, save changes

publish tab, publish function
then at the bottom, associated distributions, add association
vhs.net23.cc, distribution
viewer request, event type
cache behavior, clicked down and selected default, not sure what this is or if there's a use here for you
add association

in the code, console.log works (console.error isn't there, though!)
and logs started going into CloudWatch without you having to configure anything

if you change the code, a new deploy takes 5-10min, so use vhs.cjs









 _                        _   _ _      
| |_ _   _ _ __ _ __  ___| |_(_) | ___ 
| __| | | | '__| '_ \/ __| __| | |/ _ \
| |_| |_| | |  | | | \__ \ |_| | |  __/
 \__|\__,_|_|  |_| |_|___/\__|_|_|\___|
                                       

cloudflare turnstile is better than recaptcha because not google, and no clicking traffic lights
https://developers.cloudflare.com/turnstile/get-started/
we're also on cloudflare pages, a site hosted anywhere can use turnstile the same way we do

start in the dashboard
widget name: turnstile1
hostname management: cold3.cc
widget mode: invisible
pre-clearance: no

and then get the site key and secret key, which in our code will be:
ACCESS_TURNSTILE_SITE_KEY_PUBLIC, available to untrusted front-end code, revealed to users
ACCESS_TURNSTILE_SECRET, securely and secretly stored in the cloudflare worker

app.vue loads Turnstile site-wide as soon as the user navigates to any route
if later the user navigates to a turnstile-protected form, turnstile already has signals that indicate human behavior

in the dashboard, Widget Mode choices are:
-Managed
-Non-interactive
-Invisible
and in code, window.turnstile.render size options are:
-invisible
-normal
-compact
-flexible

running configured invisible+invisible, widget doesn't show up, 2.3 second delay
running configured noninteractive+normal, widget shows up, then sometimes disappears, 2.3 second delay, and shows a spinner, or a green check in a circle
you haven't seen the checkbox the user must check, but officially, that's a possibility you can't rule out
coding turnstile you configured it noninteractive+normal
[]but to use it later you'll change back to invisible+invisible

so, because the time delay is noticeably long, and there's no user interaction, your user experience desigin plan is this:
when the user navigates to the form, turnstile executes to get a token invisibly
the user won't notice any delay unless they fill out and submit the form faster than 2.3 seconds, or longer than 4 minutes
a really fast user will get the remaining delay hidden into the submit button being orange
a really slow user will get the full delay baked
so even if our api endpoint gets back in milliseconds, it will look like the server took a few seconds
this is a normal and expected experience on the consumer web today

with this design, we're never showing turnstile feedback,
and hiding the time delay in the user's typing in the form and the submit button
the submit button already needs a good visual "working on it" appearance (like pink hearts bubbling up or something)
and that spinner will work for both delays
here, we've also grouped two progress bars into a single progress bar, à la Office Space

even configured invisible, turnstile documentation says that there's a possibility it will show up for the user to check a box
if it does this, the experience will be:
while the button is orange, beneath it, the widget appears
the user checks the box, and then the orange button finishes normally
this experience is below our standard for user experience on the site, but is totally normal for average websites
also, it may never happen if "non-interactive" and "invisible" really mean those things

additional features you're not using yet, but which may be good
https://developers.cloudflare.com/turnstile/concepts/pre-clearance-support/
designed for SPAs, which we are, so that you can include turnstile on every POST but not get turnstile bothering the user

also there's stats about turnstile in the dashboard
see if you can see how long the browser hashing delay is for real useres, ideally p50 and p95
and if the widget has ever shown anyone the checkbox









                  _                 _   _             
  __ _  ___  ___ | | ___   ___ __ _| |_(_) ___  _ __  
 / _` |/ _ \/ _ \| |/ _ \ / __/ _` | __| |/ _ \| '_ \ 
| (_| |  __/ (_) | | (_) | (_| (_| | |_| | (_) | | | |
 \__, |\___|\___/|_|\___/ \___\__,_|\__|_|\___/|_| |_|
 |___/                                                

cloudflare comes with really good geolocation
official docs:
https://developers.cloudflare.com/network/ip-geolocation/
https://developers.cloudflare.com/rules/transform/managed-transforms/reference/#add-visitor-location-headers
https://developers.cloudflare.com/rules/transform/managed-transforms/configure/
you have to turn it on
there are two checkboxes, and they're hard to find

first one, just the country
cloudflare dashboard, domain name
Network, on the left; options on the right like IPv6 Compatibility, gRPC, WebSockets
IP Geolocation, seems to be enabled by default; "Include the country code of the visitor location with all requests to your website. Note: You must retrieve the IP Geolocation information from the CF-IPCountry HTTP header."

second one, city and more
cloudflare dashboard, domain name
Rules, on the left
click to the second page of cards to find one that has the linked category Request Header Transform Rules
Managed Transforms tab
"Add visitor location headers. Adds HTTP request headers with location information for the visitor's IP address, including city, country, continent, longitude, and latitude."
turn on, not on by default

earlier, you setup http->https and www->apex, documented above
the cards in Rules, Overview may be new, and may be an easier way to configure this
cards include:
"Redirect from HTTP to HTTPS: Always redirect HTTP requests to HTTPS based on hostname.
Redirect from WWW to Root: Always redirect HTTP requests from the WWW subdomain to the root.
Redirect to a New URL: Redirect visitors requesting one page to another page URL."


















										 _                       
 _ __ ___   __ _  __| |_ __ ___   __ _ _ __  
| '__/ _ \ / _` |/ _` | '_ ` _ \ / _` | '_ \ 
| | | (_) | (_| | (_| | | | | | | (_| | |_) |
|_|  \___/ \__,_|\__,_|_| |_| |_|\__,_| .__/ 
																			|_|    

== high level

>milestone 1, done

domain name
ssl certificate

www bucket
www distribution
www upload

>milestone 2

api functions
only callable from https://cold3.cc
that use imports
and library code
and survived webpack tree shaking

>milestone 3

vhs bucket
vhs distribution
vhs lambda@edge un-get-round-able gatekeeper
so also only callable from cold3

>milestone 4

send an email
send a sms
