
# OG Image System

Social share cards ‚Äî the little preview images that appear when someone pastes a link on Twitter, Slack, iMessage, etc. An afterthought for users, critically important for the product, and one of the most technically sophisticated parts of the Cloudflare deployment.

The site workspace uses `nuxt-og-image` 6.0.0-beta.15 to generate 1200x630 PNG images entirely inside the Cloudflare Worker at cold3.cc. No external services, no headless Chrome, no Node.js. Pure WASM.

## Goals and Requirements

Think of a site like early Twitter: every post has a card, showing its text. Billions of posts over a decade of global use. Three properties define the caching requirements:

üî• **Hot cards.** A post goes viral and millions of people worldwide request its card. The system must serve it from cache ‚Äî not re-render on every request. Some additional renders are fine (once per geographic region), but not one render per request.

üßü **Zombie cards.** The vast majority of cards ever generated will never be requested again. Users quit; posts quickly fade from relevance. A cache that stores cards forever is a cache where nearly every entry is dead weight, and storage grows without bound. The system must evict cards automatically.

üçû **Cards don't go stale.** A card shows post text, not statistics. The post can't be edited. A fresh render of a previously rendered card produces the exact same pixels. There is zero value in re-rendering for freshness ‚Äî only cost.

## How It Works

```
request ‚Üí middleware ‚Üí ‚õÖ Cache API (edge) ‚Üí üîë KV cache ‚Üí üé® satori renderer
                                                     ‚Üë
                                              (to be replaced
                                               with lru-cache)
```

Three layers, outermost to innermost:

**‚õÖ CDN edge cache** (`site/server/middleware/middleware.js`). Uses the Cloudflare Cache API (`caches.default`), explicitly managed by our middleware. On cache hit (~40ms), returns the cached PNG directly ‚Äî nothing downstream runs. On cache miss, invokes the Worker's own fetch handler via a SELF service binding, captures the rendered response, stores it in the edge cache with `waitUntil()` (non-blocking), and returns it to the client. Cache entries expire automatically per the `s-maxage` TTL. Sets `x-card-source: RECYCLED` or `FRESH` for diagnostics.

The Cache API is necessary because Cloudflare Workers run *in front of* the CDN. A response generated by a Worker goes straight to the client ‚Äî the CDN never sees it, never caches it, regardless of Cache-Control headers. The only way to get edge caching for Worker-generated responses is explicit `cache.match()` / `cache.put()`. Cache Rules and Page Rules don't help either ‚Äî they operate on the CDN layer, which Worker responses bypass entirely. `Vary: Host` is also not the problem ‚Äî Cloudflare ignores most Vary values in caching decisions.

**üîë KV cache** (nuxt-og-image built-in). The module's application-level cache, backed by a Cloudflare KV namespace (`OG_IMAGE_CACHE`). On KV hit (~100ms), serves the stored PNG without rendering. After rendering a new card, `cache.js` base64-encodes the PNG and writes it to KV alongside an `expiresAt` timestamp. On the next request, it checks the timestamp ‚Äî if expired, it deletes the entry and falls through to the renderer. This is lazy expiration: an entry is only evaluated when re-requested. Entries never re-requested persist in KV forever (the zombie problem). The module does not use KV's native `expirationTtl` (which would give true automatic deletion), and there's no configuration option to enable it. This layer will be replaced with a bounded lru-cache driver.

**üé® Satori renderer.** Two WASM modules ‚Äî yoga (flexbox layout) and resvg (SVG rasterization) ‚Äî turn a `.satori.vue` component into a 1200x630 PNG. A render takes ~650‚Äì1100ms depending on WASM cold-start state. Instances are initialized once per Worker isolate and reused for subsequent requests.

### Cookie exclusion

The browserTag cookie middleware (`site/server/middleware/middleware.js`) skips `/_og/*` routes entirely. Cloudflare's CDN refuses to cache any response with a `Set-Cookie` header (a deliberate security policy to prevent serving one user's session cookie to another). OG images are consumed by crawlers and `<img>` tags, not user sessions, so no browser tag is needed. The middleware also copies only safe headers (`content-type`, `cache-control`, `etag`, `last-modified`, `vary`) from the rendered response ‚Äî never `Set-Cookie`.

Before this fix, the cookie middleware ran on every request including `/_og/*` routes. `Set-Cookie` was present on every og:image response, and `cf-cache-status` was absent ‚Äî the CDN was never caching. Removing the cookie was necessary but not sufficient: the CDN still didn't cache because Workers bypass it entirely (see above). Both fixes ‚Äî cookie exclusion and the Cache API middleware ‚Äî were needed.

### Service binding

The middleware needs to invoke the full Nitro/nuxt-og-image pipeline on cache miss. The first attempt was a plain self-fetch ‚Äî `fetch(url, { headers: { 'x-card-render': 'RENDER' } })` ‚Äî to re-enter the Worker, with a bypass header to skip the cache check on the subrequest. This failed with HTTP 522 (Connection Timed Out). Cloudflare's infrastructure-level loop detection kills the request before any application code runs ‚Äî our bypass header was never read. Self-fetch is not a viable pattern for Cloudflare Workers.

The solution is a SELF service binding ‚Äî `env.SELF.fetch(request)` ‚Äî which invokes the Worker's own fetch handler on the same thread with zero network overhead, bypassing loop detection entirely. Configured in wrangler.jsonc: `"services": [{"binding": "SELF", "service": "site4"}]`. The `x-card-render: RENDER` bypass header works as originally intended on this path.

Self-binding is not in Cloudflare's official docs (the service bindings docs describe only Worker-A-calls-Worker-B). But it is community-recommended and acknowledged by the Cloudflare tooling team (workers-sdk #8246). The alternatives Cloudflare provides don't work for our case: `global_fetch_strictly_public` is still subject to loop detection; Custom Domains still go through the network; `env.ASSETS.fetch()` is static assets only; and refactoring into a shared function isn't viable when you need to invoke an entire framework pipeline (Nitro middleware ‚Üí nuxt-og-image handler ‚Üí satori render).

Limits: each `self.fetch()` counts toward the 32 Worker invocations per request (we use 1) and the 1,000 subrequest limit (we use 1). Service binding calls do *not* count toward the 6 simultaneous open connection limit.

### Design decision: service binding over beforeResponse hook

Two CDN caching designs were built and tested, both working and committed:

**Design A ‚Äî middleware + plugin** (commit `84f2132`). Cache reads in middleware10, cache writes in a separate Nitro plugin (`ogCachePlugin.js`) that hooks `beforeResponse`. The plugin captures `response.body` (the raw Buffer returned by nuxt-og-image's handler) and the response headers already set on the event by the module's cache.js, builds a Response, and `cache.put()`s it. This works because nuxt-og-image's handler `return`s the image buffer rather than calling `send()`. But if the module ever switched to `send()`, the `beforeResponse` hook would fire with `body: undefined` (h3 v1 issue #596) and caching would silently stop ‚Äî no error, just every request falling through to a fresh render. Fragile coupling to a beta module's internals.

**Design B ‚Äî middleware + service binding** (commit `4a00c5e`, chosen). All cache logic in one file. The service binding gives us a complete Response object we fully control ‚Äî headers, body, status ‚Äî regardless of how the downstream handler produces it internally. No dependency on h3 hook behavior. Requires the SELF binding in wrangler.jsonc.

No measurable performance difference. First-render times across all tests: 817ms (Design A), 1120ms/969ms/811ms/623ms (Design B) ‚Äî the variance is satori/WASM cold-start noise, not overhead from the service binding. Cache hit times are equivalent (~38‚Äì44ms).

**Decision:** Design B. The robustness argument is decisive ‚Äî we're pinned to a beta module, and coupling our caching to its internal return-vs-send behavior is an unnecessary risk. The SELF binding is one line of configuration and eliminates that fragility entirely.

## Configuration

`site/nuxt.config.js`:

```js
configuration.ogImage = {
  defaults: {
    cacheMaxAgeSeconds: 20*Time.minutesInSeconds  // = 1200 seconds. default if omitted is 3 days
  },
  runtimeCacheStorage: {
    driver: 'cloudflare-kv-binding',
    binding: 'OG_IMAGE_CACHE',
  },
}
```

`site/wrangler.jsonc`:

```jsonc
"kv_namespaces": [
  {"binding": "OG_IMAGE_CACHE", "id": "ee95a879988944c2a7eb9521e62eb102"}
],
"services": [
  {"binding": "SELF", "service": "site4"}
]
```

Dependencies: `nuxt-og-image` 6.0.0-beta.15, `satori` 0.15.2, `@resvg/resvg-js` ^2.6.2, `@resvg/resvg-wasm` ^2.6.2.

Key module source files (in `node_modules/nuxt-og-image/dist/runtime/server/`): `util/eventHandlers.js` (route handler, orchestrates cache then render), `util/cache.js` (`useOgImageBufferCache()`, sets headers, writes to storage), `og-image/satori/renderer.js` (the satori+resvg pipeline), `og-image/satori/instances.js` (WASM singleton management).

## How to Test

No local or staging environment ‚Äî WASM rendering, the Cache API, and KV bindings only exist in the production Worker. Every test targets production. The same test must be repeatable before and after each change, so that results are directly comparable.

Use a unique card route each run (to guarantee no prior cache state), fetch the og:image URL twice with curl, and read the headers and timing:

```sh
OG_URL=$(curl -s "https://cold3.cc/card/test$RANDOM" \
  | grep -o 'content="https://cold3.cc/_og[^"]*' | sed 's/content="//')

# first fetch ‚Äî expect x-card-source: FRESH, full satori render, ~650-1100ms
curl -s -D- -o /dev/null -w "\nTotal: %{time_total}s\n" "$OG_URL"

# wait a few seconds, then second fetch ‚Äî expect x-card-source: RECYCLED, ~40ms
curl -s -D- -o /dev/null -w "\nTotal: %{time_total}s\n" "$OG_URL"
```

**Header signals:**

- **`x-card-source`** ‚Äî our middleware's cache result (`RECYCLED` or `FRESH`)
- **`cf-cache-status`** ‚Äî the definitive CDN signal. `HIT` = edge cache. Absent = CDN not participating.
- **`age`** ‚Äî seconds since edge cached (present on cache hits)
- **`set-cookie`** ‚Äî must be absent on `/_og/*` responses. If present, CDN will refuse to cache.
- **Timing** ‚Äî ~650‚Äì1100ms = full satori render (FRESH). ~40ms = CDN edge hit (RECYCLED).

## Test Results

### Baseline (2026feb7, commit `8b358a1`)

Configuration: KV cache enabled, `cacheMaxAgeSeconds: 1200`, cookie middleware running on all routes.

**First fetch (859ms ‚Äî full satori render):**
```
content-type: image/png
cache-control: public, s-maxage=1200, stale-while-revalidate
etag: W/"Axyov5M0U5BMNLny4LeHTrN_vc6ViQjgDh418ltDoO4"
set-cookie: __Secure-current_session_password=...; Max-Age=34128000; Domain=cold3.cc; Path=/; HttpOnly; Secure; SameSite=Lax
cf-cache-status: (absent)
```

**Second fetch (128ms ‚Äî KV cache hit):**
```
etag: W/"Axyov5M0U5BMNLny4LeHTrN_vc6ViQjgDh418ltDoO4"    ‚Üê same (not re-rendered)
set-cookie: __Secure-current_session_password=...           ‚Üê still present
cf-cache-status: (absent)
```

KV works (859‚Üí128ms). CDN is not caching ‚Äî `cf-cache-status` absent on both, `Set-Cookie` present on both.

### Post-cookie-fix (2026feb8, commit `c007a5e`)

Configuration: cookie middleware now skips `/_og/` routes.

**First fetch (818ms):**
```
set-cookie: (absent)         ‚Üê cookie fix works
cf-cache-status: (absent)    ‚Üê CDN still not caching
```

**Second fetch (100ms ‚Äî KV cache hit):**
```
set-cookie: (absent)
cf-cache-status: (absent)
```

Cookie fix confirmed ‚Äî `Set-Cookie` is gone. But CDN is still not caching: timing matches a KV hit, not an edge hit. This is when we discovered that Workers bypass the CDN entirely and that the Cache API is needed.

### CDN working (2026feb8, commit `4a00c5e`)

Configuration: Cache API middleware (Design B, service binding) deployed.

**First fetch (1120ms ‚Äî full satori render, cache miss):**
```
content-type: image/png
cache-control: public, s-maxage=1200, stale-while-revalidate
etag: W/"pp-yH-_wgVISvl4U5agh3zi7arPKsIOOttSVj5_d7Zk"
last-modified: Sun, 08 Feb 2026 23:38:15 GMT
vary: accept-encoding, host
set-cookie: (absent)
cf-placement: local-DEN
x-card-source: FRESH
```

**Second fetch (38ms ‚Äî edge cache hit):**
```
cache-control: public, max-age=14400, s-maxage=1200, stale-while-revalidate
etag: W/"pp-yH-_wgVISvl4U5agh3zi7arPKsIOOttSVj5_d7Zk"    ‚Üê same
last-modified: Sun, 08 Feb 2026 23:38:15 GMT               ‚Üê same
cf-cache-status: HIT                                        ‚Üê CDN is caching
age: 8                                                      ‚Üê seconds in cache
x-card-source: RECYCLED
set-cookie: (absent)
cf-placement: local-DEN
```

1120ms ‚Üí 38ms. `cf-cache-status: HIT` and `age: 8` confirm edge caching. The system now matches the target architecture.

### Interleaved cards (2026feb8)

Two unique cards, three rounds interleaved:

```
A1  MISS  653ms  content-length: 31309
B1  MISS  556ms  content-length: 30911
     ‚Äî 3 second pause ‚Äî
A2  HIT   50ms   cf-cache-status: HIT  age: 3
B2  HIT   38ms   cf-cache-status: HIT  age: 3
     ‚Äî 3 second pause ‚Äî
A3  HIT   40ms   cf-cache-status: HIT  age: 6
B3  HIT   42ms   cf-cache-status: HIT  age: 6
```

Both cards render independently (different content-length), cache independently, and `age` increments correctly across rounds.

## Remaining Work

### Replace KV with lru-cache

KV's lazy expiration means entries never re-requested persist forever ‚Äî unbounded storage growth. The simple alternative ‚Äî removing the `runtimeCacheStorage` block entirely ‚Äî falls back to Nitro's default in-memory driver, a plain Map with no size limit. This has the same unbounded growth problem, just in process memory instead of KV. On a Cloudflare Worker, isolates can live for weeks; a long-lived isolate would accumulate an entry for every unique card it renders until it hits the 128MB memory limit.

unstorage ships an `lru-cache` driver (backed by `lru-cache` v11) as a bounded alternative. `lru-cache` is already in the dependency tree as a transitive dep of unstorage ‚Äî no install needed. The replacement config:

```js
configuration.ogImage = {
  defaults: {
    cacheMaxAgeSeconds: 2*Time.hoursInSeconds,
  },
  runtimeCacheStorage: {
    driver: 'lru-cache',
    max: 1,
  },
}
```

`max: 1` means the LRU holds at most one card. The module's cache code still runs and still sets `Cache-Control` headers on every response, but the in-isolate cache is effectively inert ‚Äî it can't serve a hit unless two consecutive requests for the exact same card land on the same isolate. This makes test results clean: a fast response is a CDN hit, a slow response is a render. (`max: 0` doesn't mean zero entries ‚Äî it means "not set," and lru-cache throws a TypeError. Don't use it.)

`cacheMaxAgeSeconds` fans out to three places in nuxt-og-image's cache.js: (1) the `Cache-Control` header that enables the CDN, (2) the storage driver's `expiresAt` entry, (3) `maxAge` in h3's `handleCacheHeaders` for 304 conditional requests. Only (1) matters for us, but there's no way to set it independently.

Can be bumped to `max: 50` or higher later if the in-isolate cache proves valuable as a second layer behind the CDN. At ~37KB per entry, `max: 50` caps at ~1.8MB regardless of isolate lifetime.

### After deploying

Delete the `OG_IMAGE_CACHE` KV namespace from the Cloudflare dashboard and remove the `kv_namespaces` block from `site/wrangler.jsonc`.
